[
  {
    "path": "models/2023-11-25-advanced-machine-learning-cnn/",
    "title": "Lesson 3: Advanced Machine Learning (Ⅱ)",
    "description": "In the field of ecology, there are lots of pattern related to image recognition, such as species identification, soundscape and remote sensing images. This section will introducde how to do these complex tasks using CNN.",
    "author": [],
    "date": "2023-11-25",
    "categories": [],
    "contents": "\n\nContents\n2. An example of a CNN model\n2.1 Preparing data with generator\n2.2 Training and turning model\n2.3 Prediction with CNN model\n\n\n2. An example of a CNN model\nThe main code is modified from the website, the website and the website.\n2.1 Preparing data with generator\nIn this context, we import a number of libraries that provide specific tools and functions for each stage of our project. These libraries not only facilitate our work, but also ensure that we can make the most of their advanced features. Here are some of the libraries we import.\n\n\n# library(tidyverse) # Data wrangling\n# library(imager) # Image manipulation\n# library(keras) # Deep learning\n# library(caret) # Model Evaluation\n\n\n2.1.1 Data list and path\nImage processing is a crucial step. In this stage, raw images need to be processed to make them suitable for use in the desired model, involving image preprocessing, resizing to uniform dimensions, normalizing pixel intensities. Additionally, image augmentation can be performed to enhance data variability and diversity.\n\n\n# rm(list = ls())\n# \n# # path to images\n# train_folder_list <- list.files(\"fruits-360/Training/\") # image folders\n# train_folder_list\n# train_images_list <- c(\"Kiwi\", \"Banana\", \"Apricot\") # list of images\n# train_images_path <- paste0(\"fruits-360/Training/\", train_images_list, \"/\")\n# train_images_path\n# \n# # get file name\n# train_file_name <- map(train_images_path, function(x) paste0(x, list.files(x))) %>% unlist()\n# head(train_file_name) # first 6 file name\n# tail(train_file_name) # last 6 file name\n# length(train_file_name)\n# \n# # Plot image\n# set.seed(123)\n# sample_image <- sample(train_file_name, 6) # Randomly select image\n# img <- map(sample_image, load.image) # Load image into R\n# par(mfrow = c(2, 3)) # Create 2 x 3 image grid\n# map(img, plot) # Plot image\n\n\n2.1.2 Checking and uniforming data\nWe need to check the image dimension and create a proper input dimension for building the deep learning model.\n\n\n# img <- load.image(train_file_name[1]) # First Image \n# img\n# dim(img) # get the value of each dimension\n\n\nNow let’s create a function that will automatically get the height and width of an image and convert it into a dataframe.\n\n\n# # automatically for acquiring width and height of an image\n# get_dim <- function(x){\n#   img <- load.image(x) \n#   df_img <- data.frame(height = height(img),\n#                        width = width(img),\n#                        filename = x)\n#   return(df_img)\n# }\n# get_dim(train_file_name[1])\n# \n# # automatically for acquiring width and height of all images\n# set.seed(123)\n# sample_file <- sample(train_file_name) # Randomly get 1328 sample images\n# file_dim <- map_df(sample_file, get_dim) # Run the get_dim() for each image\n# head(file_dim, 10)\n# summary(file_dim)\n\n\nLet’s resize to uniform dimensions. The dimensions must not too small to avoid any lost data information, and also not too big to avoid very slow training time. The configuration of batches helps optimize memory usage and computational efficiency during the training process.\n\n\n# # image size to scale down to (original images are 100 x 100 px)\n# img_width <- 20\n# img_height <- 20\n# target_size <- c(img_width, img_height)\n# \n# # RGB = 3 channels\n# channels <- 3\n\n# define batch size\n# batch_size <- 32\n\n\n2.1.3 Spliting dataset\nNormalizing pixel intensities, combined with rescaling and validation splitting, can contributes to creating a more robust and diverse dataset for training. We use the train_data_gen generator() to prepares data with the transformations.\n\n\n# set.seed(100)\n# # Image Generator\n# train_data_gen <- \n#   image_data_generator(rescale = 1/255, # Scaling pixel value\n#                        horizontal_flip = T, # Flip image horizontally\n#                        vertical_flip = T, # Flip image vertically \n#                        rotation_range = 45, # Rotate image from 0 to 45 degrees\n#                        zoom_range = 0.25, # Zoom in or zoom out range\n#                        validation_split = 0.2, # 20% data as validation data\n#                        fill_mode = \"nearest\")\n\n\nNow we can insert our image data into the data generator using the flow_images_from_directory().\n\n\n# # Training Dataset\n# train_image_array_gen <- \n#   flow_images_from_directory(directory = \"fruits-360/Training/\", \n#                              classes = train_images_list,\n#                              target_size = target_size, # the image dimension  \n#                              color_mode = \"rgb\", # use RGB color\n#                              batch_size = batch_size , \n#                              seed = 123,  # set random seed\n#                              subset = \"training\", # declare it for training data\n#                              generator = train_data_gen)\n# \n# # Validation Dataset\n# val_image_array_gen <- \n#   flow_images_from_directory(directory = \"fruits-360/Training/\",\n#                              target_size = target_size, \n#                              classes = train_images_list,\n#                              color_mode = \"rgb\", \n#                              batch_size = batch_size ,\n#                              seed = 123,\n#                              subset = \"validation\", # declare it as validation\n#                              generator = train_data_gen)\n\n\nBalance target classes is very important for classification task. Here we check the class proportion of the train dataset. Our target classes/labels proportion is balance enough.\n\n\n# # Number of training samples\n# train_samples <- train_image_array_gen$n\n# # Number of validation samples\n# valid_samples <- val_image_array_gen$n\n# # Number of target classes/categories\n# output_n <- n_distinct(train_image_array_gen$classes)\n# # Get the class proportion\n# table(\"\\nFrequency\" = factor(train_image_array_gen$classes)) %>% \n#   prop.table()\n\n\n2.2 Training and turning model\n2.2.1 Model architecture\nWe can start building the model architecture for the deep learning. We will build a simple model first with the following layer:\nConvolutional layer to extract features from 2D image with relu activation function\nMax Pooling layer to downsample the image features\nFlattening layer to flatten data from 2D array to 1D array\nDense layer to capture more information\nDense layer for output with softmax activation function\nDon’t forget to set the input size in the first layer. If the input image is in RGB, set the final number to 3. If the input image is in grayscale, set the final number to 1.\n\n\n# # input shape of the image\n# c(target_size, 3) \n# model <- keras_model_sequential() %>% \n#   # Convolution Layer\n#   layer_conv_2d(filters = 32,\n#                 kernel_size = c(3,3),\n#                 padding = \"same\",\n#                 activation = \"relu\",\n#                 input_shape = c(target_size, 3) \n#                 ) %>% \n#   # Max Pooling Layer\n#   layer_max_pooling_2d(pool_size = c(2,2)) %>% \n#   # Flattening Layer\n#   layer_flatten() %>% \n#   # Dense Layer\n#   layer_dense(units = 32, activation = \"relu\") %>% \n#   # Output Layer\n#   layer_dense(units = output_n, activation = \"softmax\", name = \"Output\")\n#   \n# model\n\n\n2.2.2 Model Fitting\nWe start fitting the data into the model. For starter, we will use 30 epochs to train the data. For multilabel classification, we use categorical cross-entropy as the loss function. We will also evaluate the model with the validation data from the generator.\n\n\n# model %>% \n#   compile(\n#     loss = \"categorical_crossentropy\",\n#     optimizer = optimizer_adam(),\n#     metrics = \"accuracy\"\n#   )\n# \n# # Fit data into model\n# history <- model %>% \n#   fit(\n#   # training data\n#   train_image_array_gen,\n#   # training epochspp\n#   steps_per_epoch = as.integer(train_samples / batch_size), \n#   epochs = 30, \n#   # validation data\n#   validation_data = val_image_array_gen,\n#   validation_steps = as.integer(valid_samples / batch_size)\n# )\n# \n# plot(history)\n\n\n2.2.3 Model evaluation\nLet’s evaluate the model using confusion matrix. First, we need to acquire the file name from the data validation. From the file name, we will extract the categorical label as the actual value of the target variable.\n\n\n# val_data <- data.frame(\n#   file_name = paste0(\"fruits-360/Training/\", val_image_array_gen$filenames)) %>%\n#   mutate(class = str_extract(file_name, \"Kiwi|Banana|Apricot\"))\n# head(val_data, 10)\n\n\nWe need to get the image into R by converting the image into an array. Since our input dimension for CNN model is image with 20 x 20 pixels with 3 color channels (RGB). The reason of using array is that we want to predict the original image fresh from the folder, so we will not use the image generator since it will transform the image and does not reflect the actual image.\n\n\n# # Function to convert image to array\n# image_prep <- function(x) {\n#   arrays <- lapply(x, function(path) {\n#     img <- image_load(path, target_size = target_size, \n#                       grayscale = F) # Set FALSE if image is RGB\n#     \n#     x <- image_to_array(img)\n#     x <- array_reshape(x, c(1, dim(x)))\n#     x <- x/255 # rescale image pixel\n#   })\n#   do.call(abind::abind, c(arrays, list(along = 1)))\n# }\n# \n# test_x <- image_prep(val_data$file_name)\n# # Check dimension of testing data set\n# dim(test_x)\n\n\nAfter we have prepared the data test, we now can proceed to predict the label of each image using our CNN model.\n\n\n# # for muli-classes, using %>% k_argmax(), for binary classes, using %>% `>`(0.5) %>% k_cast(\"int32\")\n# pred_test_m <- predict(model, test_x) %>% k_argmax()\n# \n# # Convert encoding to label\n# decode <- function(x){\n#   case_when(x == 0 ~ \"Kiwi\",\n#             x == 1 ~ \"Banana\",\n#             x == 2 ~ \"Apricot\")\n# }\n# \n# pred_test <- sapply(pred_test_m, decode) \n# pred_test\n\n\nNow let’s evaluate using confusion matrix to check the accuracy and other metrics.\n\n\n# confusionMatrix(as.factor(pred_test), \n#                 as.factor(val_data$class))\n\n\n2.2.4 Model Tuning\nlet’s check our model. We actually extract information from an 2D image array. The first layer only extract the general features of our image and then being downsampled using the max pooling layer. Therefore, we can stacks more layers in the model for capturing more information like this.\n\n\n# # constructing model\n# model_tuned <- keras_model_sequential() %>% \n#   # Convolution Layer\n#   layer_conv_2d(filters = 32,\n#                 kernel_size = c(3,3),\n#                 padding = \"same\",\n#                 activation = \"relu\",\n#                 input_shape = c(target_size, 3)) %>% \n#   # Convolution Layer\n#   layer_conv_2d(filters = 32,\n#                 kernel_size = c(3,3),\n#                 padding = \"same\",\n#                 activation = \"relu\",\n#                 input_shape = c(target_size, 3)) %>% \n#   # Max Pooling Layer\n#   layer_max_pooling_2d(pool_size = c(2,2)) %>% \n#   # Flattening Layer\n#   layer_flatten() %>% \n#   # Dense Layer\n#   layer_dense(units = 16, activation = \"relu\") %>% \n#   # Output Layer\n#   layer_dense(units = output_n,\n#               activation = \"softmax\",\n#               name = \"Output\")\n#   \n# model_tuned\n# \n# # Fit data into model\n# model_tuned %>% \n#   compile(\n#     loss = \"categorical_crossentropy\",\n#     optimizer = optimizer_adam(lr = 0.001),\n#     metrics = \"accuracy\"\n#   )\n# \n# history <- model_tuned %>% \n#   fit(\n#   # training data\n#   train_image_array_gen,\n#   # training epoch\n#   steps_per_epoch = as.integer(train_samples / batch_size), \n#   epochs = 30, \n#   # validation data\n#   validation_data = val_image_array_gen,\n#   validation_steps = as.integer(valid_samples / batch_size))\n# \n# plot(history)\n# \n# # evaluating model\n# pred_test_t <- predict(model_tuned, test_x) %>% k_argmax()\n# # Convert encoding to label\n# decode <- function(x){\n#   case_when(x == 0 ~ \"Kiwi\",\n#             x == 1 ~ \"Banana\",\n#             x == 2 ~ \"Apricot\")\n# }\n# \n# pred_test <- sapply(pred_test_t, decode) \n# pred_test\n# \n# confusionMatrix(as.factor(pred_test), \n#                 as.factor(val_data$class))\n\n\n2.3 Prediction with CNN model\nAfter we have trained the tuned-model and satisfied with the model performance on the validation dataset, we will do prediction/classification using new data test.\n2.3.1 Preparing data\nAs the previous section for preparing training data, let’s do the same steps for test data.\n\n\n# # List of test images\n# test_folder_list <- list.files(\"fruits-360/Test/\") # image folders\n# test_folder_list\n# test_images_list <- c(\"Kiwi\", \"Banana\", \"Apricot\") # list of images\n# test_images_path <- paste0(\"fruits-360/Test/\", test_images_list, \"/\")\n# test_images_path\n# # get file name\n# test_file_name <- map(test_images_path, function(x) paste0(x, list.files(x))) %>% unlist()\n# length(test_file_name)\n# \n# # Uniform and rescale \n# # image size to scale down to (original images are 100 x 100 px)\n# img_width <- 20\n# img_height <- 20\n# target_size <- c(img_width, img_height)\n# # RGB = 3 channels\n# channels <- 3\n# # define batch size\n# batch_size <- 32\n# \n# # Test image Generator\n# test_data_gen <- \n#   image_data_generator(rescale = 1/255, # Scaling pixel value\n#                        horizontal_flip = T, # Flip image horizontally\n#                        vertical_flip = T, # Flip image vertically \n#                        rotation_range = 45, # Rotate image from 0 to 45 degrees\n#                        zoom_range = 0.25)# Zoom in or zoom out range\n# \n# test_image_array_gen <- \n#   flow_images_from_directory(directory = \"fruits-360/Test/\",\n#                              classes = test_images_list,\n#                              target_size = target_size, \n#                              color_mode = \"rgb\", \n#                              batch_size = batch_size ,\n#                              seed = 123,\n#                              generator = test_data_gen)\n# \n# test_data <- data.frame(file_name = paste0(\"fruits-360/Test/\", test_image_array_gen$filenames)) %>% \n#   mutate(class = str_extract(file_name, \"Kiwi|Banana|Apricot\")) \n# head(test_data, 10)\n# \n# # Check dimension of testing data set\n# test1 <- image_prep(test_data$file_name)\n# dim(test1)\n\n\nAfter we have prepared the data test, we now can proceed to predict the label of each image using our CNN model.\n\n\n# pred_test1_t <- predict(model_tuned, test1) %>% k_argmax()\n# head(pred_test1_t, 10)\n# \n# # Convert encoding to label\n# decode <- function(x){\n#   case_when(x == 0 ~ \"Kiwi\",\n#             x == 1 ~ \"Banana\",\n#             x == 2 ~ \"Apricot\")\n# }\n# \n# pred_test2 <- sapply(pred_test1_t, decode) \n# pred_test2\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-25T10:38:04+08:00",
    "input_file": {}
  },
  {
    "path": "models/2023-10-15-advanced-machine-learning/",
    "title": "Lesson 2: Advanced Machine Learning",
    "description": "Unlike classic mechine learning in which features are extracted manually, deep learning (DL) can automatically extract features from data. When the amount of data is increased, DL is an effective algorithm, and gives better performance than classic machine larning. This section will guide you how to build DL with your own data.",
    "author": [],
    "date": "2023-11-23",
    "categories": [],
    "contents": "\n\nContents\n1. Running DL with Rstudio\n1.1 Some R packages for Networks\n1.2 Setting R env for running keras\n\n2. Basic conception of DL models\n2.1 Neutral network architechture\n2.2 Building neutral networks\n2.3 Compiling neutral networks\n2.4 Improving network models\n\n3. Convolutional Neural Network\n3.1 The architecture of CNN\n3.2 Basic conception of CNN\n3.3 Defining and training CNN Model\n\n4. The application of DL to ecology\n\nDL is a subfield of machine learning that works with algorithms inspired by the structure and functions of human brain called “neural networks”. It has 3 types of neural networks: Artificial Neural Network (ANN), Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN), and has a large number of potential applications in ecology, such as image recognition, analysis of acoustic signals, or any other tasks.\n1. Running DL with Rstudio\n1.1 Some R packages for Networks\nThere are some packages that can fit basic neural networks. For example, the nnet package can fit feed-forward neural networks with one hidden layer. The neuralnet package fits neural networks with multiple hidden layers using back-propagation. The RSNNS package makes many model components for training a wide variety of models. The deepnet package provides a number of tools for deep learning, as well as allows for different activation functions and the use of dropout for regularization.\nThe h2o (https://www.h2o.ai/) is an excellent, general machine learning framework written in Java, and has an API that allows you to use it from R. However, most deep learning practitioners prefer other deep learning libraries, such as TensorFlow, CNTK, and MXNet. Keras is actually a frontend abstraction for other deep learning libraries, and can use TensorFlow in the background.\n1.2 Setting R env for running keras\nKeras is a high-level, open source, deep learning framework that emphasizes iterative and fast development. Keras models can be deployed on practically any environment, such as a web server, iOS, Android, a browser, or the Raspberry Pi. Now using Tensorflow’s Keras is recommended over the standalone keras package.\nTo learn more about using Keras in R, go to https://keras.rstudio.com; this link will also has more examples of R and Keras, as well as a handy Keras cheat sheet that gives a thorough reference to all of the functionality of the Keras package. Please pay much attention to the link of the different languages, and understand it for operating on them.\n\nFor running keras in Rstudio, we should make sure which python that we use, and create a virtual environment. Here is the code for creating a virtual python env. named “r-reticulate”.\n\n\n# library(reticulate)\n# py_config() # for environment information\n# install_python # for linux OS \n# install_miniconda() # for win OS\n\n\nThen we install the python packages, such as tensorflow and keras using py_install() from reticulate or install them with pip the terminal in Rstudio. After that, we install the R packages of keras and tensorflow.\n\n\n\n2. Basic conception of DL models\n2.1 Neutral network architechture\nANN is a forward propagation network that is consists of artificial neurons together and grouped in different levels known as layers. These are the 3 layers applied in structured artificial neural networks: Input Layer, Hidden Layer and Output Layer. This network is like this:\n\nThe architecture of ANN is different from that of RNN, which is used in natural language and speech recognition by processing sequential data. RNN can process different types of data at various moments in time. Its Architecture is like this.\n\n2.2 Building neutral networks\nA Keras model is the Sequential or functional class. Here we focus on the sequential model. For more details, see the site\nNeural networks are composed of neurons, each of which individually performs only a simple computation (see the box1).\n\n\n\nBox 1: Basic knowledge of neural networks\n\n\nSingle Input\nA larger network starts with a single neuron model as a baseline. Single neuron models are linear models. For example, training a model with ‘sugars’ as input and ‘calories’ as output in the dataset of cereals like this.\n\n\n\nThe formula for this neuron would be \\(y=wx+b\\)\nMultiple Inputs\nIf we wanted to expand our model to include things like fiber or protein content, we can just add more input connections to the neuron, and multiply each input to its connection weight and then add them all together.\n\n\n\nThe formula for this neuron would be \\(y=w_{0}x_{0}+w_{1}x_{1}+w_{2}x_{2}+b\\).\n\n\n\n\n\nWe can define the sequential model for allowing a neural network to “learns” by modifying the weight of each feature.\n\n\n# Create a network with one linear unit\n#model = keras.Sequential([\n#    layers.Dense(units=1, input_shape=[3])\n#])\n\n\nThe first argument, units, defining how many outputs we want. In the case just predicting ‘calories’, so units=1. The second argument, input_shape, tell Keras the dimensions of the inputs. Setting input_shape=[3] ensures the model to accept three features as input (‘sugars’, ‘fiber’, and ‘protein’). This model is now ready to be fit to training data!\nWe can combine and modify the single units to model complex relationships that DL work for. Let’s see the box2 to understand how we stack layers to get complex data transformations. We have some nonlinearity with activation functions (see the box2).\n\n\n\nBox 2: Constructing neural networks\n\n\nLayers\nWe collect together linear units, and then get a dense layer through a common set of inputs.\n\n\n\nEach layer in a neural network performs relatively simple transformation. Through a deep stack of layers, a neural network can transform its inputs in more and more complex ways.\nActivation Function\nWe stack layers inside a Sequential model, and add an activation function to each hidden layer, then a network has an ability to learn more complex (non-linear) relationships in data. Without activation functions, neural networks only learn linear relationships (lines and planes). For details on how to choose an activation function of a hidden Layer, see the website.\nThe common activation function is the rectifier function max(0,x).\n\n\n\nWhen we attach the rectifier to a linear unit, we get a rectified linear unit or ReLU. Applying a ReLU activation to a linear unit means the output becomes \\(max(0, w * x + b)\\). For the following network, its hidden layers’ activation function is ReLU.\n\n\n\nFor an output layer, if it is a linear unit with no activation function, that makes this network appropriate to a regression task, where we can predict some arbitrary numeric value. Other tasks (like classification) might require an activation function, such as softmax, on the output.\n\n\n\n\n\n\nThe Sequential model we’ve been using will connect together a list of layers in order from first layer (input) to last layer (output). This creates a model in the above figure.\n\n\n#model = keras.Sequential([\n    # the first hidden layer\n    #   layers.Dense(units=4, activation='relu', input_shape=[2]),\n\n    # the second hidden layer\n    #   layers.Dense(units=3, activation='relu'),\n\n    # the linear output layer\n    #   layers.Dense(units=1),\n##])\n\n\nAs for the dataset of cereals, we choose a three-layer network with over 1500 neurons. It can be capable of learning fairly complex relationships in the data.\n\n\n#model = keras.Sequential([\n#    layers.Dense(512, activation='relu', input_shape=[11]),\n#    layers.Dense(512, activation='relu'),\n#    layers.Dense(512, activation='relu'),\n#    layers.Dense(1),\n#])\n\n\n2.3 Compiling neutral networks\nTraining a ANN model is to adjust its weights so that it can transform the features (inputs) into the target (output). If successfully train a network, its weights must represent some relationships between those features and that target (see the box3).\n\n\n\nBox 3: Compiling neural networks\n\n\nThe Loss Function\nThe loss function measures the disparity between the target’s true value and the predicted value by models. A common loss function for regression is the mean absolute error (MAE), i.e., abs(y_true - y_pred). Other loss functions for regression problems are the mean-squared error (MSE) or the Huber loss (available in Keras). For classification, the common function is to measure the accuracy.\nOptimizer, minibatch and epoch\nAll of the optimization algorithms used in DL belong to a family called stochastic gradient descent (SGD). They are iterative algorithms that train a network in steps like this:\nSample training data and run it to pass a network to make predictions.\nMeasure the loss between the predicted and true values.\nFinally, adjust the weights in a direction that makes the loss smaller.\nAdam is an SGD algorithm that has an adaptive learning rate that makes it suitable for most problems without any parameter tuning. Adam is a great general-purpose optimizer. RMSpro is a good one for regression problems.\nEach iteration sample of training data is called minibatch or batch, while a complete round of the training data is called an epoch. The number of epochs you train is how many times the network will see each training example.\nLearning Rate and Batch Size\nThe learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds.\n\n\n\n\n\n\nAfter understanding these, now we can compile the squential model with the optimizer and loss function.\n\n\n# for regression\n# model.compile(\n#    optimizer='adam',\n#    loss='mae')\n\n\nNow we’re ready to start training! We tell Keras to feed the optimizer 256 rows of the training data at a time (the batch_size) and to do that 10 times all the way through the dataset (the epochs).\n\n\n#history = model.fit(\n#    X_train, y_train,\n#    validation_data=(X_valid, y_valid),\n#    batch_size=256,\n#    epochs=10)\n\n\n2.4 Improving network models\nCapacity: A model capacity (the size and complexity of the patterns it is able to learn) is determined by its architecture, in which how many neurons it has and how the neurons are connected together. If the network is underfitting data, we should try increasing its capacity.\nWe can increase the capacity of a network either by making it wider (more units to layers) or by making it deeper (more layers). The wider networks have an easier time learning more linear relationships, while the deeper networks prefer more nonlinear ones. Which is better just depends on the dataset.\n\n\n#model = keras.Sequential([\n#    layers.Dense(16, activation='relu'),\n#    layers.Dense(1)])\n\n#wider = keras.Sequential([\n#    layers.Dense(32, activation='relu'),\n#    layers.Dense(1)])\n\n#deeper = keras.Sequential([\n#    layers.Dense(16, activation='relu'),\n#    layers.Dense(16, activation='relu'),\n#    layers.Dense(1)])\n\n\nEarly Stopping: Keras keeps a history of the training and validation loss over the epochs. We’ll examine the learning curves for evidence of underfitting and overfitting for correcting it (see the box4).\n\n\n\nBox 4: Improving neural networks\n\n\nLearning Curves\nWhen we train a model we’ve been plotting the loss on the training set epoch by epoch. To this we’ll add a plot the validation data too. These plots we call the learning curves.\n\n\nWhen a model learns signal both curves go down, but when it learns noise a gap is created in the curves. You should create models that minimize the size of the gap.\nEarly Stopping\nWhen a model is too eagerly learning noise, the validation loss may start to increase during training. To prevent this, we can simply stop the training whenever it seems the validation loss isn’t decreasing anymore. Interrupting the training this way is called early stopping.\n\n\nOnce we detect that the validation loss is starting to rise again, we can reset the weights back to where the minimum occured. This ensures that the model won’t continue to learn noise and overfit the data.\n\n\n\n\n\n\nIn Keras, we include early stopping in our training through a callback. A callback is just a function you want run every so often while the network trains. The early stopping callback will run after every epoch.\n\n\n#early_stopping = callbacks.EarlyStopping(\n#    min_delta=0.001, # minimium amount of change to count as an improvement\n#    patience=20, # how many epochs to wait before stopping\n#    restore_best_weights=True,\n#)\n\n\nSpecial layers: There are some special layers that do not contain any neurons, but that they are added prevent overfitting and stabilize training. See the box5\n\n\n\nBox 5: Adding special layers\n\n\nDropout\nWe randomly drop out some fraction of a layer’s input units every step of training, making it much harder for the network to learn those spurious patterns in the training data. Instead, it has to search for broad, general patterns, whose weight patterns tend to be more robust. If you’re familiar with random forests as an ensemble of decision trees, it’s the same idea.\n\n\nBatch Normalization\nIt’s generally a good idea to put all of data on a common scale. A good practise for normalizing inside the network! This special kind of layer can do this. A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then putting the data on a new scale with two trainable rescaling parameters. A batch normalization can correct training that is slow or unstable.\n\n\n\n\n\n\nIn Keras, the dropout rate argument rate defines what percentage of the input units to shut off. Put the Dropout layer just before the layer you want the dropout applied to.\n\n\n#keras.Sequential([\n#    # ...\n#    layers.Dropout(rate=0.3), # apply 30% dropout to the next layer\n#    layers.Dense(16),\n#    # ...\n#])\n\n\nAs for batch normalization layers, you can put it after a layer…\n\n\n#layers.Dense(16, activation='relu'),\n#layers.BatchNormalization(),\n\n\n… or between a layer and its activation function:\n\n\n#layers.Dense(16),\n#layers.BatchNormalization(),\n#layers.Activation('relu'),\n\n\nAnd if you add it as the first layer of your network it can act as a kind of adaptive preprocessor, standing in for something like Sci-Kit Learn’s StandardScaler.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3. Convolutional Neural Network\n3.1 The architecture of CNN\nCNN is designed for image recognition. They can capture the spatial characteristics of an image. They work in a way like this.\n\nThe architectures contain three kinds of layers: convolutional layer, pooling layer, and fully-connected layer. For details about how each layer works, see here, as well as others.\n3.2 Basic conception of CNN\nThere is much information about CNN conception. You can watch this video.\n3.2.1 Convolution on an image\nLet’s take a look this imagine. It has 8x8 pixels, the values of which range from 0 to 255, representing black and white, respectively. For convolution on the imagine, see the box1.\n\n\n\nBox 6: Convoluting on an imagine\n\n\nconvolution operation: Convolution is a fundamental operation in image processing because it is a highly effective way to extract features and filter noise from images. The convolution operation involves sliding the filter over the image, computing the dot product between the filter and the corresponding image pixels at each position, and assigning the result to the central pixel of the filter.\n\n\n\nYou can use multiple filters extract pixel values for getting multiple feature maps, other than only one.\n\n\n\nIf convolution on a color image (i.e., RGB, three channels), you should select a filter with same channels to combined their values for a feature map like this.\n\n\n\nFor the convolution operation, you should notice the padding and stride. Padding and stride are two techniques used to improve convolutions operations and make the more efficient.\npadding: CNNs commonly use convolution filters with odd height and width values, such as 1, 3, 5, or 7 to avoid undesirable shrinkage of original imagines.\n\n\n\nstride: The amount of movement between applications of the filter to the input image is referred to as the stride. The stride of the filter on the input image can be used to downsample the size of the output feature map.\n\n\n\n\n\n\n\n\n\n3.2.2 Pooling operation\nAfter extracting features with filters, you can reduce data dimension and emphasize features by pooling layers (see the box7).\n\n\n\nBox 7: Pooling on a feature map\n\n\npooling operation: The pooling operation involves sliding a two-dimensional filter over each feature map and summarizing the features lying within the region covered by the filter. This makes the model more robust to variations in the position of the features in the input image.\n\n\n\n\n\n\n\n\n\n3.2.3 Flattening and activation functions\nAfter convolution and pooling steps, we are literally going to flatten our pooled feature map into a column like in the image below to feed ANN later on.\n\n\n\nBox 8: Flattening for a columon\n\n\nBasically, flattening just takes the numbers row by row, and put them into this one long column.\n\n\n\nActivation function: First note that the hidden and output layers usually have activation functions such as sigmoid, tanh, ReLU, identity, etc. Activation functions in hidden layers are usually nonlinear, e.g. ReLU (rectified linear unit), which is a piecewise linear function that introduces the most simple nonlinearity. ReLU Those in an output layer used are linear function g(z)=z, such as softmax.\n\n\n\n\n\n\n\n\n\n3.3 Defining and training CNN Model\n3.3.1 Designing CNN model\nNow there have some pictures of animals with dogs, cats and loans. There are many architectures for image classification, one of the most popular being convolutional neural networks (CNNs). CNNs are especially effective at image classification because they are able to automatically learn the spatial hierarchies of features, such as edges, textures, and shapes, which are important for recognizing objects in images.\nWe define the CNN architecture using the Keras library. The model will consist of several convolutional layers followed by max pooling layers, and a fully connected layer with a softmax activation function (see the box9).\n\n\n\nBox 9: Designing a CNN model\n\n\nHere is a CNN architecture for the classification of animal picture.\n\n\n\n\n\n\n\n\n\n\n3.3.2 Defining and training CNN model\nFor the code, please visit the website\n4. The application of DL to ecology\nPrediction of farmland fertility: The networks receive images from satellites like LSAT and can use this information to classify lands based on their level of cultivation. Consequently, this data can be used for making predictions about the fertility level of the grounds or developing a strategy for the optimal use of farmland.\nTopological analysis: A key aim of ecosystem analysis is to determine how stable an ecosystem is, and how it might react to environmental change. Therefore, a large amount of research has been concerned with determining the stability of ecosystems using theoretical techniques. These ideas were introduced by Elton, who described a community matrix M of size n × n for which the (i, j)-th entry represents the impact that a species h has on another species j around an equilibrium point of some unobserved dynamical system. This allowed stability analysis techniques from dynamical systems literature to be used, where a system is considered stable to small perturbations if the eigenvalues λ of M all have negative real parts. The original study was concerned with random community matrices, but the same ideas have since been refined and applied to realistic community matrices. This area of research has been used to contribute to the ongoing debate about the relationship between biodiversity and ecosystem stability.\nSpecies importance metrics: It is often of interest to ecologists which species are the most influential within their ecosystem. These species are known as keystone species. This has consequences in conservation, since the loss of a keystone species could lead to the collapse of an entire ecosystem. Traditionally, keystone species were identified from their natural history, but it is often difficult to experimentally verify this. This has led to the introduction of graph-theoretic species importance metrics which aim to discover keystone species by considering the topology of the network.\nNetwork structure prediction: Machine learning algorithms are able to discover patterns in data, and use those patterns to make predictions about previously unseen data samples. This had led to widespread use of ML in many different domains to automate laborious tasks which are expensive in terms of both time and money. Ecological networks are generally constructed in such ways, and therefore it is highly desirable to develop machine learning algorithms which automate the process. In addition,mMissing link prediction - as the name suggests - is the problem of predicting which links are missing from a network. There is a wealth of literature in network analysis concerning this problem and its applications to a range of complex networks. Recently, attention has turned to the problem of link prediction in the context of ecological networks. This has immediately useful practical applications in ecology. The randomness present in an ecosystem makes it unlikely that all actual interactions are observed when collecting data to construct ecological networks. Therefore, the use of link prediction algorithms could help ecologists to discover actual unobserved interactions when used in conjunction with traditional ecological network construction methods.\n\n\n\n",
    "preview": "models/2023-10-15-advanced-machine-learning/multi_layer.png",
    "last_modified": "2023-11-23T16:13:20+08:00",
    "input_file": {}
  },
  {
    "path": "models/2023-10-17-one-shot-learning-with-r/",
    "title": "Lesson 3: One Shot Learning with R",
    "description": "One-shot learning is a machine learning algorithm using a limited amount of training set data to compare the similarities and differences between two images. This section will introduce you to build one-shot learning model with R keras.",
    "author": [],
    "date": "2023-11-13",
    "categories": [],
    "contents": "\n\nContents\n1. Learning algorithm with limited data\n1.1 Zero-Shot Learning\n1.2 One-Shot Learning\n1.3 Few-Shot Learning\n\n2. Building a one-shot learning model\n2.1 Loading and spliting data\n2.2 Building a self define generator\n2.3 Building and training a model\n2.4 Verifing with test data\n\n3. Import things of deep learning\n3.1 Difference between ANN and CNN\n3.2 keras generators, Callback and Tensorboard\n\n\n1. Learning algorithm with limited data\nTraditional machine learning algorithms, including classic and advanced, require large amounts of labelled data for training. However, in real world, obtaining such data can be expensive, time-consuming, or even impossible. Zero-, one- and few-shot learning algorithms are designed to address the challenge of learning from limited labelled data. Each approach refers to the number of examples available for training and the level of generalization required.\n1.1 Zero-Shot Learning\nThe model is designed to train with no examples for certain classes. It has the two following characteristics.\nGeneralization: The goal is to classify instances from classes not seen during training. The model learns to generalize its knowledge to unseen classes.\nCommon Technique: Transfer learning, semantic embeddings, and attribute-based methods are often used in zero-shot learning. These approaches leverage auxiliary information about classes to perform classification.\n1.2 One-Shot Learning\nThe model is designed to train with just one example per class. It has the following charateristics.\nGeneralization: The goal is to learn a representation that can differentiate between classes based on a single example.\nCommon Technique: Siamese, matching, and prototypical networks are often used in one-shot learning. These architectures focus on learning relationships between examples.\n1.3 Few-Shot Learning\nThis is a more general term that encompasses both one-shot learning and scenarios where a small number of examples (more than one) per class are available. It has the following charateristics.\nGeneralization: The goal is to enable the model to classify instances from classes with very few training examples.\nCommon Techniques: These methods include variants of one-shot learning models and meta-learning strategies. These techniques teach the ability to adapt quickly to new tasks or classes.\n2. Building a one-shot learning model\nWe can learn the basics of Keras by walking through a simple example of the MNIST dataset. It consists of 28 x 28 grayscale images of handwritten digits. The dataset includes labels for each image, telling us which digit it is. For machine learning, such as ANN, with the dataset, please visit the site and another one\nHere we build one-shot learning model with a dataset of mnist number. The code modified according to this page.\n2.1 Loading and spliting data\nFirst, we load and split the data into training, validation, and testing sets using the Keras and abind libraries in R. Here’s the code:\n\n\n# # the first three lines should be run for checking the python env\n# # rm(list = ls())\n# # library(reticulate) # loading virtual environment for project\n# # py_config() # By default, the env is ~/.virtualenvs/r-reticulate\n# library(reticulate)\n# library(keras) # loading the package for data and modelling\n# library(abind) # operating multidimensional arrays, which are often expressed any image.\n# # mnist <- dataset_mnist() # load the dataset from the keras package\n# # np <- import(numpy)\n# # mnist <- np$load(\"/home/tank/Desktop/ecodatasci/images/mnist.npz\") # full path\n# mnist <- dataset_mnist(\"/home/tank/Desktop/ecodatasci/images/mnist.npz\") # full path\n\n\nLet’s explore the format of the dataset before training the model.\n\n\n# str(mnist)\n\n\nWe have four arrays: train_images and train_labels, test_images and test_labels. We can separate each array from each other by performing the following code.\n\n\n# train_images1 <- mnist$train$x\n# train_labels1 <- mnist$train$y\n# test_images  <- mnist$test$x \n# test_labels  <- mnist$test$y\n\n\ntrain_images1 contain 60,000 images. Each image shows 28 x 28 pixels, which values ranging between 0 and 255. train_labels1 with 60000 elements are arrays of integers, ranging from 0 to 9.\n\n\n# dim(train_images1)\n# dim(train_labels1)\n# train_labels1[1:20]\n\n\ntest_images contain 10000 images, which is used to evaluate how accurately the network learned. Each image is represented as 28 x 28 pixels.\n\n\n# dim(test_images)\n# dim(test_labels)\n\n\nUsing Keras, we can set aside a portion of the training data for validation. This helps monitor model performance and detect overfitting. At the point, we have 48,000 training. Additional 12,000 data are validation.\n\n\n# set.seed(9478)\n# val_idx      <- sample(1:nrow(train_images1),\n#             size = ceiling(0.2*nrow(train_images1)), # the smallest integer > the value\n#                        replace = F)\n# val_images   <- train_images1[val_idx,,] # the third comma means the channel of accolor image\n# val_labels   <- train_labels1[val_idx]\n# train_images <- train_images1[-val_idx,,]\n# train_labels <- train_labels1[-val_idx]\n\n\nThe pixel values fall in the range of 0 to 255. We scale pixel values to a range of 0 (black) to 1 (white) before feeding to model. You can visualize the mnist’s data as blow. The title is its label and image is its array.\n\n\n# par(mar = c(0,0,4,0)) # seting the bottom, left, top and right margins of a plot region.\n# i <- 1\n# plot(as.raster(train_images[i,,]/255)) # i and two comma denote 3-dimensional array of an image\n# title(train_labels[i])\n\n\n2.2 Building a self define generator\nThis step includes: setting parameters, building self define generator, building self define layer, building self define backend function.\nParameter Setting\nIn any machine learning model, you would typically need to set various parameters for configuring the model.\n\n\n# num_classes  <- 10 # only number : 0,1,2,3,4,5,6,7,8,9 \n# shape_size   <- 28 # mnist shape (28,28)\n# train_batch  <- 20 \n# val_batch    <- 20 \n# test_batch   <- 1\n\n\nWe create train_data_list and val_data_list to hold the training and validation data.The code is a general template for organizing and reshaping data for using Keras. It can be adapted to other datasets with modifications to class labels, images, and dimensions.\n\n\n# train_data_list    <- list() \n# grp_kind     <- sort(unique(train_labels)) # # get unique class labels from data \n#   for( grp_idx in 1:length(grp_kind)) { # iterate over each class, grp_idx = 1,  \n#     label    <- grp_kind[grp_idx]  # get label of the current class  \n#     tmp_images <- train_images[train_labels==label,,] # get images of the current class \n#     tmp_images     <- array(tmp_images, dim = c(dim(tmp_images), 1))  # reshape to 4D (batch_size, height, width, channels) for keras's generator.  \n#     train_data_list[[grp_idx]] <- list( data  = tmp_images, # x                   \n#                                         label = train_labels[train_labels==label])# y\n#   }  \n# \n# val_data_list      <- list() \n# grp_kind     <- sort( unique( val_labels ) )   \n#   for( grp_idx in 1:length(grp_kind) ) { # grp_idx = 1     \n#     label                      <- grp_kind[grp_idx]     \n#     tmp_images                 <- val_images[val_labels==label,,]     \n#     tmp_images                 <- array( tmp_images , dim = c( dim(tmp_images) , 1) )     \n#     val_data_list[[grp_idx]]   <- list( data  = tmp_images ,                                         \n#                                         label = val_labels[val_labels==label]      \n#                                       )   \n#   }\n\n\nTo determine the exact number of samples for each class, you can inspect the length of the list within the train_data_list. That is, using length(train_data_list[[1]]$lebal) gives you the number of samples for the first class. You can repeat this for each class to get the number of samples for each class.\n\n\n# # Initialize a variable to store the sum\n# total_sum <- 0\n# \n# # Use a for loop to sum values from 1 to 10\n# for (i in 1:10) {\n#   counts <- length(train_data_list[[i]]$label)\n#   total_sum <- total_sum + counts\n# }\n# \n# # Print the total sum\n# cat(\"The sum of values from 1 to 10 is:\", total_sum, \"\\n\")\n\n\nSelf a define generator\nData augmentation is a technique to increase the diversity of your training dataset by applying various transformations to images, and improves the model’s generalization and robustness. Here we first build generators with image_data_generator for each mnist’s number to generalize the number, and then collect all numbers’ generators into a list.\n\n\n# train_datagen = image_data_generator( # data argumentation\n#   rescale = 1/255          , # scaling pixel values to the range [0, 1]\n#   rotation_range = 5       , # randomly rotate images by up to 5 degrees\n#   width_shift_range = 0.1  , # shift by up to 10% of the image's width\n#   height_shift_range = 0.05, # shift by up to 5% of the image's height\n#   #shear_range = 0.1,\n#   zoom_range = 0.1         , # randomly zoom in/out by up to 10%\n#   horizontal_flip = FALSE  , # disable horizontal flipping\n#   vertical_flip = FALSE    , # disable vertical flipping\n#   fill_mode = \"constant\"     # fill mode for new pixels (constant value)\n# )\n\n\nWe will train the model to differentiate between digits of different classes. For example, digit 0 needs to be differentiated from the rest of the digits (1 through 9). To carry this out, we will select N random images from class A (for example, for digit 0) and pair them with N random images from another class B (for example, for digit 1). The following code prepares data generators for individual classes, making it easier to handle and process data during training and validation. It is especially useful for one-shot learning tasks.\n\n\n# train_0_generator <- flow_images_from_data( # for 0 number\n#   x = train_data_list[[1]]$data  ,   \n#   y = train_data_list[[1]]$label ,   \n#   train_datagen                  , # data augmentation configuration  \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1  # for one-shot learning, each batch contains one pair of images \n# )\n# \n# train_1_generator <- flow_images_from_data( # for 1 number  \n#   x = train_data_list[[2]]$data  ,   \n#   y = train_data_list[[2]]$label ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# train_2_generator <- flow_images_from_data( # for 2 number  \n#   x = train_data_list[[3]]$data  ,   \n#   y = train_data_list[[3]]$label ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# train_3_generator <- flow_images_from_data( # for 3 number  \n#   x = train_data_list[[4]]$data  ,   \n#   y = train_data_list[[4]]$label ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# train_4_generator <- flow_images_from_data( # for 4 number  \n#   x = train_data_list[[5]]$data  ,   \n#   y = train_data_list[[5]]$label ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# train_5_generator <- flow_images_from_data( # for 5 number  \n#   x = train_data_list[[6]]$data  ,   \n#   y = train_data_list[[6]]$label ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# train_6_generator <- flow_images_from_data( # for 6 number  \n#   x = train_data_list[[7]]$data  ,   \n#   y = train_data_list[[7]]$label ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# train_7_generator <- flow_images_from_data( # for 7 number  \n#   x = train_data_list[[8]]$data  ,   \n#   y = train_data_list[[8]]$label ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# train_8_generator <- flow_images_from_data( # for 8 number   \n#   x = train_data_list[[9]]$data  ,   \n#   y = train_data_list[[9]]$label ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# train_9_generator <- flow_images_from_data( # for 9 number   \n#   x = train_data_list[[10]]$data ,   \n#   y = train_data_list[[10]]$label,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# val_test_datagen = image_data_generator(\n#   rescale = 1/255\n# )\n# \n# val_0_generator <- flow_images_from_data( # for 0 number  \n#   x = val_data_list[[1]]$data    ,   \n#   y = val_data_list[[1]]$label   ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# val_1_generator <- flow_images_from_data( # for 1 number  \n#   x = val_data_list[[2]]$data    ,   \n#   y = val_data_list[[2]]$label   ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# val_2_generator <- flow_images_from_data( # for 2 number   \n#   x = val_data_list[[3]]$data    ,   \n#   y = val_data_list[[3]]$label   ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# val_3_generator <- flow_images_from_data( # for 3 number   \n#   x = val_data_list[[4]]$data    ,   \n#   y = val_data_list[[4]]$label   ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# val_4_generator <- flow_images_from_data( # for 4 number   \n#   x = val_data_list[[5]]$data    ,   \n#   y = val_data_list[[5]]$label   ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# val_5_generator <- flow_images_from_data( # for 5 number  \n#   x = val_data_list[[6]]$data    ,   \n#   y = val_data_list[[6]]$label   ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# val_6_generator <- flow_images_from_data( # for 6 number  \n#   x = val_data_list[[7]]$data    ,   \n#   y = val_data_list[[7]]$label   ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# val_7_generator <- flow_images_from_data( # for 7 number  \n#   x = val_data_list[[8]]$data    ,   \n#   y = val_data_list[[8]]$label   ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# val_8_generator <- flow_images_from_data( # for 8 number  \n#   x = val_data_list[[9]]$data    ,   \n#   y = val_data_list[[9]]$label   ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n# \n# val_9_generator <- flow_images_from_data( # for 9 number  \n#   x = val_data_list[[10]]$data   ,   \n#   y = val_data_list[[10]]$label  ,   \n#   train_datagen                  ,   \n#   shuffle = TRUE                 ,   \n#   seed = 9487                    ,   \n#   batch_size = 1 \n# )\n\n\nThe following code is responsible for generating pairs of data samples for one-shot learning, where half of the pairs are from the same class (positive) and the other half are from different classes (negative). It does so by selecting classes and fetching data samples from their corresponding generators. The following code is to create a data generator that generates pairs of data samples and their associated similarity labels for training a siamese neural network.\n\n\n# # creating sample pairs from the same and different classes\n# train_generator_list <- list(\n#   train_0_generator ,\n#   train_1_generator ,\n#   train_2_generator ,\n#   train_3_generator ,\n#   train_4_generator ,\n#   train_5_generator ,\n#   train_6_generator ,\n#   train_7_generator ,\n#   train_8_generator ,\n#   train_9_generator \n# )\n# \n# val_generator_list <- list(\n#   val_0_generator   ,\n#   val_1_generator   ,\n#   val_2_generator   ,\n#   val_3_generator   ,\n#   val_4_generator   ,\n#   val_5_generator   ,\n#   val_6_generator   ,\n#   val_7_generator   ,\n#   val_8_generator   ,\n#   val_9_generator \n# )\n# \n# join_generator <- function(train_generator_list, batch) { \n#   function() { \n#     batch_left  <- NULL \n#     batch_right <- NULL \n#     similarity  <- NULL \n#       for( i in seq_len(batch)) { # i = 1 \n#           if( i <= ceiling(batch/2)) { # front half\n#             grp_same    <- sample(seq_len(num_classes), 1) # randomly sampling one class\n#             # combining array vertically\n#             batch_left  <- abind(batch_left, generator_next(generator_list[[grp_same]])[[1]], along = 1) \n#             batch_right <- abind(batch_right, generator_next(generator_list[[grp_same]])[[1]], along = 1)\n#             similarity  <- c(similarity, 1) # 1 : from the same number\n#             # par(mar = c(0,0,4,0))\n#             # plot(as.raster(batch_left[21,,,]))\n#             # title(batch_left[[2]])\n#           } else { # after half \n#             grp_diff    <- sort(sample(seq_len(num_classes) , 2)) \n#             batch_left  <- abind(batch_left, generator_next(generator_list[[grp_diff[1]]])[[1]], along = 1) \n#             batch_right <- abind(batch_right, generator_next(generator_list[[grp_diff[2]]])[[1]], along = 1) \n#             similarity  <- c( similarity , 0 ) # 0 : from the differnet number\n#           } \n#       } \n#     return(list(list(batch_left, batch_right), similarity)) \n#   } \n# }\n# \n# train_join_generator   <- join_generator( train_generator_list, train_batch )\n# val_join_generator     <- join_generator( val_generator_list  , val_batch   )\n\n\n2.3 Building and training a model\nthe siamese architecture\nIn one-shot learning, the model is trained to recognize new objects or classes with only a single example per class. There are several architectures of one-shot learning, including Siamese, matching, and prototypical networks.\nSiamese Networks: Siamese networks involve training a neural network to learn a similarity metric between pairs of input examples. This allows the network to distinguish between similar and dissimilar instances, making it suitable for one-shot learning tasks in conversion ecology, such as species recognition, identifying plant diseases.\nMatching Networks: Matching networks combine the concepts of attention mechanisms and recurrent networks to make predictions based on a context set of examples. These networks learn to weigh the importance of each sample in the context when making predictions for a new instance.\nPrototypical Networks: Prototypical networks learn a prototype representation for each class based on a few examples. During testing, new samples are compared to these prototypes to make predictions.\nWe build simple convolution as conv_base and let two images use the same conv_base model which share the same weight (see the figure).\n\nThe architecture of a simple siamese\n\nbuilding model with siamese\nThe code for building the one-shot learning with siamese is as follows.\n\n\n# left_input_tensor      <- layer_input(shape = list(shape_size, shape_size, 1), name = \"left_input_tensor\")\n# right_input_tensor     <- layer_input(shape = list(shape_size, shape_size, 1), name = \"right_input_tensor\")\n# \n# conv_base              <- keras_model_sequential()           %>%\n#   layer_flatten(input_shape=list(shape_size, shape_size, 1)) %>%\n#   layer_dense(units = 128, activation = \"relu\", name='fc1')  %>%\n#   layer_dropout(rate = 0.1, name='dropout1')                 %>%\n#   layer_dense(units = 128, activation = \"relu\", name='fc2')  %>% \n#   layer_dropout(rate = 0.1, name='dropout2')                 %>%\n#   layer_dense(units = 128, activation = \"relu\", name='fc3')\n# \n# left_output_tensor     <- left_input_tensor  %>%   \n#                           conv_base  \n# \n# right_output_tensor    <- right_input_tensor %>%   \n#                           conv_base  \n# \n# L1_distance <- function(tensors) { # build keras backend's function  \n#   c(x,y) %<-% tensors   \n#   return( k_abs( x - y ) ) \n# }       \n# \n# L1_layer    <- layer_lambda( object = list(left_output_tensor,right_output_tensor) , # To build self define layer, you must use layer_lamda                                \n#                              f = L1_distance                              \n#                            )   \n# \n# prediction  <- L1_layer%>%                \n#                layer_dense( units = 1 , activation = \"sigmoid\" )  \n# \n# model       <- keras_model( list(left_input_tensor,right_input_tensor), prediction)\n\n\ntraining the model\nDuring training, the model is exposed to pairs of examples. For each pair, one example is treated as the “query” example, and the other is treated as a “support” or “reference” example. The model learns to differentiate between similar and dissimilar pairs. It learns to embed the examples so that similar examples are close in the embedding space and distinct examples are far apart. The learning rate (1e-5) will lead to slowly optimize progress, so we suggest use 1e-3 as our learning rate. Below is comparison, you will find 1e-3 is learning faster than 1e-5 more.\n\n\n# model %>% compile(\n#   loss      = \"binary_crossentropy\",\n#   optimizer = optimizer_rmsprop(learning_rate = 1e-3),\n#   metrics   = c(\"accuracy\")\n# )\n# \n# history <- model %>% fit_generator(\n#   train_join_generator,\n#   steps_per_epoch = 100,\n#   epochs = 40,\n#   validation_data = val_join_generator,\n#   validation_steps = 50\n# )\n# \n# plot(history)\n\n\n2.4 Verifing with test data\nSame number match\nAfter training, the model’s performance is evaluated on new, unseen examples. To make predictions, the model typically calculates distances or similarities between the embeddings of the query example and the support examples. The class associated with the closest support example is predicted as the class for the query example.\n\n\n# # same number\n# mnist_number_left  <- 8\n# filter_idx_left    <- sample( which( test_labels == mnist_number_left  ) , 1 )\n# img_input_left     <- test_images[filter_idx_left ,,]/255\n# mnist_number_right <- 8\n# filter_idx_right   <- sample( which( test_labels == mnist_number_right ) , 1 )\n# img_input_right    <- test_images[filter_idx_right,,]/255\n# img_input_left     <- array_reshape(img_input_left , c(1, shape_size, shape_size, 1))\n# img_input_right    <- array_reshape(img_input_right, c(1, shape_size, shape_size, 1))\n# \n# similarity         <- model %>% predict(list(img_input_left,img_input_right))\n# par(mar = c(0,0,4,0))\n# plot( as.raster( abind(img_input_left[1,,,] ,\n#                        img_input_right[1,,,],\n#                        along = 2\n#                       ) \n#                ) \n# )\n# title( paste0( test_labels[filter_idx_left] , \" v.s \" , test_labels[filter_idx_right] , \" , similarity : \" , round(similarity,3) ) )\n\n\nDifferent number match\n\n\n# # different number\n# mnist_number_left  <- 8\n# filter_idx_left    <- sample( which( test_labels == mnist_number_left  ) , 1 )\n# img_input_left     <- test_images[filter_idx_left ,,]/255\n# mnist_number_right <- 7\n# filter_idx_right   <- sample( which( test_labels == mnist_number_right ) , 1 )\n# img_input_right    <- test_images[filter_idx_right,,]/255\n# img_input_left     <- array_reshape(img_input_left , c(1, shape_size, shape_size, 1))\n# img_input_right    <- array_reshape(img_input_right, c(1, shape_size, shape_size, 1))\n# \n# similarity         <- model %>% predict(list(img_input_left,img_input_right))\n# par(mar = c(0,0,4,0))\n# plot( as.raster( abind(img_input_left[1,,,] ,\n#                        img_input_right[1,,,],\n#                        along = 2\n#                       ) \n#                ) \n#     )\n# title( paste0( test_labels[filter_idx_left] , \" v.s \" , test_labels[filter_idx_right] , \" , similarity : \" , round(similarity,3) ) )\n\n\n3. Import things of deep learning\n3.1 Difference between ANN and CNN\nAs the full code for mnist recolonization, olease visit the site. Here we just mention the import things to be paid.\nLoading the data\nThe dataset can be uploaded from the keras package, and also from local computer. Here we upload it from local computer by performing the following code.\n\n\n\nAfter that, we should know the structure of the data and may need to do preprocessing thing. Here is the code for the checking and pre-processing data.\n\n\n# x_train <- mnist$train$x\n# y_train <- mnist$train$y\n# x_test <- mnist$test$x\n# y_test <- mnist$test$y\n# # dim(x_train)\n\n\nPre-processing for ANN and CNN\nThe x data is a 3-d array (images,width,height) of grayscale values. We convert the 3-d arrays into the 2-d matrices by reshaping width and height into a single dimension (28x28 images are flattened into 784 vectors). The 2-d representation is used for fully connected neural networks or multilayer perceptrons (MLPs) as the input layer typically expects a 2-d matrix where each row is a training sample. Then, we convert the grayscale values from integers ranging between 0 to 255 into floating point values ranging between 0 and 1.\n\n\n# # for ANN algorithm\n# # reshape\n# x_train_a <- array_reshape(x_train, dim = c(nrow(x_train), 784))\n# x_test_a <- array_reshape(x_test, dim = c(nrow(x_test), 784))\n# # rescale\n# x_train_a <- x_train_a / 255\n# x_test_a <- x_test_a / 255\n\n\nWe convert the x_train into a 4-d tensor with dimensions (nrow(x_train), 28, 28, 1). The nrow(x_train) represents the number of training samples. It retains the original 2-d spatial structure of the images (28x28), but it adds an extra dimension at the end. This additional dimension is often used for color channels (e.g., 1 for grayscale images, 3 for RGB images). It’s a common practice to have this 4-d shape to make the input compatible with CNNs that expect 4-d input tensors.\n\n\n# # for CNN algorithm\n# x_train_c <- array_reshape(x_train, dim = c(nrow(x_train), 28, 28, 1))\n# x_test_c <- array_reshape(x_test, dim = c(nrow(x_test), 28, 28, 1))\n# # rescale\n# x_train_c <- x_train_c / 255\n# x_test_c <- x_test_c / 255\n\n\nThe y data is an integer vector with values ranging from 0 to 9. For both ANN and CNN, we use one-hot encoding to encode the vectors into binary class matrices using the Keras to_categorical() function.\n\n\n# y_train <- to_categorical(y_train, 10)\n# y_test <- to_categorical(y_test, 10)\n\n\nModel difference between ANN and CNN\nThe core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. For ANN, the first layer specifies the shape of the input data (a length 784 numeric vector). The final layer outputs a length 10 numeric vector using a softmax activation function.\n\n\n# model <- keras_model_sequential() \n# model %>% \n#   layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>% \n#   layer_dropout(rate = 0.4) %>% \n#   layer_dense(units = 128, activation = 'relu') %>%\n#   layer_dropout(rate = 0.3) %>%\n#   layer_dense(units = 10, activation = 'softmax')\n# summary(model)\n\n\nFor CNN, the input_shape has 4-d. Its model structure is as follows.\n\n\n# model <- keras_model_sequential() \n# model %>%   \n#   layer_conv_2d(filters = 32, kernel_size = c(3, 3), padding = 'same',  input_shape = c(28, 28, 1)) %>%\n#   layer_activation('relu') %>%\n#   # layer_max_pooling_2d(pool_size=c(2, 2), strides=c(2, 2)) %>%\n#   layer_conv_2d(filters = 16, kernel_size = c(2, 2), dilation_rate = 1, activation = 'softplus', padding = 'same') %>%\n#   layer_max_pooling_2d(pool_size=c(2, 2)) %>%\n#   layer_flatten() %>%\n#   layer_dense(1000, activation = 'relu') %>%\n#   layer_dropout(0.5) %>%\n#   layer_dense(10, activation = 'softmax')\n# summary(model)\n\n\nNext, we compile the model with appropriate loss function, optimizer, and metrics, and use the fit() function to train the model for 30 epochs using batches of 128 images. For ANN, using x_train_a to fit the model.\n\n\n# # for ANN by feeding x_train_a\n# model %>% compile(\n#   loss = 'categorical_crossentropy',\n#   optimizer = optimizer_rmsprop(),\n#   metrics = c('accuracy')\n# )\n# \n# history <- model %>% fit(\n#   x_train_a, y_train, \n#   epochs = 30, batch_size = 128, \n#   validation_split = 0.2\n# )\n# \n# plot(history)\n\n\nFor CNN, feeding x_train_c to fit the model.\n\n\n# for CNN by feeding x_train_c\n# model %>% compile(\n#   loss = 'categorical_crossentropy',\n#   optimizer = optimizer_rmsprop(),\n#   metrics = c('accuracy')\n# )\n# \n# history <- model %>% fit(\n#   x_train_c, y_train, \n#   epochs = 10\n# )\n# \n# plot(history)\n\n\nEvaluate the model’s performance on the test data and predictions on new data.\n\n\n# model %>% evaluate(x_test_a, y_test)\n# model %>% predict(x_test_a)\n\n\n3.2 keras generators, Callback and Tensorboard\nAs usual, the training data are too large to fit into memory. So we can use generators to read data, preprocess and eventually feed them into model for training. generator in R is to define a function within a function, which is well documented in the website and the website.\nSampling generator\nwe can define a sampling generator, which serves as pre-processing pipelines – linking raw data to our expected data format by doing sampling, re-scaling, re-shaping and one-hot encoding.\n\n\n# library(keras)\n# np = reticulate::import('numpy') # converting data to numpy type\n# \n# # Load the MNIST dataset\n# mnist <- dataset_mnist()\n# x_train <- mnist$train$x\n# y_train <- mnist$train$y\n# \n# # Sample size and indices\n# sample_size <- 100  # Adjust as needed\n# sample_indices <- sample(1:nrow(x_train), sample_size, replace = FALSE)\n# \n# # Create a data generator with preprocessing\n# mnist_generator <- function(batch_size, sample_indices, x_data, y_data) {\n#   function() {\n#     batch_indices <- sample(sample_indices, batch_size, replace = TRUE)\n#     x_batch <- x_data[batch_indices,, , drop = FALSE] / 255\n#     y_batch <- to_categorical(as.numeric(y_data[batch_indices]), 10)\n#     \n#     # Convert to numpy arrays\n#     x_np <- np$array(x_batch)\n#     y_np <- np$array(y_batch)\n#     \n#     list(x = x_np, y = y_np)\n#   }\n# }\n# \n# # Define the ANN model\n# model <- keras_model_sequential() %>%\n#   layer_flatten(input_shape = c(28, 28, 1)) %>%\n#   layer_dense(units = 128, activation = 'relu') %>%\n#   layer_dense(units = 64, activation = 'relu') %>%\n#   layer_dense(units = 10, activation = 'softmax')\n# \n# model %>% compile(\n#   optimizer = 'adam',\n#   loss = 'categorical_crossentropy',\n#   metrics = c('accuracy')\n# )\n# \n# # Define the batch size and number of epochs\n# batch_size <- 32\n# epochs <- 10  # Adjust as needed\n# \n# # Train the model using the generator\n# history <- model %>% fit_generator(\n#   generator = mnist_generator(batch_size, sample_indices, x_train, y_train),\n#   steps_per_epoch = sample_size / batch_size,\n#   epochs = epochs\n# )\n# \n# # Print training history\n# print(history)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWritting callbacks\nCallbacks are used to control the training process, such as saving model, early stop and reducing learning rate. Here’s an example that includes a sampling generator in the chunk along with the callbacks for model checkpointing, early stopping, and reducing learning rate.\n\n\n# Load necessary libraries\n# library(keras)\n# library(reticulate)\n# np = reticulate::import('numpy')\n# # Load the MNIST dataset\n# mnist <- dataset_mnist()\n# x_train <- mnist$train$x\n# y_train <- mnist$train$y\n# x_test <- mnist$test$x\n# y_test <- mnist$test$y\n#\n# # Convert data to numpy arrays\n# x_train_np <- np$array(x_train / 255)\n# y_train_np <- np$array(to_categorical(y_train, 10))\n# x_test_np <- np$array(x_test / 255)\n# y_test_np <- np$array(to_categorical(y_test, 10))\n#\n# # Function to define a simple model\n# define_model <- function() {\n#   model <- keras_model_sequential() %>%\n#     layer_flatten(input_shape = c(28, 28, 1)) %>%\n#     layer_dense(units = 128, activation = 'relu') %>%\n#     layer_dense(units = 64, activation = 'relu') %>%\n#     layer_dense(units = 10, activation = 'softmax')\n#\n#   model %>% compile(\n#     optimizer = 'adam',\n#     loss = 'categorical_crossentropy',\n#     metrics = c('accuracy')\n#   )\n#\n#   return(model)\n# }\n#\n# # Create the model\n# model <- define_model()\n#\n# # Train the model using the fit function\n# history <- model %>% fit(\n#   x = x_train_np,\n#   y = y_train_np,\n#   epochs = 20,\n#   batch_size = 32,\n#   validation_data = list(x_test_np, y_test_np),\n#   callbacks = list(\n#     callback_model_checkpoint(\"model_checkpoint.h5\", save_best_only = TRUE),\n#     callback_early_stopping(monitor = \"val_loss\", patience = 3)\n#   )\n# )\n#\n# # Print training history\n# print(history)\n\n\nTraining visualization\nThere are a number of tools available for visualizing the training of Keras models, including: 1) A plot method for the Keras training history returned from fit(); 2) Real time visualization of training metrics within the RStudio IDE; 3) Integration with the TensorBoard visualization tool included with TensorFlow.\nPlotting History\nThe Keras’s fit() method returns an R object containing the training history, including the value of metrics at the end of each epoch. You can plot the training metrics by epoch using the plot() method.\nHere we compile and fit a model with the “accuracy” metric.\n\n\n# library(keras) # loading the package for data and modelling\n# library(abind) # operating multidimensional arrays, which are often expressed any image.\n# mnist <- dataset_mnist(\"/home/tank/Desktop/ecodatasci/images/mnist.npz\")\n# x_train <- mnist$train$x\n# y_train <- mnist$train$y\n# x_test <- mnist$test$x\n# y_test <- mnist$test$y\n\n# # for ANN algorithm\n# # reshape\n# x_train_a <- array_reshape(x_train, dim = c(nrow(x_train), 784))\n# x_test_a <- array_reshape(x_test, dim = c(nrow(x_test), 784))\n# # rescale\n# x_train_a <- x_train_a / 255\n# x_test_a <- x_test_a / 255\n# model <- keras_model_sequential()\n# model %>%\n#   layer_dense(units = 256, activation = 'relu', input_shape = c(784)) %>%\n#   layer_dropout(rate = 0.4) %>%\n#   layer_dense(units = 128, activation = 'relu') %>%\n#   layer_dropout(rate = 0.3) %>%\n#   layer_dense(units = 10, activation = 'softmax')\n# summary(model)\n# # for ANN by feeding x_train_a\n# model %>% compile(\n#   loss = 'categorical_crossentropy',\n#   optimizer = optimizer_rmsprop(),\n#   metrics = c('accuracy')\n# )\n# history <- model %>% fit(\n#   x_train_a, y_train, \n#   epochs = 30, batch_size = 128, \n#   validation_split = 0.2\n# )\n# \n# plot(history)\n# model %>% evaluate(x_test_a, y_test)\n# model %>% predict(x_test_a)\n\n# # for CNN algorithm\n# x_train_c <- array_reshape(x_train, dim = c(nrow(x_train), 28, 28, 1))\n# x_test_c <- array_reshape(x_test, dim = c(nrow(x_test), 28, 28, 1))\n# # rescale\n# x_train_c <- x_train_c / 255\n# x_test_c <- x_test_c / 255\n# y_train <- to_categorical(y_train, 10)\n# y_test <- to_categorical(y_test, 10)\n# model <- keras_model_sequential()\n# model %>%\n#   layer_conv_2d(filters = 32, kernel_size = c(3, 3), padding = 'same',  input_shape = c(28, 28, 1)) %>%\n#   layer_activation('relu') %>%\n#   # layer_max_pooling_2d(pool_size=c(2, 2), strides=c(2, 2)) %>%\n#   layer_conv_2d(filters = 16, kernel_size = c(2, 2), dilation_rate = 1, activation = 'softplus', padding = 'same') %>%\n#   layer_max_pooling_2d(pool_size=c(2, 2)) %>%\n#   layer_flatten() %>%\n#   layer_dense(1000, activation = 'relu') %>%\n#   layer_dropout(0.5) %>%\n#   layer_dense(10, activation = 'softmax')\n# summary(model)\n# \n# model %>% compile(\n#   loss = 'categorical_crossentropy',\n#   optimizer = optimizer_rmsprop(),\n#   metrics = c('accuracy')\n# )\n# \n# history <- model %>% fit(\n#   x_train_c, y_train,\n#   epochs = 10, batch_size = 128, \n#   validation_split = 0.2\n# )\n# \n# plot(history)\n\n\nTensorboard\nTensorboard is the UI view to compare different models as well as the model structure visualization. To launch your tensorboard, type this in your terminal. Beyond just training metrics, TensorBoard has a wide variety of other visualizations available including the underlying TensorFlow graph, gradient histograms, model weights, and more. TensorBoard also enables you to compare metrics across multiple training runs.\n\n\n# # launch TensorBoard\n# tensorboard(\"logs/run_a\")\n# \n# # fit the model with the TensorBoard callback\n# history <- model %>% fit(\n#   x_train, y_train,\n#   batch_size = batch_size,\n#   epochs = epochs,\n#   verbose = 1,\n#   callbacks = callback_tensorboard(\"logs/run_a\"),\n#   validation_split = 0.2\n# )\n# history <- model %>% fit(\n#   x_train, y_train,\n#   batch_size = 128,\n#   epochs = 10,\n#   verbose = 1,\n#   callbacks = callback_tensorboard(\"logs/run_a\"),\n#   validation_split = 0.2\n# )\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-13T15:40:59+08:00",
    "input_file": {}
  },
  {
    "path": "models/2023-10-15-classic-machine-learning/",
    "title": "Lesson 1: Classic machine learning",
    "description": "The last few years have seen a surge of interest in applying powerful machine learning tools to challenging problems in ecology. Our goal in this work is to introduce ecologically useful ML-based algorithms.",
    "author": [],
    "date": "2023-11-01",
    "categories": [],
    "contents": "\n\nContents\n1. From statistics to machine learning\n2. Machine learning and its main types\n3. Training models with R packages\n3.1 Data collection and importing\n3.2 Exploratory Data Analysis (EDA)\n3.3 Data Preprocessing\n3.4 Model training and Evaluation\n\n4. other learning resoures\n\n1. From statistics to machine learning\nLinear regression is frequently used in statistical data analysis, i.e., according to least squares, you can calculate its coefficient and intercept. For more details, please check the book. The model performance is examined by \\(R^2\\), and the significance for \\(R^2\\). For details, see the book.\n.\nLinear regression is also achieved via a machine learning algorithm. That is, gradient descent (GD). The operation of GD works by starting with random values for each coefficient. The sum of the squared errors are calculated for each pair of input and output values. A learning rate is used as a scale factor and the coefficients are updated in the direction towards minimizing the sum square errors. The process is repeated until a minimum sum squared error is achieved or no further improvement is possible.\nAll machine learning models are based on GD algorithms, which are different from statistical methods.\n2. Machine learning and its main types\nThe two main paradigms of ML are supervised and unsupervised learning. Supervised learning is a subfield of ML concerned with finding a function f such that \\(\\hat{y} = f(x)\\) where x is an input sample vector, y is an output sample vector, and \\(\\hat{y}\\) is a predictor of y. Common supervised learning tasks include classification and regression. Unsupervised learning aims to find a function g which transforms an input sample x to a representation z in order to reveal underlying information about the input sample. A common unsupervised learning task is clustering.\nIn machine learning algorithms, a parameterisable function \\(f_{θ}\\) is often defined. The parameters θ of the function can then be learned through an iterative process of updating the parameters and evaluating the performance of the function. This process is known as optimization or training; these training algorithms often rely on a function \\(L(y, \\hat{y})\\) which quantifies how incorrect a prediction is compared to the target vector. This is usually known as the cost, error, or loss function, and is chosen depending on the task in hand.\nDuring optimization, parameters are adjusted according to training data. In the case of supervised learning, training data comprises pairs of input and output vectors, each taking the form (x, y). The algorithm will be shown each data sample multiple times. The number of times an algorithm “sees” the entire set of training data is known as an epoch, and is used as a measure of how much an algorithm has been trained. In some circumstances, an algorithm can perform well on the training data but does not perform well on new data. This is known as overfitting, and occurs when the algorithm has learned to predict the target output of each sample in the training data by random noise in the input features, rather than by the important underlying variables.\nOverfitting can be detected by splitting the training data into three sets: training, validation, and test. Under this split, the algorithm is trained on the training set, and after each epoch is evaluated on the validation set. When the performance on the validation set does not increase, the algorithm has stopped learning useful properties and has begun to overfit. The algorithm can then be stopped -known as early-stopping - and evaluated on the unseen test set to give a true indication of the algorithm’s performance. The general configuration of the function f is usually governed by hyperparameters, which - unlike parameters - are fixed and are not adjusted during training.\nML algorithms are attractive options for solving some problems, because the learned functions \\(f_{θ}\\) are derived directly from the training data without intervention. This makes ML algorithms particularly useful on complex problems for which it is difficult or near-impossible to manually define suitable functions. However, the usefulness of ML is not limited to predictive tasks; after training the learned function can also be interpreted to yield useful information about the data.\n3. Training models with R packages\nBelow, we’ll examine fundamental machine learning ideas, methods, and a step-by-step procedure of machine learning model developments by utilizing Caret package. Please check this website for details. In this section, we need the libraries, including:\nggplot2: for interactive graphs and visualization.\nggpubr: for making plot beautiful along with that of ggplot2.\nreshape: for melting dataset.\ncaret: providing many machine learning algorithms.\n\n\n# # To load all necessary packages\n# library(ggplot2)\n# #library(ggpubr) \n# library(reshape)\n# library(caret)\n\n\nWe will walk through each step of implementing Caret package in this part. The general steps to be followed in any Machine learning project are:\n3.1 Data collection and importing\nNext, we will import our data to a R environment.\n\n\n# # Dataset\n# data(\"iris\")\n#   \n# # To display the first five rows of our data\n# head(iris)\n\n\n3.2 Exploratory Data Analysis (EDA)\nUnderstanding and assessing the data you have for your project is one of the important steps in the modeling preparation process. This is accomplished through the use of data exploration, visualization, and statistical data summarization with a measure of central tendencies. You will gain an understanding of your data during this phase, and you will take a broad view of it to get ready for the modeling step.\n\n\n# Summary statistics of data\n# summary(iris)\n\n\nVisualizing the outliers by using boxplot. As we use ggplot2 we will take numerical variables by subsetting the entire of it. Using of reshape package we melt the data and plot it to check for the presence of any outliers.\n\n\n# df <- subset(iris, select = c(Sepal.Length, \n#                               Sepal.Width, \n#                               Petal.Length, \n#                               Petal.Width))\n# # plot and see the box plot of each variable\n# ggplot(data = melt(df), \n#        aes(x=variable, y=value)) + \n#         geom_boxplot(aes(fill=variable))\n\n\nLet’s now use a histogram plot to visualize the distribution of our data’s continuous variables.\n\n\n# a <- ggplot(data = iris, aes(x = Petal.Length)) +\n#     geom_histogram( color = \"red\", \n#                    fill = \"blue\", \n#                    alpha = 0.01) + geom_density()\n#   \n# b <- ggplot(data = iris, aes(x = Petal.Width)) +\n#     geom_histogram( color = \"red\", \n#                    fill = \"blue\", \n#                    alpha = 0.1) + geom_density()\n# c <- ggplot(data = iris, aes(x = Sepal.Length)) +\n#     geom_histogram( color = \"red\", \n#                    fill = \"blue\", \n#                    alpha = 0.1) + geom_density()\n#   \n# d <- ggplot(data = iris, aes(x = Sepal.Width)) +\n#     geom_histogram( color = \"red\", \n#                    fill = \"blue\", \n#                    alpha = 0.1) +geom_density()\n#   \n# #ggarrange(a, b, c, d + rremove(\"x.text\"), \n# #          labels = c(\"a\", \"b\", \"c\", \"d\"),\n# #          ncol = 2, nrow = 2)\n\n\nNext, we will move to the Data Preparation phase of our machine learning process. Before that, lets split our dataset into train, test and validation partition.\n\n\n# # Create train-test split of the data \n# limits <- createDataPartition(iris$Species, \n#                               p=0.80, \n#                               list=FALSE)\n#   \n# # select 20% of the data for validation\n# testiris <- iris[-limits,]\n#   \n# # use the remaining to training and testing the models\n# trainiris <- iris[limits,]\n\n\n3.3 Data Preprocessing\nThe quality of our good predictions from the model depends on the quality of the data itself, data preprocesing is one of the most important steps in machine learning. We can see from the box plot that there are outliers in our data, and the histogram also shows how skewed the data is on the right and left sides. We shall thus eliminate those outliers from our data.\n\n\n# Q <- quantile(trainiris$Sepal.Width, \n#               probs=c(.25, .75), \n#               na.rm = FALSE)\n\n\nAfter obtaining the quantile value, we will additionally compute the interquartile range in order to determine the upper and lower bound cutoff values. Then, we eliminate the outliers.\n\n\n# # Code to calculate the IQR, upper and lower bounds\n# iqr <- IQR(trainiris$Sepal.Width)\n# up <-  Q[2]+1.5*iqr \n# low<- Q[1]-1.5*iqr\n\n# # Elimination of outliers by using of iqr\n# normal <- subset(trainiris, \n#                  trainiris$Sepal.Width > (Q[1] - 1.5*iqr) \n#                  & trainiris$Sepal.Width < (Q[2]+1.5*iqr))\n# normal\n\n\nBy using a boxplot, we can additionally see the outliers that were eliminated from the data.\n\n\n# # boxplot using cleaned dataset\n# boxes <- subset(normal, \n#                 select = c(Sepal.Length, \n#                            Sepal.Width, \n#                            Petal.Length, \n#                            Petal.Width))\n# ggplot(data = melt(boxes), \n#        aes(x=variable, y=value)) + \n#         geom_boxplot(aes(fill=variable))\n\n\n3.4 Model training and Evaluation\nIt’s time to use the clean data to create a model. We don’t have a specific algorithm in mind, Let’s compare LDA and SVM for practical purposes and choose the best one. For accuracy and prediction across all samples, we will employ 10-fold cross validation.\n\n\n# # crossvalidation set to 10\n# crossfold  <- trainControl(method=\"cv\", \n#                            number=10, \n#                            savePredictions = TRUE)\n# metric <- \"Accuracy\"\n\n\nLet’s start training model with Linear Discriminant Analysis.\n\n\n# # Set a random seed to 42\n# set.seed(42) \n# fit.lda <- train(Species~., \n#                  data=trainiris, \n#                  method=\"lda\", \n#                  metric=metric, #\n#                  trControl=crossfold)\n# print(fit.lda)\n\n\nWe can also use SVM model for the training.\n\n\n# set.seed(42)\n# fit.svm <- train(Species~., \n#                  data=trainiris, \n#                  method=\"svmRadial\", \n#                  metric=metric,\n#                  trControl=crossfold)\n# print(fit.svm)\n\n\nThe results show that both algorithms functioned admirably with only minor variations. Although the model can be tuned to improve its accuracy accurate, for the purposes of this lesson, let’s stick with LDA and generate predictions using test data.\n\n\n# # prediction on test data\n# predictions <- predict(fit.lda, testiris)\n# confusionMatrix(predictions, testiris$Species)\n\n\nAccording to the summary of our model above, We see that the prediction performance is poor; this may be because we neglected to consider the LDA algorithm’s premise that the predictor variables should have the same variance, which is accomplished by scaling those features. We won’t deviate from the topic of this lesson because we are interested in developing machine learning utilizing the Caret module in R.\n4. other learning resoures\nHere is a concise guide to machine learning techniques for ecological data. This practical guide to machine learning includes explaining and exploring different machine learning techniques, from CARTs to GBMs, using R.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-11-01T09:59:08+08:00",
    "input_file": {}
  },
  {
    "path": "models/2023-10-15-deep-learning-as-data-driven-methods/",
    "title": "Lesson 3: Deep Learning as Data-driven Methods",
    "description": "The immediate aim of this section is to develop a data-driven machine learning algorithm to predict which interactions are missing from ecological networks, and to explore ways in which ecological insight can be extracted from the algorithm. In particular, we introduce key concepts and terminology of data-driven models.",
    "author": [],
    "date": "2023-10-15",
    "categories": [],
    "contents": "\n\nContents\n1. What are neural networks and how do they learn?\n2. Appplications of deep learning to ecology and evolution\n2.1 Automated species identification\n2.2 Environmental monitoring and modeling\n2.3 Behavioral studies\n2.4 Genomics, population genetics, and phylogenetics\n\n\nEcology and evolutionary biology investigate complex patterns and processes. A mathematical toolkit has been necessary to describe and explain fundamental components of organic evolution and ecological interactions. This wealth of data is driving the development of analytic tools that can provide new understanding, greater efficiency, and ease of use. Likelihood-based mechanistic approaches designed to consider many variables can be so computationally expensive that they can no longer be applied to data routinely generated in modern studies. A promising alternative is likelihood-free inference, one example of which is machine learning. The goal of machine learning is to find a model that performs well at making predictions from the data. This contrasts with likelihood-based methods, which assume the model generating the data is known. More recently machine learning has seen a dramatic surge in popularity\nwith a slew of new algorithms and applications.\nOne of the approaches rapidly gaining popularity is deep learning. Deep learning relies on multilayered, connected processing units (artificial neural networks or ANNs). The successes of deep learning were possible because of a major advantage of deep learning over classical machine learning approaches. Classical machine learning requires that important data features are identified using expert domain knowledge. Neural networks can automatically discover the most important data features and patterns relevant for the task. Researchers are now beginning to apply deep learning to problems across ecology and evolutionary biology, from community science projects and environmental monitoring through sequencing equipment output processing, to population genetics\nand phylogenetic inference.\n1. What are neural networks and how do they learn?\nHere describe what artificial neural networks are and how they are used as inference tools (Box 1).\n\n\n\nBox 1: Common neural network architectures\n\n\nArtificial Neural Networks(ANNs) is a group of multiple perceptrons/ neurons at each layer. Neurons can be connected to other neurons in different layers but not within the same layer. In the simplest form, a neural network has one input layer, at least one intermediate or hidden layer, and an output layer. The trainable network parameters consist of biases and weights. Each node has a bias value (b), which determine how easy it is for it to “fire”. Each connection has a weight value (W) which represents connection strength. Each node also has an activation function (f), for example sigmoid\nfunction. Given some input (x), the so-called feedforward output (y) of a node is determined by a simple equation: \\(y = f(W × x + b)\\).\nANN is a Feed-Forward Neural network because inputs are processed only in the forward direction. Recurrent neural networks is to add loops to information flow. Information flows from the input to the output of the network but it can flow back from the output to the input of the hidden layer through recurrent weights.\n\n\n\nHowever, simple RNNs such as the one shown in the figure are difficult to train because weights in these networks can quickly diverge during training. More advanced RNN, such as Long Short-Term Memory networks (LSTMs) or Gated Recurrent Units (GRUs), address this problem and are commonly used with time series data. In evolutionary biology, deep learning solutions including GRU have been used to predict recombination landscapes. In the field of ecology. reserchers are trying to use deep learning techonologies for the analysis of remote sensing data. The convolutional neural networks (CNNs) can excel at capturing complex, hierarchical patterns and are the architecture used in most identification and classification problems.\n\n\n\n\nIn mathematical sense, neural networks are simply a function mapping input onto a desired output. This general design is simple, but it makes neural networks\nextraordinarily powerful: a network with information flowing from input to output layer with at least one intermediate layer (i.e. feedforward network) can approximate any continuous function, regardless of its complexity. These approximations can describe pixels of an image, for example, and networks with multiple intermediate layers (deep neural networks) can also learn relationships between them as high-level concepts such as lines, geometric shapes, and even whole scenes.\nANNs learn continuous distributions but their output can represent probabilities of distinct data classes, as well as continuous values. Such networks can thus be used to construct classifiers, which are models distinguishing among discrete categories, as well as regression models, which infer continuous values. But feedforward operations alone do not allow the network to learn or generalize to new data, which is the essence of most ANN applications.\nIn order for an ANN to be a predictive tool, one needs to assess how good predictions are and be able to adjust ANN parameters to improve performance. A measure of how far off the output of the network is is called a loss function. One example of a loss\nfunction is the sum of squares error (SSE), which is simply the sum of differences between each predicted value (y) and the true value (\\(\\overline{y}\\)) squared for absolute value:\n\\[SEE= \\sum_{i = 1}^{n}(y_i - \\overline{y}_i)^2\\]\nThe network also needs a mechanism for finding the set of parameters that minimize the loss function. Once the loss (error) is measured at the output, it has to be traced back across the network to measure how parameters contributed to it. This process is called backpropagation and it uses chain rule calculus to find the derivative (slope) of the loss function with respect to the network’s trainable parameters. The process of increasing or decreasing parameters such that they minimize the derivative of the loss function is called gradient descent. This process is iterative, occurring every time a batch of training data is processed, and collectively referred to as the training loop. When devised correctly, it results in improvement of inference accuracy with each pass of the loop.\nThe fact that ANNs are universal approximators for continuous functions that can be trained makes them powerful predictive tools. This learning scheme is most easily illustrated with an example of supervised learning, where the network is trained on a data set, e.g. images of expert-identified species or methylated vs. unmethylated DNA sequences. Supervised training usually involves splitting data into three subsets:\ntraining, validation, and testing. The validation set is not directly used in training but prediction on it is performed at the end of each training cycle to assess how well the network generalizes outside of the training set. Test set is held back for the final estimate of accuracy.\n2. Appplications of deep learning to ecology and evolution\nIn the sections that follow, we review how deep learning has been applied in ecology and evolution, including species identification and monitoring, ecological and behavioral studies, and population genetics and phylogenetics. We use these examples to showcase the variety of deep learning techniques extending its usage beyond the general picture described above.\n2.1 Automated species identification\nDeep learning enabled breakthroughs in automated image classification, largely possible thanks to CNNs. Image recognition has obvious applications in biology and was\nadopted early for problems of species identification and wildlife monitoring. It is not surprising then that identification or classification of individuals or species from image, video, and sound data is the most common use of deep learning in the field. These efforts already span many taxa, from bacteria, through protozoans, plants to insects and vertebrates, both extant and fossil and at scales ranging from local to global. Intensifying efforts to digitize natural history collections provide troves of\nimage data that can be used for this purpose.\nCamera trap systems and deep learning classifiers are now commonly used for vertebrate wildlife monitoring and systems automating environmental monitoring of aquatic macroinvertebrates are also being developed. Many publications present systems or deep learning models for detecting and identifying pests or crop diseases in agroecosystems or stored agricultural\ncommodities. Despite economic importance and demonstrated potential for crop pest and disease monitoring, at present few non-proprietary systems or open source software\napplications exist. A notable exception is a mobile application system for identifying diseases of cassava plants, one of the most important tropical crops.\nDeep learning has also been applied to identification from audio recordings, including bird and bat sounds, and even wing beats of mosquitoes. Unsurprisingly, the technology has been applied most often to bird calls, where it has been used not only to identify species, but also monitor their abundance. The recently developed BirdNET is a deep neural network capable of identifying North American and European birds from vocalizations in complex soundscapes, available on a variety of platforms, including user-friendly smartphone apps. Most of these studies use audio converted to spectrograms, image representations of sound, to train CNNs as in visual recognition problems.\nGiven its utility for automated identification, deep learning is increasingly used in community science initiatives. Examples include a growing number of mobile phone applications such as plant-focused identification app Pl@ntNet, bird identification tool Merlin or the citizen naturalist portal iNaturalist, as well as a number of more local or taxon-specific guides. Many of these applications crowd-source training data collection and identification verification by users. They improve by periodically re-training their deep learning classifiers as more reliable data is collected.\nMany of these studies employ data handling approaches that increase performance of deep learning classifiers. Several use data augmentation, a technique that relies on altering training data with distortion. These modifications, applied to each data input in each training epoch, effectively increase training set size. Data augmentation is an important strategy for reducing overfitting and almost always results in increased classifier accuracy. By ensuring that the neural network never sees the same input twice, augmentation only partly addresses the fact that acquiring large, human-labeled datasets is a bottleneck for many applications. An alternative approach is to train an initial classifier in a supervised way, using a labeled training set, and then use this reasonably well-performing classifier for adding more images in an unsupervised manner, without human intervention.\nAnother technique ubiquitous in identification and classification tasks is transfer learning. Transfer learning is most commonly accomplished by first training on a different, usually larger and more general dataset than the one assembled for the problem on hand. The resulting network parameters can then be used as the starting point for fine-tuning on the focal dataset. In species recognition from images it is common to use networks pre-trained on large, public datasets of everyday objects such as ImageNet or COCO as illustrated by several of the studies cited above. Using pre-trained networks makes the network learn faster and often results in higher accuracy.\nIn addition to properly assigning a label to an image, termed image classification, a common computer vision problem is to localize objects. Object recognition is a term often used for the combination of the two: drawing a bounding box around an object and predicting its class. Because there may be many objects in an image, this is a more challenging problem. The many proposed solutions involve either extracting candidate regions from images prior to prediction or predicting classes directly on grids of image pixels. Examples are common in agriculture, where object detection has been used to identify and count pests.\nThe deep learning framework allows training several neural networks of the same or varying architectures on one dataset and averaging their predictions. Known as model ensembling, this technique reduces variance in predictions and can improve accuracy. Examples in species identification include Finnish fungi recognition and UK ladybird beetles.\nFinally, deep learning is not limited to considering image pixels alone but can also take advantage of contextual information such as locality or phenology. For example, output can be improved by filtering out\nnonsensical predictions given prior occurrence data. This approach, however, does not jointly consider the available data in a common framework. Neural networks can be trained on multiple data inputs simultaneously and consider them jointly in the final layers. One study used this approach for beetle identification from images and found improvement in accuracy with information about location, date, weather, habitat, and\nuser expertise.\n2.2 Environmental monitoring and modeling\nThe above mentioned approaches to automated identification of species or individuals are also being scaled to ecosystem scale and applied to diversity assessment, conservation, and resource management. Examples using techniques detailed\nin the previous section include detecting and estimating abundance of zooplankton\nand detecting and counting sea turtles and whales using drone and satellite imagery. Other uses combine digital imagery with LiDAR and other remote sensing or geospatial data for mapping of vegetation, forest carbon stock, and the footprint of fishing across the world’s oceans. Similar applications include integrated systems for real-time wildlife monitoring using data from camera traps and microphone and raise the prospect of surveillance of social media posts for illegal animal trade.\nIn addition to classification and mapping of static information, RNNs and similar approaches have been used with temporal ecological data. Examples include predicting\neutrophication, phytoplankton blooms, and benthic invertebrate community dynamics. As mentioned previously, combining inputs from different sources is natural for deep learning and Rammer and Seidl take advantage of this to predict and map future bark beetle outbreaks based on temporal information on climate, vegetation, and past outbreaks. Capinha et al. proposed a generalized approach to classification and prediction from ecological time series data leveraging automated choice of the best network architecture for the task at hand.\nFinally, neural networks are being used to develop more realistic models and simulations of real world patterns and phenomena. Benkendorf and Hawkins found that deep neural networks can be used to generate accurate species distribution models but\nalso noted that other machine learning approaches perform as well or better with limited training data. Strydom et al. designed a system to predict species interactions from co-occurrence data. A study using reinforcement learning investigated how learning to hunt or avoid predators by individual agents influenced predator-prey dynamics.\n2.3 Behavioral studies\nThe study of animal behavior, both in the field and controlled laboratory settings, is another research area of ecology and evolution that is poised to greatly benefit from adoption of deep learning. Recent technological advancements in sensing, monitoring, and automation allow behavioral ecologists to collect and analyze large amounts of data Long-standing challenges in identifying, quantifying, and analyzing animal behavior still limit the ability to fully automate processing of these data, however. Deep learning has the potential to address many of these challenges and it is increasingly being adopted in studies involving identification of individual animals, body posture and movement tracking, and classification of behaviors.\nIn the area of animal body posture, deep learning can provide non-invasive estimation of the position of animals’ body parts from video recordings. Several open-source toolkits have been developed for this purpose, ranging from species-specific solutions, to generic frameworks applicable to any species, some of which offer 3-dimensional and/or multiple animals tracking. In addition to pose estimation,\ndeep learning is also being adopted to enhance the performance of established computer vision methods used to track spatial position of animal detection or the identification of markers, as well as to automatically perform behavioral analysis of spatial trajectories.\nDeep learning can also allow for the identification, classification, and subsequent re-identification of individual animals from camera feeds or traps, both in the wild and in captivity. Usually based on the use of CNNs for image recognition, deep\nlearning can also be combined with other technologies to develop automated\ndata-processing pipelines to collect and label samples bird species. A popular application in this area is face recognition enabling mark-recapture studies for monitoring populations of individuals, their behavior, and social interactions. Examples in the wild include identification of elephants, chimpanzees, right whales and brown bears. Studies performed in captivity have been carried out\non pandas and pigs.\nFinally, deep learning is being applied to automatically detect and classify the behavior of animals from raw data, a crucial step towards overcoming time-consuming and error-prone manual labeling tasks. Largely based on CNNs, a number of different solutions have been developed to recognize and label behaviors from images as well as video and sound recordings. These behavior detection systems can discriminate between behaviors, with the possibility of concurrent behaviors and thus multi-labeling, or be specifically designed to detect binary events (e.g. distinguish whale vocalizations from noise, or rare social changes in otherwise stable insect colonies). In addition to behavior recognition, deep learning solutions are also being devised to predict behavioral measurements that would otherwise require specialized recording devices. For example, Browning et al. used artificial neural networks to predict the diving behavior of seabirds from GPS data alone without specialized time-depth records,\nwhereas Liu et al. used vertical movement sensors alone to predict locomotor energy expenditure of sharks.\n2.4 Genomics, population genetics, and phylogenetics\nA rapidly growing number of studies apply deep learning to study genomes. Deep learning is used in DNA sequencing for translating the raw signal of long-read Oxford Nanopore sequencers into nucleotide calls, outperforming other basecallers. Another example of successful application is variant calling, or identification of small nucleotide polymorphisms and indels in diploid or polyploid genomes. DeepVariant is a tool that converts text file representations of multiple sequences aligned to a reference (read pileups) to images and uses a CNN to predict alternative alleles. Another tool predicts gene copy number variations from high-throughput sequencing reads. Deep learning has been particularly successful in functional and regulatory genomics and has been used for predicting sequence specificity of nucleic acidbinding proteins, methylation status, identification of transcription start sites, predicting expression patterns from genotypes, classification of transposable elements, and more. These applications are not strictly within the purview of ecology and evolution and have been comprehensively reviewed elsewhere.\nDeep learning is a part of a growing trend to apply machine learning to the study of evolution of populations and species. One of the early studies applying neural networks to population genetic data showed them capable of estimating population-scale mutation rates, population sizes and their changes through time, recombination rates,\nand detecting introgressed loci and positive selection on simulated data. That study\ndemonstrated that CNNs are capable of estimating population genetic parameters in scenarios for which likelihood-based methods have yet to be developed, such as accurately inferring recombination rates from read coverage data in autotetraploid genomes. The impressive performance of deep learning for population genetics encouraged recent development of user-friendly tools for inference from empirical data, including selective sweep classification, quantifying selection strength, jointly inferring selection and population size change, and inferring recombination landscapes. Other studies relied on custom approaches to identifying deleterious variants in sorghum and positive selection in SARS-CoV-2. An emerging approach involves combining deep learning with approximate Bayesian computation (ABC). It has been applied to inferring population size change through time, identifying hybridization from pairwise nucleotide divergences, and choosing best-fitting demographic scenarios based on site frequency spectra or SNP data. Most of the above approaches use CNNs, which in their standard formulation are sensitive to permutations. This means that the ordering of chromosomes in the input, for example, is significant for training and prediction. Flagel et al. dealt with this by sorting chromosomes by similarity but network architectures insensitive to input ordering are\nalso being developed.\nDeep learning has also been used for inference and visualization of population structure. Here neural networks are used for dimensionality reduction, similar to\nprincipal component analysis, rather than for solving a classification or regression problem. To achieve this, the authors used variational autoencoders (VAEs), a pair of neural networks that learn efficient representations of data in an unsupervised manner. In this method the encoder network compresses input data into latent variables, while the decoder network attempts reconstructing the original data from those variables. The loss function in this case is a combined measure of how good the reconstruction is and desirable properties of latent variables. The goal was to visualize population structure in a two-dimensional space and so the data were compressed into two variables representing coordinates.\nAs the importance of the spatial component is becoming increasingly highlighted in population genetics, deep learning is also beginning to be used for predicting sample origins based on genetic variation and local-ancestry inference, which aims to identify populations from which a genetic locus descended. This application involves using generative adversarial networks (GANs) to create artificial human genomic sequences of known ancestry.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-10-25T09:38:16+08:00",
    "input_file": {}
  },
  {
    "path": "models/2023-10-15-remote-sensing-data-and-models/",
    "title": "Lesson 4: Remote sensing data and models",
    "description": "Remote sensing is the science of identifying, observing, collecting, and measuring objects without coming into direct contact with them. This can be accomplished through many devices that carry sensors and capture the characteristics of Earth remotely. Sensors on board satellites also record the electromagnetic energy that is reflected or emitted from objects on Earth. They are especially useful for natural resoures and a variety of socio-economic research and applications.",
    "author": [],
    "date": "2023-10-15",
    "categories": [],
    "contents": "\n\nContents\n1. Remote sensing data\n2. Data operations and tools\n2.1 Download data of aoi\n2.2 Merging, cropping and masking\n2.3 Extracting values and computing statistics\n2.4 Storing and exporting results\n\n3. A practical example\n\n1. Remote sensing data\nForest Cover Data\nThis section is adopted from the module. we will primarily work with the Vegetation Continuous Fields (VCF) data provided by the Land Processes Distributed Active Archive Center (LP DAAC), a component of NASA’s Earth Observing System Data and Information System (EOSDIS). The MOD44B Version 6 VCF is a yearly representation of surface vegetation from 2000 to 2020 at 250 m resolution. Each pixel stores a percentage of three ground cover components: percent tree cover, percent non-tree cover, and percent bare.\nThe ground cover percentages are estimates from a machine learning model based on the combination of the Moderate Resolution Imaging Spectroradiometer (MODIS) data and other high resolution data from NASA and Google Earth. The machine learning model incorporates the visible bandwidth as well as other bandwidth such as brightness temperature (from MODIS bands 20, 31, 32).\nThe VCF data utilize thermal signatures and other correlates to distinguish forest and non-forest plantation, which is an improvement compared to the Normalized Differenced Vegetation Index (NDVI). For this use case, VCF also improves on the Global Forest Cover (GFC) data set, another source used to study deforestation, which only provides binary data points. GFC records baseline forest cover in the year 2000 and includes a binary indicator for the year of deforestation for each 30m × 30m pixel. If over 90% of the forest cover was lost in a pixel by a given year, then the pixel is marked as deforested, while a pixel is marked as reforested if the forest cover went from 0 in 2000 to a positive value. The VCF records continuous changes in percent of ground cover components, which provides more details than the GFC data.\nNighttime lights\nThere is a strong correlation between nighttime lights and Gross State Product (GSP) or Gross Domestic Product (GDP) measures, at the national, state and regional levels or even at a more granular resolution. Thus, nighttime light observations can be used as a proxy for economic activity, especially over periods or regions where these data are not available or where the statistical systems are of low quality or when no recent population or economic censuses are available. Similarly, changes in nighttime light intensity can be used by economists as an additional measure of income growth when no other measures of income growth are available.\nProville et al. (2017) examined trends observed by DMSP-OLS in the period 1992-2013 and their correlation with a series of socio-economic indicators. They found the strongest correlations between nighttime lights, electricity consumption, CO2 emissions, and GDP, followed by population, CH4 emissions, N2O emissions, poverty and F-gas emissions.\n2. Data operations and tools\n2.1 Download data of aoi\nIn order to perform data manipulation, we need to attach packages. We are going to use the package luna to download data from MODIS and the packages terra, tidyverse, raster, and sf for data manipulation.\n\n\n\nWe follow thistutorial to get MODIS data with luna. For details of the terra package, please refer to the package manuscript and this tutorial. If you are not familiar with the tidyverse workflow, please refer to the R for Data Science.\nOnce the required packages have been attached, we can access VCF in R. We prefer using R for its ability to download large numbers of files and enable regular, automated updates.\nWe can first use luna to check the list of data products available from MODIS. Since luna can also access data from the LANDSAT and SENTINEL platforms, we add “MOD|MYD|^MCD” to narrow our scope to MODIS data. The printed results below list six products from MODIS.\n\n\n\nThe product name for VCF is MOD44B. We can use the function productInfo to launch the information page of VCF.\n\n\n\nWe can query MODIS and only download a subset of the data. We need to specify the start and end dates and our area of interest (AOI). The date format is “yyyy-mm-dd”. Suppose here we want to subset data from 2010 to 2012.\n\n\n\nIn order to subset your area of interest, you need to provide a “map” to getModis(). This can be obtained from online databases such as the global administrative area database (GADM). You can download map data directly from GADM or you can use R to obtain GADM map data. We will use R below, which requires first installing the package geodata.\n\n\n\nGeographic levels in GADM are defined as:\nlevel 0: National\nlevel 1: State/province/equivalent\nlevel 2: County/district/equivalent\nlevel 3/4: Smaller administrative levels\nFor our example, we are interested in India at the district level. We can download the map of India and its level 2 administrative areas with the following code:\n\n\n\nThe boundary data is downloaded to the path that you specified in the path argument. The downloaded data through gadm() will be in the PackedSpatVector class. If you want to convert it to another class (for example, the sf class, which is easier to work with in R), you can first read it using readRDS(), then convert to a SpatVector via vect() from the terra package, and finally convert it to a sf object.\n\n\n\nThe map we downloaded is at the district level (level 2). Assume our AOI is the state of Odisha. Each row of the data represents a county in Odisha, and the geospatial information for each county is stored in the last column: geometry. We can filter to obtain the boundaries for our AOI, which will return aoi in vector format, stored as a data frame in R.\n\n\n\n\n\n\nNow that we have our AOI as well as time frame, we can filter the MODIS VCF data on these values and see what is available.\n\n\n\nThe products we are going to download are tiled products. For details of tiled products, the tilling system, and the naming convention, please refer to the MODIS overview page. In essence, we will be downloading grids of maps that cover our AOI.\nTo actually download these files from the NASA server, you will need a username and password. Please register on NASA Earth Data if you haven’t done so.\nThe following code will download the files. Replace the path value with the location on your computer where you would like to store these files. Replace the username and password values with your NASA Earth Data credentials from above.\n\n\n\nThe data format from MODIS is HDF and may include sub-datasets. We can use terra to read these files and create raster files. For example,\n\n\n\nWe can find basic information such as the coordinate reference system, number of cells, and resolution from the above output. There are 7 layers in each of the VCF tiled files. We are interested in the percent tree coverage layer.\n\n\n\nA quick plot of the data can be done with the plotRBG() function.\n\n\n\n2.2 Merging, cropping and masking\nSince there are four hdf files in each year for our AOI, we can first merge the four SpatRaster files into one file per year. We’ll use 2010 as an example. We can filter to only include our layer of interest - percent of tree cover - from each hdf file, which can be done by subsetting the output using [[1]] (using 1 because percent tree cover is the first layer in each file).\n\n\n\nBefore we merge these SpatRster objects, it is often a good practice to check their origins and resolutions. merge requires origin and resolution to be the same across objects.\n\n\n\n\n\n\nWe see that origins of these files are slightly different, but all are close to (0, 0). We do not need to worry about these slight differences, as merge will handle them automatically.\n\n\n\nNote: cells with 200% represent water and rivers.\nWe are now ready to crop and mask the raster file to match our AOI. This tutorial explains the difference between cropping and masking.\nTo crop a raster file according to vector data boundaries (eg, our aoi object representing Odisha districts), we first align the coordinate reference systems of our raster file and vector file. Then, use crop(raster data, vector data). To mask, use mask(raster data, vector data). Note that for terra::mask(), the second argument needs to be SpatVector. terra does not support sf objects yet, so we use vect(aoi) to convert our sf object aoi to a SpatVector.\n\n\n\nTo plot our new raster file, we use:\n\n\n\n2.3 Extracting values and computing statistics\nAfter we have cropped and masked the raster file to our AOI, we can extract values for each county in the state of Odisha.\n\n\n\nThe values extracted by terra::extract are stored in a data frame. Note that the ID corresponds to the row number of your vector file (i.e. object aoi in our case). We can then compute statistics based on this data frame. Here we compute several statistics describing the percent of forest cover for each county. Note that cells with 200% represent water and river and should be excluded from calculation.\n\n\n\n2.4 Storing and exporting results\nWith terra you can easily write shape files and several formats of raster files. The main function for writing vector data is writeVector(), while for writing raster data we use writeRaster(). For details, you can refer to this page and the documentation of terra.\n3. A practical example\nWe will replicate some main results in the paper. To access the full replication data and code, check this github repo. We are going to replicate Table 3 in the paper.\nThe research question is whether newly constructed rural roads impact local deforestation. The authors explored this question using two empirical strategies: fuzzy RD and difference-in-difference. In the following sections, we implement the difference-in-difference method and replicate the regression results.\nIn order to run fixed effects models, we will need the fixest package. This tutorial is a good reference for introducing fixest functions.\nData for this exercise was processed and stored in pmgsy_trees_rural_panel.csv, which you can find the through the link to the CSV data in the github repo. Each row of the data frame presents a village in a specific year.\n\n\n\nThe paper estimated the following equation:\n\\[\nForest_{vdt} = β_{1}Award_{vdt} + β_{2}Complete_{vdt} + α_{v} + γ_{dt} + X_{v}⋅V_{t} + η_{vdt}\n\\]\nwhere \\(Forest_{vdt}\\) is forest cover of village \\(v\\) in district \\(d\\) in year \\(t\\). \\(Award_{vdt}\\) is a dummy variable which takes one during the period when the new road is awarded to the village but has not been built. \\(Complete_{vdt}\\) is also a dummy variable which takes one for all years following the completion of a new road to village \\(v\\). \\(α_{v}\\) are village fixed effects, while \\(γ_{dt}\\) are the district-year fixed effects. \\(X_{v}\\) controls some baseline characteristics (e.g. forest cover in 2000, total population) and is interacted with year fixed effects \\(V_{t}\\).\nThere is one more step before we run the regressions. In Stata, which the authors used for their regression, reghdfe removed singleton groups automatically. However, the fixest package currently doesn’t possess this functionality, so for now, we will manually remove these observations.\n\n\n\nFinally, we can run our regressions. Following the authors, we test the effect of being awarded a new road and receiving the road on the log forest cover as well as on the average forest cover.\n\n\n\nOur results align with the authors’ findings presented in Table 3 which show that being awarded a road has a negative impact on forest cover (approximately 0.5% loss in the construction period between being awarded a road and its completion), but after the road is constructed, forest cover appears to return. This could incorrectly be interpreted as a positive effect of roads on tree cover if the award term is left out. This determination that rural roads have no effect on forest loss, in combination with the authors’ additional findings of substantial forest loss due to highway construction, have important policy implications for governments considering similar infrastructure expansion. The use of VCF data in this study enabled significant insights, and the potential use cases for VCF data remain numerous.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-10-25T09:38:16+08:00",
    "input_file": {}
  }
]
