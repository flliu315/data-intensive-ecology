{
  "articles": [
    {
      "path": "about.html",
      "title": "About the class",
      "description": "ECOL6002P, the second semester of an academic year\n\nInstructors: Fanglin Liu\n\nTeaching assistants:\n\nWhen & where: Wednesday and Friday 3,4,5 (9:45-12:10), 3A409\n\nSome notes by Fanglin Liu: https://github.com/flliu315\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\n1. Introduce of the class\n2. Preparing for the class\n3. Expected learning outcomes\n4. Final evaluation\n\n1. Introduce of the class\nEcology is the study of the relationships between plants/animals, including humans, and their abiotic environment. It seeks to understand the vital connections between living organisms and the world around them.\nEcology also provides information about the benefits of ecosystems and how we can use Earth’s resources in ways that leave the environment healthy for future generations.\nRead more here\nThe distributed sensor networks allow for the acquisition of huge volumes of data on many relevant aspects, ranging from soil and vegetation characteristics, abiotic conditions like weather, to animals’ behavior. The collection of large amounts of data leads to a shift away from frequentist hypothesis testing towards analytics that is more focused on prediction, classification, pattern recognition or anomaly detection. To this end, machine learning techniques are often used, usually by high performance computing.\nIn the digital era, researchers are embracing data science, i.e., unifying data processing, statistics, artificial intelligence and their related algorithms to extract knowledge from data. Hence, data science is increasingly becoming an integral part of decision making in many fields, including ecology and wildlife conservation. To keep up with these steps, young scientists and students need to become acquainted with the terms, concepts and methodology of data science, including the integration and pre-processing of data from different sources, and the engineering of informative and discriminating features for creating effective algorithms.\nThis class covers the main elements using a data science approach to solve ecological problems. Students will be guided through the main concepts and skills that are required to become a successful data scientist. This class builds upon, and expands, the understanding and skills generated in other courses, and focuses on combining these in an interdisciplinary way to be optimally able to solve ecological problems with a data-driven approach. In the class, students will increase their knowledge and skills that will benefit their future career in academia.\nRead more here\n2. Preparing for the class\nMake sure to bring your own computer for this class, with access to school wifi and enough battery for three hours of work (or bring your power cable). Any operating system (Windows, Mac, Linux) is OK, as far as you are able to operate it.\nInstall the latest version of R and RStudio before you come for the first class - see instructions here.\nIf you used R and RStudio before, please make sure that you updated to the version required for this class; instructions on how to update and required R and RStudio versions are here.\nIt is essential that we all are using the same version of R and RStudio, to avoid the situation that you will get error messages just because of using outdated program versions.\nThis textbook provides many learning resources and reference materials for the class.\n3. Expected learning outcomes\nExperience with programming in R is needed to follow and successfully complete this course. The students without prior experience with programming in R are expected to master R programming in the class, including:\nMain types of R objects (vector, matrix, data frame, list), reading and exporting data, manipulating data (sorting, merging), creating fully reproducible R script\nHow to draw effective scientific figures, including how to choose them and draw them, as well as raster vs vector graphics\n\nAfter successful completion of this class, students are expected to be able to understand the significance of data science in solving typical ecological problems, including:\nunderstand how key features of ecological data influence the selection, training, validation and evaluation of algorithms;\nidentify and select machine learning algorithms appropriate to specific ecological problems;\napply data science skills (data processing, feature engineering, and machine learning algorithms) to analyse ecological data;\ncritically evaluate the results and performance of trained algorithms, and assess the reliability and adequacy of trained algorithms in predicting ecological phenomena;\ncreate ecological insight from data using a data science approach.\n\n4. Final evaluation\nThe final evaluation consists of three parts:\nHomework assignments (30%), see here\nActivity in the class, individual work (40%)\nFinal presentation (30%), see here\n\n\n\n",
      "last_modified": "2023-10-31T10:46:08+08:00"
    },
    {
      "path": "blog.html",
      "title": "My great Blog",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2023-10-31T10:46:08+08:00"
    },
    {
      "path": "data_exploration.html",
      "title": "Data exploratory analysis",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2023-10-31T10:46:09+08:00"
    },
    {
      "path": "general_overview.html",
      "title": "Overview: From data to models",
      "description": "The growth of Earth observation systems together with innovative data processing and analysis provide opportunities to monitor spatial–temporal changes of biodiversity and to evaluate the effects of human impact on ecological processes. In this section we will give a general introduce about the ecological data sources, and the methods for data pre-process and modelling with machine learning.\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\n1. Ecodata sources\n1.1 General introduce\n1.2 The data of biodiversity\n1.3 Eco-environmental data\n1.4 Citizen Science\n\n2. Exploratory Data Analysis\n2.1 Data integration\n2.2 Data pre-process\n2.3 Statistics analysis\n\n3. Machine learning models\n3.1 Decision tree\n3.2 Random forest\n3.3 Artificial Neural Networks\n\n\n1. Ecodata sources\n1.1 General introduce\nThere are lots of mix databases, websites, and tools to find interdisciplinary data for your ecological researches. For example, this iDigBIo website provides natural history museum and fossil records about specimens all over the world.\nThere is a growing need for baseline data to evaluate the changes in biodiversity at the community level and to distinguish these change that can be attributed to external factors, such as anthropogenic activities, from underlying natural change. Here lists three aspects of global long-term datasets that benefit to macroecological reseraches.\n1.2 The data of biodiversity\nThe changes of species abundance and distribution can be tracked using a long-term dataset, which is simply information on the variety, and ideally the abundance, of species (or other taxonomic units) at one or more locations at a number of points in time. Such datasets can be retrievered from many repositories:\nGBIF: This web provides an international network and data infrastructure funded by the world’s governments and aimed at providing anyone, anywhere, open access to data about all types of life on Earth.\nFragSAD: This dataset collects species abundance and distribution in habitat fragments from temperate forests and grasslands from all continents except Antarctica. The data include invertebrates, plants, birds, mammals, and reptiles and amphibians.\nBIEN: This web brings together many types of data needed to address several pressing ecological and global change questions. You access to the data of all plant species in the New World, including: 1) plant observations from herbarium and plots, and species geographic distribution maps; 2) vegetation and plot inventories and plant traits; 3) cross-continent, continent, and country-level species lists.\n1.3 Eco-environmental data\nThe general methods of environmental data has been derived from maps, which include GIS data and remote sensing images for investigation of environmental matrices, such as soil, water, vegetation, and air, etc., and a better understanding of ecological and environmental interactions to support the development of sustainable solutions.\nGIS data: This page provides a categorised list of links to over 500 sites providing freely available geographic datasets - all ready for loading into a Geographic Information System. The data include Boundaries, Elevation, Weather/climate, Hydrology, Snow/Ice, and Land Cover, etc.\nRemote sensing images: NASA has a wide variety of satellite and aerial remote sensing datasets and interactive maps at their Earthdata Search and other sites. In addition, Remote sensing of light emissions offers a unique perspective for investigations into some human behaviors. Day/Night Band (DNB) data are often used for estimating population, assessing electrification of remote areas, monitoring disasters and conflict, and understanding biological impacts of increased light pollution.\n1.4 Citizen Science\nCitizen Science can be used as a methodology where public volunteers help in collecting and classifying data, which are used extensively in studies of biodiversity and pollution (For details, see box1).\n\n\n\nBox1: Citizen science \n\n\nCitizen science, a two-way cooperation between the scientific and public communities in the long-term monitoring programs, is not new. One of the oldest citizen science programs is organised by the National Audubon Society. In the winter of 2000–2001, the participants came up to 52471 people in 1823 places involved 17 countries. Advances in electronic recording and communication systems via the web and mobile phones have led to a resurgence with projects like ProjectBudburst. Involving unpaid volunteers has the advantages of being economic, can extend the geographic range of study sites and the frequency of visits. Exploring the citizen science data collection can be optimized for biodiversity research.\n\n\n\n\n\n2. Exploratory Data Analysis\n2.1 Data integration\nVarious data sources provide information on the study system components that operate at different scales. Macro-ecology is the study of ecological patterns and processes at broad spatio-temporal scales. Such broad- and multi-scale research questions need combining disparate datasets from different data sources.\nThe most common challenges for data integration were: (1) mismatches in spatial or temporal scale of data sources, (2) differences in the quantity and/or information content of data sources, (3) sampling biases, and (4) optimizating model development and assessment. One of the task of data exploratory analysis (EDA) is to integrate data correctly.\n2.2 Data pre-process\nSome datasets contain missing data. You should properly handle them, either exclude them from your analysis or delete them. Also, datasets may include some outliers and features of the data that might be unexpected. You must understand where outliers occur and how variables are related, which can help designing statistical analyses that yield meaningful results.\n2.3 Statistics analysis\nAnother task of EDA is to identify general patterns in the data. The patterns should be revealed before further analysis. You can use descriptive statistics to complete such work. Scatterplots and correlation coefficients can provide useful information on relationships between pairs of variables. But when analyzing numerous variables, basic methods of multivariate visualization can provide greater insights. Mapping data also is critical for understanding spatial relationships among samples.\n3. Machine learning models\n3.1 Decision tree\nEcological data are specifically known to be non-linear and highly dimensional with intense interaction effects. To make statistic methods work, which assume linearity, researchers cope in various ways, including data transformations and break up systems into bits with fewer complexes. However, machine learning (ML) techniques have been shown to outperform traditional statistical approaches in solving the problems.\nLots of ML algorithms have been developed, including unsupervised ML and supervised ML. Unsupervised ML is often used to identify patterns in data. Supervised ML is used to train machine to learn from other examples for predicting a categorical outcome (classification) or a numeric outcome (regression), or inferring the relationships between the outcome and its explanatory variables.. Thus, supervised ML is widely used to make predictions and inferences\nThe widely used ML in ecology is tree-base algorithms. For these methods, a tree is built by iteratively splitting the data set based on a rule that results in the divided groups being more homogeneous than the group before.\nThe basic idea behind the algorithm is to find a point in the independent variable to split the data-set into 2 parts, so that the mean squared error (mse) is the minimized at that point. The algorithm does this in a repetitive fashion and forms a tree-like structure. Let’s consider a dataset where we have 2 variables, as shown below:\n\n\n\nThe first step is to sort X from min to max, and use the median of the first 2 points of X (i.e. (84 + 100)/2 = 92) to divide the dataset into 2 parts (Part A and Part B) , separated by x < 92 and X ≥ 92.\nThe averages of all Y values in Part A and those of all Y values in Part B are used to be the output of the decision tree for x < 92 and x ≥ 92 respectively. Based on the predicted and original values, calculate the mse according to the formula:\n\\[mse = \\frac{1}{n}\\sum_{i = 1}^{n} (y_i-\\hat{y_i})\\]\nWe repeat the procedure using for the second 2 points of X (i.e. (100 + 180)/2 = 140), and split the dataset into part A (X < 140) and part B (X ≥ 140) as predicted output. Based on the 2 parts, we calculate mse as step 1. Similarly, mse is repeatedly calcualted for the third 2 points of X, the fourth 2 points of X, …, till \\((n-1)^th\\) 2 points of X.\nNow that we have n-1 mse, and choose the point, at which the mse is smallest. In this case, the point is x = 2115. Thus, we can split the dataset into the x < 2115 and x ≥ 2115 parts. The first split can be obtained using rpart package with maxdepth = 1 as follows:\n\n\n\nWe can improve the tree by adding cross-validation like this:\n\n\n\nYou then obtain decision boundary by running the codes:\n\n\n\nThe left (x < 2115) and right (x ≥ 2115) parts are further recursively exposed to the same algorithm for the following split. When reducing mse, the tree can recursively split the data-set into a large number of subsets to the point where a set contains only one. Finally, we could obtain a piecewise function like below:\n\n\n\n3.2 Random forest\nRandom Forest is a relatively new tree-based method that fits a user-selected number of trees to a data set and then combines the predictions from all trees. The Random Forest algorithm creates a tree for a subsample of the data set. At every decision only a randomly selected subset of variables are used for the partitioning. The predicted output of an observation in the final tree is calculated by the majority of the predictions for that observation in all trees with ties split randomly.\nEnsemble tree-based methods, especially Random Forest, have been shown to outperform statistical and other ML methods in ecology applications. They can cope with small sample sizes, mixed data types, and missing data. The single-tree methods are fast to calculate and the results are easy to interpret, but they are susceptible to overfitting and need “pruning” of terminal nodes. The ensemble methods are computationally expensive, but resist overfitting. Random Forest can provide measures of relative variable importance and the data point similarity that can be useful in other analyses, but can be clouded by correlations between independent variables.\n3.3 Artificial Neural Networks\nMultiple percetion machine\nArtificial Neural Network (ANN) is a ML approach inspired by the way neurological systems process information. There are many ANN algorithms, which are supervised or unsupervised learners, but only a few are typically used in ecology. An ANN has three parts: 1) the input layer, 2) the hidden layer, and 3) the output layer.\nEach layer have several “neurons”. Each neuron is connected to the other neurons in the neighboring layer, but not the neurons in the same or in non-adjacent layers. The input layer contains a neuron for an independent variable. The output layer can have one neuron (for binary or continuous output) or more (for categorical output). The number of neurons in the hidden layer can be changed by the user to optimize the trade-off between overfitting and variance. Too many neurons in this layer can lead to overfitting. Each neuron has an activity level and each connection has a weight. The activity level of the input neurons are set by the value of the independent variable. Training the ANN involves an algorithmic search for an optimal set of connection weights that produces an output value with a small error relative to the observed value. Performance can be sensitive to initial connection weights and the number of hidden neurons, so multiple networks should be processed while varying these parameters.\nANN can be a powerful modeling tool when the underlying relationships are unknown and the data are imprecise and noisy. Interpretation of the ANN can be difficult and ANNs are often referred to as a “black box” method. Many ANNs mimic standard statistical methods, so a good practice while using ANNs is to also include a rigorous suite of validation tests and a general linear model for comparison.\nDeep learning\nA neural network comprises an input layer, a hidden layer, and an output layer. Deep learning, on the other hand, is made up of several hidden layers of neural networks that perform complex operations on massive amounts of structured and unstructured data.\n\n\n\n",
      "last_modified": "2023-10-31T10:46:10+08:00"
    },
    {
      "path": "getting_started.html",
      "title": "Working R with github",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\n1. Using Rstudio to interact with R\n1.1 Installing R and RStudio\n1.2 Updating R and RStudio\n1.3 Setting up RStudio Cloud\n1.4 Installing R packages\n\n2. Creating a R project\n2.1 Using a project to organize your work\n2.2 Working directory structure\n2.3 General settings\n2.4 Automatically running\n\n3. Connecting a R project to Github\n3.1 Checkinf that RStudio can find Git\n3.2 Adding a new R project to GitHub\n3.3 Adding an existing R project to GitHub\n\n4. Writing R code\n4.1 Using script\n4.2 Using rmarkdown\n4.3 Objects and operators\n\n\nYou need to install R and RStudion on your computers. We will go through the following steps to install two free programs. Also, you’ll understand basic R commands, and the RStudio interface with R in order to start programming.\n1. Using Rstudio to interact with R\n1.1 Installing R and RStudio\nR and RStudio are two separate pieces of software:\nR is a programming language that is especially powerful for data exploration, visualization, and statistical analysis\nRStudio is an integrated development environment (IDE) that makes using R easier\nIn this class we will use RStudio to interact with R. The following steps outline a simple and effective process for installing R and RStudio on your computers.\nWindows\nDownload R from CRAN. Run the .exe file that was just downloaded.\nNext to RStudio to download the RStudio Installer for Windows, and double click the file to install it.\nA good practice to install RTools, corresponding to R version, for building R and R packages from source on Windows.\nOnce installed, open RStudio to make sure it works and you don’t get any error messages.\nMacOS\nGo to CRAN, and select the <mark style=“background-color: #F0F0F0”.pkg file for the latest R version. Double click on the downloaded file to install R.\nA good idea to install XQuartz (needed by some packages).\nGo to RStudio to download the RStudio Installer for MacOS. Double click the file to install RStudio.\nOnce installed, open RStudio to make sure it works and you don’t get any error messages.\nLinux\nFollow the instructions from CRAN, run\nsudo apt-get install r-base for Debian/Ubuntu and sudo yum install R for Fedora, but the installed versions of R are usually out of date.\nGo to RStudio, select the version that matches your distribution to run sudo dpkg -i rstudio-YYYY.MM.X-ZZZ-amd64.deb for Debian/Ubuntu at the terminal.\nOnce installed, open RStudio to make sure it works and you don’t get any error messages.\nIf R and RStudio are installed, determine whether your R and RStudio versions are necessary to up to date.\n1.2 Updating R and RStudio\nUpdating R\nOpening RStudio, your R version will be printed in the console. Alternatively, you can type sessionInfo() into the console. If the R version is 4.0.0 or later, don’t need to update R for this class. If the version of R is older than that, download and install the latest version of R.\nIt is not necessary to remove old versions of R from your system,\nbut if you wish to do so you can check\nHow do I uninstall R?.\nNormally, your old code should still work after updating your R version. But if breaking changes happen, it is useful to know that you have multiple versions of R installed in parallel and that you can switch between them in RStudio by going to Tools > Global Options > General > Basic\nAfter installing a new version of R, you will have to reinstall all your packages with the new one. For Windows, there is a package called installr that can help you with upgrading your R version and migrate your package library.\nUpdating RStudio\nTo update RStudio, open RStudio and click on\nHelp > Check for Updates. If a new version is available, RStudio will automatically notify you every once in a while.\n1.3 Setting up RStudio Cloud\nIf it isn’t feasible to install R and RStudio Desktop on your computer, you can use RStudio Cloud and run R in an online browser window. For this purpose, you need to sign up and create a new account of RStudio Cloud.\n\n\nSetting up\nGo to https://rstudio.cloud/.\nSign up with an email and create a password of your choice.\nSign In.\nNavigating\nOnce log in, you should look for a few things. On the left hand side, you should see column that displays your “Spaces”, you can check for Learning, as well as some additional info on the system status and terms and conditions.\n\nOn the right hand side, you should see a small chart showing your Account Usage. This is what you want to keep track of. Depending on how much time you spend actually running code, your time will vary, but the standard free account provides 15 hours per month.\nGetting Setup\nClick on New Project, and wait a second for things to initialize.\nName your project.\nCopy and install the packages we’ll need.\nCheck everything is installed…you should get TRUE’s if everything worked.\n1.4 Installing R packages\nMost of the work in R is done by basic functions, which are wrapped into packages. Except for basic functions, R packages have build-in sample data sets.\nBy default, a set of R packages is installed during R installation. There also are many packages that are needed to install later from the central repositories like CRAN or Bioconductor, as well as developer repositories like R-Forge(https://r-forge.r-project.org/) or GitHub.\nInstalling R packages\nTo install R packages, open RStudio and copy and paste the following command into the console window, then execute the command.\nInstalling from CRAN\n\n\ninstall.packages(\"tidyverse\")\ninstall.packages(\"RSQLite\")\n# Alternatively\ninstall.packages(c(\"tidyverse\", \"RSQLite\"))\n\n\nInstalling from GitHub\n\n\ninstall.packages (\"devtools\")\ndevtools::install_github (\"grssnbchr/hexbin\")\n\n\nInstalling from R-Forge\n\n\n#install.packages(\"patchwork\", repos=\"http://R-Forge.R-project.org\")\n\n\nAlternatively, you can install the packages using RStudio interface by going to Tools > Install Packages and typing the names of the packages separated by a comma.\nWhen the installation has finished, you can try to load the\npackages by pasting the following code into the console:\n\n\nlibrary(tidyverse)\nlibrary(hexbin)\nlibrary(patchwork)\nlibrary(RSQLite)\n\n\nUpdating R packages\nIt is recommended to keep your R version and all packages up to date. To update the packages that you have installed, click Update in the Packages tab in the bottom right panel of RStudio, or go to Tools > Check for Package Updates....\n2. Creating a R project\n2.1 Using a project to organize your work\nWhen using R several years ago, it is ususally to first set a working directory using setwd(), which takes an absolute file path as an input and sets it as the current working directory of the R process, and to then use getwd() for finding out whether the current working directory is correctly set. The problem with this approach is that since setwd() relies on an absolute file path. This makes the links break very easily, and very difficult to share your analysis with others.\nAn RStudio project solves the problem of ‘fragile’ file paths by making file paths relative. An RStudio projects is the file that sits in the root directory, with the extension .Rproj. When your RStudio session is running through .Rproj, the current working directory points to the root folder where that .Rproj file is saved.\nThis .Rproj file can be created by going to File > New Project… in RStudio, which is then link a specified folder or directory that is stand-alone and portable. You can reading data from or writing data to files within this directory, except for cases where your analysis requires interacting with an Internet data source, such as web-scraping. When opening an existing project, you will open the .Rproj file and subsequently open R scripts (extensions with .R) from the RStudio session, rather than going to the R scripts to open them. There are lots of documents on RStudio projects, which have detail information on .RData and .Rhistory files.\n2.2 Working directory structure\nThis directory structure ‘template’ can provide a good starting point for organizing projects if workflow is new to you. However, different projects will have different needs, and thus one should think about what is needed and what will happen while setting up the working directory structure. A template of an R project like this:\nThe data folder\nThe data folder is is the subfolder where data are stored. They include any source files, such as SPSS, Excel/CSV or .RDS., and some generated ones. Someone would like to split the subfolder into three parts:\na data/raw/ folder, which is usually is symlinked to a folder that is read-only to the R user\na data/temp/ folder, which contains temp data\na data/output/ folder, if warranted\nThe src folder\nThis folder stores R script files (with the extension .R) and Rmarkdown ones (with the extension .Rmd) for data analysis and visualization. There are three types of R scripts:\nScripts: It is helpful to separate multiple scripts for different tasks on a single data set. Most analysis R scripts are saved here. But the key analysis script is better to be stored lonely.\nFunctions: It is optional whether you have your custom functions saved in a separate sub-folder. If you want to re-use a function that you remember you’ve written in a particular project, I can at a quick glance browse all the functions you’ve written for that project. Saving functions separately accompanies a workflow where you use source() to read functions into the ‘main analysis script’, rather than having it together with main analysis.\nRMarkdown: RMarkdown files are a special case, as they work slightly differently to .R files in terms of file paths, i.e. they behave like mini projects of their own, where the default working directory is where the Rmd file is saved. To save RMarkdown files in this set up, it’s recommended that you use the {here} package and its workflow. Alternatively, you can run knitr::opts_knit$set(root.dir = “../”) in your setup chunk so that the working directory is set in the root directory rather than another sub-folder where the RMarkdown file is saved (less ideal than using here package and its workflow). You briefly discussed a directory structure for combining multiple RMarkdown files into a single long RMarkdown document.\nThe output folder\nIn Output folder, save all your outputs here, including plots, HTML, and data exports.\nHaving this Output folder helps others identify what files are outputs of the code, as opposed to source files that were used to produce the analysis.\nWhat you have set up as the sub-folders don’t matter too much, as long as they’re sensible. Normally, the output folders structure as output/ figs/ or output/ plots/, rather than top-tier folders. They also include word or PDF report results. You may decide to set up the sub-folders so that they align with the analysis rather than type of file export.\nThe timed_fn() function from my package surveytoolbox (available on GitHub) helps create timestamps for file names, which you use often to ensure that you don’t lose work when you am iterating analysis.\n2.3 General settings\nThe requirement.R file: In this case, you should have a requirements.R file. Running it for fundamental settings you like to rely on, such as setting the locale appropriately. It also includes a CRAN install check script, although the Packrat package is advised to use.\nUsing keyring package: The keyring package, which interacts with OS X’s keychain, Windows’s Credential Store and the Secret Service API on Linux (where supported)\nUsing environment variables: Using environment variables to hold certain secrets has become extremely popular especially for Dockerised implementations of R code, as envvars can be very easily set using Docker. If you create an envfile called .Renviron in the working directory, it will store values in the environment.\nUsing a .gitignored secret file: config is a package that allows you to keep a range of configuration settings outside your code, in a YAML file, then retrieve them. You can create a default configuration for an API. A dedicated secrets file is a better place for credentials than a config file, as this file can then be wholesale .gitignore.\n2.4 Automatically running\nThere is a main runner script or potentially a small number. These go through scripts in a sequence. It is a sensible idea in such a case to establish sequential subfolders or sequentially numbered scripts that are executed in sequence. Typically, this model performs better if there are at most a handful distinct pipelines.\n3. Connecting a R project to Github\n3.1 Checkinf that RStudio can find Git\nThe first task is to ensure that Git can be located by RStudio on your machine. To do this, open RStudio and go to Tools > Global Options > Git/SVN.\nUnder “Git executable”, you should be able to see a path to Git. Take ubuntu as example, it will be in /usr/bin/git.\nIf Git is not in this location or you want to check where the Git executable path is, open the Command Prompt in ubuntu or Windows terminal. Type where git to reveal the Git executable file path.\nIf it doesn’t match the dialogue box in RStudio, click on “Browse…” and navigate to your Git executable file. Once complete, press “OK”.\n3.2 Adding a new R project to GitHub\nIf you want to start a new RStudio project and have it backed-up on GitHub, follow the following steps:\nFirstly, create an acount of Github, and a new repository on the GitHub website. You can choose public or private for your visibility setting\nNext, open up RStudio and go to File > New Project… > Version Control, and click on the “Git” option\nFill in the URL of the new GitHub repository that you just created in the “Repository URL”“Project Directory Name” will auto-fill. Click on “Open in New Session” and then click on “Create Project”. A new project window will open up in RStudio containing your new project. You will notice that it will contain some files under the “Files” window including .gitignore, .Rproj and README.md. The last file was pulled-down from your GitHub repository\nNext, to demonstrate how changes can be saved, you will create a new script file and add some code. This will then be saved locally. Following that, you will “push” my changes to GitHub so that my changes are also saved remotely. To do this, go to File > New File > R Script in RStudio. Write an R script and save it\nThis has saved your work to your computer, but not to GitHub. For saving to GitHub, go to the “Git” tab in the upper right pane. Check the “Staged” box for any files whose existence or modifications you want to commit\nClick on “Commit” and a new dialogue box will open. Under “Commit Message”, add a brief description of the changes that you have made\nClick on “Commit”. A Git Commit dialogue box will be displayed showing that the files are committed to GitHub. You may close this second window by clicking “Close”\nComplete the final step by pressing Push. This will upload the R files to GitHub. You will see a dialogue box come up confirming this in the form of a string followed by HEAD -> main\n3.3 Adding an existing R project to GitHub\nIf you have an existing project in RStudio and decide later that maintaining version control in GitHub would be a good idea, follow the steps:\nFirstly, create a new repository on GitHub via the website, and choose a repository name like above\nNext, open the Command Prompt (or ubuntu terminal) and go to the folder that contains your existing R project, then substitute the URL for your GitHub repository address for remote add origin and push by performing the following codes\n\n\n# git remote add origin https://github.com/yourusername/yourrepo\n# git pull origin main\n# git push -u origin main\n\n\n4. Writing R code\n4.1 Using script\nUsing R understand language to tell R what and how do the thing that you want. We refer this discription to “script”.\nCreating R script\nA script is simply a text file that contains a set of commands and comments. It can be saved and reused later. It can also be edited so you can execute a modified version of the commands.\nTo create a new script in RStudio, you can open a new empty script by clicking the New File–>New File Menu–>R Script. The script editor opens with an empty script, which is ready for text entry. Here is an example to familiarize you with the Script Editor interface.\n\nSaving R script\nYou can save your script by clicking on the Save icon at the top of the Script Editor panel. When you do that, a Save File dialog will open.\nThe default script name is Untitled.R. The Untitled part is highlighted. You will save this script as First script.R. Start typing First script. RStudio overwrites the highlighted default name with your new name, leaving the .R file extension.\nNotice that RStudio will save your script to your current working folder. Press the Save button and your script is saved to your working folder. Notice that the name in the file at the top of the Script Editor panel now shows your saved script file name.\nWhile it is not necessary to use an .R file extension for your R scripts, it does make it easier for RStudio to work with them if your use this file extension.\nOpening R script\nClick on the Open an existing file icon in the RStudio toolbar. A choose file dialog will open. Select the R script you want to open and click the Open button. Your script will open in the Script Editor panel with the script name in an editor tab.\nCommenting R script\nIn scripts, it can be very useful to save a bit of text which is not to be evaluated by R. You can leave a note to yourself about what the next line is supposed to do, what its strengths and limitations are, or anything else you want to remember later. To leave a note, we use “comments”, which are a line of text that starts with the hash symbol #. Anything on a line after a # will be ignored by R.\n\n\n# This is a comment. Running this in R will have no effect.\n\n\nExecuting R script\nThe Run button in the Script Editor panel toolbar will run either the current line of code or any block of selected code. You can use your First script.R code to gain familiarity with this functionality.\nPlace the cursor anywhere in line 3 of your script [x = 34]. Now press the Run button in the Script Editor panel toolbar. Three things happen: 1) the code is transferred to the command console, 2) the code is executed, and 3) the cursor moves to the next line in your script. Press the Run button three more times. RStudio executes lines 4, 5, and 6 of your script.\n4.2 Using rmarkdown\nLearn how to construct an RMarkdown file, please visit the site.\n4.3 Objects and operators\nR objects\nIn RStudio, if you want to create a object called x and give it a value of 4 using the symbol “<-”, we would write:\n\n\nx <- 4\nx\n\n[1] 4\n\nThe middle “<-” tells R to assign the value on the right to the object on the left. After running the command above, when running x in a command, it would be replaced by its value 4. If adding 3 to x, the expect would get 7.\n\n\nx + 3\n\n[1] 7\n\nObject in R can store more than just simple numbers. It can store lists of numbers, functions, graphics, etc., depending on what values get assigned to the object.\nYou can always reassign a new value to a object. If telling R that x is equal to 32:\n\n\nx <- 32\n\n\nthen x takes its new value:\n\n\nx\n\n[1] 32\n\nNaming objects and functions in R is pretty flexible. A name has to start with a letter, but that can be followed by letters or numbers. Names in R are case-sensitive, which means that Weights and weights are completely different things to R. A good idea is to give object names be as descriptive as possible, so that you will know what you meant later on when looking at it. Sometimes clear naming means that it is best to have multiple words in the name, but you can’t have spaces. A common approach is to chain the words with underscores, as in weights_before_hospital.\nR operators\nAn operator is a symbol that tells the compiler to perform specific mathematical or logical manipulations. R language is rich in built-in operators and provides following types of operators.\nArithmetic Operators: +, -, *, /\nRelational Operators: >, <, =, !=, etc\nLogical Operators: TRUE, FALSE\nAssignment Operators: <-\nMiscellaneous Operators: %in%, :, %*%\n\n\n\n",
      "last_modified": "2023-10-31T10:46:53+08:00"
    },
    {
      "path": "homework_finalpresent.html",
      "title": "Homework assignments and Final presentation",
      "description": "Homework assignments and Final presentation are two important parts of the class, each of which will give you 30% of the overall credit if you finish them all and submit them on time!\n",
      "author": [],
      "date": "`r Sys.Date()`",
      "contents": "\n\nContents\n1. Homework assignments\n1.1 Preparing assignments\n1.2 Rules for evaluation\n\n2. Final presentation\n2.1 Suggested topics\n2.2 How will we do it\n2.3 Rules for presentation\n\n3. Rules for evaluation\n\n1. Homework assignments\n1.1 Preparing assignments\nThere will be a total of 9-10 homework assignments during the semester, each counting for ~ 3% of the total evaluation.\nAlways be created and saved an R script as a *.R file, which is reproducible (see here what reproducible means)\nAlways be included your name (Chinese) and student ID, which is inserted as the first line of the R script like this:\n\n\n# Ecology Homework x, Author: your_name, Student ID: xxxxxxxx\n\n\nUsually be uploaded the R script into your github before the deadline. Otherwise, 50% of the grade is going down.\n1.2 Rules for evaluation\nEach homework is evaluated on a scale between 0 to 100% in the following way:\nSubmit the homework on time, and it is correct: 100%, but it has some flaws, we will minus the following:\nmissing header line: -10%\ngarbage code present, e.g. names (data), View (Df): -10%\nscript saved as a wrong file type: -20%\nscript not reproducible: -10%\nscript not clean and tidy: -10% (see here what clean and tidy script means)\n\nSubmit the homework late, but is correct but late: 50%\nSubmit the homework on time, but it is wrong: 30%\nNot submit the homework at all: 0%\n2. Final presentation\n2.1 Suggested topics\nFree to choose what topic you want to present, considering that it fits the following criteria:\nthe topic is related to your interest/study\nyou solved it in R\nit is reasonably simple that everyone in the class can have at least a rough idea of what is going on\n2.2 How will we do it\nLast two classes will be focused on these presentations and discussions. Half of the students will present the first and half the second class\nEach presentation is very brief with exactly four slides (saved this talk in pdf)\nAfter the section with talks, we will start a personal discussion. At that moment, you will be either presenter or reviewer. Each presenter will be evaluated by teachers and several other reviewers\nEach presenter will sit with her/his computer at the table with an empty chair beside it, and reviewers will come for discussion. Each reviewer will have assigned presenters she/he has to visit, plus can visit some others and discuss.\nAs a presenter, you need to present your project and then be available for discussion\nAs a reviewer, you need to be present at the class for talks and for personal review\n2.3 Rules for presentation\nPrepare a presentation with exactly four slides:\nSlide 1: Opening slide with the title of your presentation, your name (Chinese), your major and the year of the study\nSlide 2: Brief introductory slide - what’s going on, what kind of data and what kind of problem/method\nSlide 3: Results - what came out? (figures, numbers, or something else)\nSlide 4: the highlights of the R script used for the presentation. The highlights may include important libraries and functions used in the script or important sections of the R code. Do not copy the whole R code!\nPresentation slides, presentations and R code annotation should be in English\nThe presentation should take 3 minutes\nPlease check the pdf file; please, make sure you upload your slides and R code before the deadline!\nR code should be fully reproducible, but it should be as tidy as possible.\nNote that all presentations and all R codes will be made available to all students within the class!\n3. Rules for evaluation\nThe evaluation will consist of two equal parts: evaluation by classmates and evaluation by me. As a reviewer, you need to evaluate four criteria (presentation, idea, whether they understand their R code, and whether the R code is correct, tidy and clean, see below)\nRules for evaluation (max 30% of overall class score):\nitems\ncriteria\nscores\nPresentation\nWhether to relate, and whether to attract the attention. If not, leave blank.\n6%\nIdea\nHow interesting is the idea, in your opinion?\n6%\nR code\nWhether and how the R code works, if clearly, is ok\n6%\nR code correct, tidy & clean?\nCan the script run, and is it reproducible? Is it free of mistakes?\n6%\nReviewing others\nDoing and returning the review of 5 presenters assigned to you as reviewers\n6%\nTotal\n\n30%\n\n\n\n",
      "last_modified": "2023-10-31T10:46:54+08:00"
    },
    {
      "path": "index.html",
      "title": "Data-Driven Ecology",
      "author": [],
      "contents": "\n\n          \n          \n          Home\n          \n          \n          About the Class\n          Getting Started\n          \n          \n          Lessons\n           \n          ▾\n          \n          \n          General overview\n          Data exploratory analysis\n          Machine learning models\n          \n          \n          Blog\n          ☰\n          \n          \n      \n        \n          \n            \n              \n            \n              Data-Driven Ecology\n            \n            \n              \n                \n                    \n                      \n                         GitHub\n                      \n                    \n                  \n                                    \n                    \n                      \n                         Email\n                      \n                    \n                  \n                                  \n            \n          \n        \n        \n        \n          \n            \n            Welcome!\n            \n            \n            \n            \n            This class walks participants through the steps required\n            to use R for a wide array of data analysis relevant to\n            research in biology and ecology.\n            The core skills we will cover are as follows:\n            best practices in data retriever, management, and\n            visualization\n            basic knowledge about data analysis with machine\n            learning methods\n            beneficial practices in modeling complex patterns and\n            processes\n            The class is extremely participatory, where a lot of\n            training modules are delivered based on the learner’s\n            needs.\n            \n          \n        \n      \n    \n\n    \n      \n        \n          \n            \n              \n            \n              Data-Driven Ecology\n            \n            \n              \n                \n                                    \n                    \n                       GitHub\n                    \n                  \n                                    \n                    \n                       Email\n                    \n                  \n                                  \n              \n            \n            \n              \n              Welcome!\n              \n              \n              \n              \n              This class walks participants through the steps\n              required to use R for a wide array of data analysis\n              relevant to research in biology and ecology.\n              The core skills we will cover are as follows:\n              best practices in data retriever, management, and\n              visualization\n              basic knowledge about data analysis with machine\n              learning methods\n              beneficial practices in modeling complex patterns and\n              processes\n              The class is extremely participatory, where a lot of\n              training modules are delivered based on the learner’s\n              needs.\n              \n            \n        \n      \n    \n\n    \n    \n    ",
      "last_modified": "2023-10-31T10:47:42+08:00"
    },
    {
      "path": "ml_models.html",
      "title": "ML_models",
      "author": [],
      "contents": "\n\n\n\n",
      "last_modified": "2023-10-31T10:47:43+08:00"
    },
    {
      "path": "README.html",
      "author": [],
      "contents": "\nEcodatasci: A primary tutorial of the method for data-intensive ecology\nCourse Summary\nUsing data-intensive method for ecology researches is increasingly important. In this course you will learn how to explore the patterns of ecological data and model ecological process with R, which is one of the most popular tools to accomplish this. For the majority of this course, you will learn everything need to setup your projects using R, as well as runing Python in Rstudio with the reticulate package, which allows R to talk to Python.\nCourse outline and topics\nWelcome to the course\nR basic and machine learning algorithms\nExploratory data analysis\nCommon machine learning\nAdvanced mchine learning\nReprodutive workflow\nConclusion\n",
      "last_modified": "2023-10-31T10:47:43+08:00"
    }
  ],
  "collections": ["posts/posts.json", "datexpl/datexpl.json", "models/models.json"]
}
