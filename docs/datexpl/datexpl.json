[
  {
    "path": "datexpl/2023-10-15-datamaniplation/",
    "title": "Lesson 1: Data manipluation",
    "description": "In this lesson, you will learn what types of data and how to maniplate data, including data import and export, as well as simple statistic analysis.",
    "author": [],
    "date": "2023-10-15",
    "categories": [],
    "contents": "\n\nContents\n1. Acquring data from R\n1.1 The data within R\n1.2 The data outside R\n\n2. Data types and manipulation\n2.1 Vectors and maniplation\n2.2 lists and manipluation\n2.3 Matrics and manipulation\n2.4 Arrays and manipulation\n2.5 Factors and manipulation\n2.6 Data frames and manipulation\n\n3. Data visualization in R\n3.1 Pie charts\n3.2 Bar charts\n3.3 Boxplots\n3.4 Histograms\n3.5 Line Charts\n3.6 Scatterplots\n\n\n1. Acquring data from R\n1.1 The data within R\nThere are several ways to find the included datasets in R:\nUsing data() to list the datasets of all loaded packages (not only the ones from the datasets package); the datasets are ordered by package\nUsing data(package = .packages(all.available = TRUE)) to list all datasets in the available packages on your computer (i.e. also the not-loaded ones)\nUsing data(package = “packagename”) to list the datasets built in the package, so data(package = “plyr”) will give the datasets in the plyr package\n1.2 The data outside R\nIn R you can read data from files stored outside the R environment. You can also write data into files which will be stored and accessed by computers. R can read and write into various file formats like csv, excel, xml etc.\nThe file should be present in current working directory so that R can read it. You can set our own directory and read files from there. You can use the getwd() function to check current directory, and also use setwd()functional to set a new working directory.\n\n[1] \"D:/education-website/_data_exploration/2023-10-15-datamaniplation\"\n\nA CSV or excel File\nThe csv file is a text file in which the values in the columns are separated by a comma. You can use read.csv() function to read it into R.\nMicrosoft Excel is the most widely used spreadsheet which stores data in the .xls or .xlsx format. R can read directly from the files using some specific packages, such as xlsx package.\n\n[1] FALSE\n\nFrom the web site\nMany websites can provide data. For example, WHO provides reports on health and medical information in the form of CSV, txt and XML files. Using R, we can programmatically extract data from websites. Some R packages, such as “RCurl”, “XML”, and “stringr”, are used to connect to the URL’s, identify required links for the data and download them to R environment.\n\n\n\nFor example, if you visit the URL weather data and download the CSV files using R for the year 2015.\n\n\n\nFrom the databases\nThe data is Relational database systems are stored in a normalized format. So, to carry out statistical computing you will need very advanced and complex Sql queries. But R can connect easily to many relational databases like MySql, Oracle, Sql server etc. and fetch records from them as a dataframe. Once the data is available in the R environment, it becomes a normal R data set and can be manipulated or analyzed using packages and functions. Below you will be using MySql as our reference database for connecting to R.\nOnce the RMySQL package is installed we create a connection object in R to connect to the database. It takes the username, password, database name and host name as input.\n\n\n\nYou can query the database tables in MySql using the dbSendQuery() function. The query executs in MySql and the result is returned using the fetch() function. Finally it is stored as a dataframe in R.\n\n\n\nYou can pass any valid select query to get the result.\n\n\n\nYou can update the rows in a Mysql table by passing the update query to the dbSendQuery() function.\n\n\n\nYou can create tables in the MySql using the dbWriteTable() function. It overwrites the table if it already exists and takes a dataframe as input.\n\n\n\nYou can drop the tables in MySql database passing the drop table statement into the dbSendQuery() in the same way as we used it for querying data from tables.\n\n\n\n2. Data types and manipulation\nGenerally, you may store information of data types like character, integer, floating point, and Boolean, etc. Based on the data type of a variable, the operating system allocates memory and decides what can be stored. There are many types of R-objects. The frequently used ones are:\nVectors\nLists\nMatrices\nArrays\nFactors\nData Frames\n2.1 Vectors and maniplation\nVectors\nIn R the very basic data types are the R-objects called vectors. When creating a vector with more than one element, you should use c() function which means to combine the elements into a vector.\n\n\n# Create a vector.\napple <- c('red','green',\"yellow\")\nprint(apple)\n\n[1] \"red\"    \"green\"  \"yellow\"\n\nVector manipulation\nTwo vectors of same length can be added, subtracted, multiplied or divided giving the result as a vector output.\n\n[1]  7 19  4 13  1 13\n[1] -1 -3  4 -3 -1  9\n[1] 12 88  0 40  0 22\n[1] 0.7500000 0.7272727       Inf 0.6250000 0.0000000 5.5000000\n\nElements in a vector can be sorted using the sort() function.\n\n[1]  -9   0   3   4   5   8  11 304\n\n\n[1] 304  11   8   5   4   3   0  -9\n\n2.2 lists and manipluation\nLists\nA list is an R-object which can contain many different types of elements inside it like vectors, functions and even another list inside it.\n\n\n# Create a list.\nlist1 <- list(c(2,5,3),21.3,sin)\n\n# Print the list.\nprint(list1)\n\n[[1]]\n[1] 2 5 3\n\n[[2]]\n[1] 21.3\n\n[[3]]\nfunction (x)  .Primitive(\"sin\")\n\nList manipulation\nA list can be converted to a vector so that the elements of the vector can be used for further manipulation. All the arithmetic operations on vectors can be applied after the list is converted into vectors. To do this conversion, we use the unlist() function. It takes the list as input and produces a vector.\n\n[[1]]\n[1] 1 2 3 4 5\n[[1]]\n[1] 10 11 12 13 14\n\n\n[1] 1 2 3 4 5\n[1] 10 11 12 13 14\n[1] 11 13 15 17 19\n\n2.3 Matrics and manipulation\nMatrices\nA matrix is a two-dimensional rectangular R-object. It can be created using a vector input to the matrix function.\n\n\n# Create a matrix.\nM = matrix(c('a','a','b','c','b','a'), nrow = 2, ncol = 3, byrow = TRUE)\nprint(M)\n\n     [,1] [,2] [,3]\n[1,] \"a\"  \"a\"  \"b\" \n[2,] \"c\"  \"b\"  \"a\" \n\nMatrix manipulation\nCreate a matrix taking a vector of numbers as input.\n\n     [,1] [,2] [,3]\n[1,]    3    4    5\n[2,]    6    7    8\n[3,]    9   10   11\n[4,]   12   13   14\n     [,1] [,2] [,3]\n[1,]    3    7   11\n[2,]    4    8   12\n[3,]    5    9   13\n[4,]    6   10   14\n     col1 col2 col3\nrow1    3    4    5\nrow2    6    7    8\nrow3    9   10   11\nrow4   12   13   14\n\nVarious mathematical operations are performed on the matrices using the R operators. The result of the operation is also a matrix.\n\n     [,1] [,2] [,3]\n[1,]    3   -1    2\n[2,]    9    4    6\n     [,1] [,2] [,3]\n[1,]    5    0    3\n[2,]    2    9    4\nResult of addition \n     [,1] [,2] [,3]\n[1,]    8   -1    5\n[2,]   11   13   10\nResult of subtraction \n     [,1] [,2] [,3]\n[1,]   -2   -1   -1\n[2,]    7   -5    2\n\n2.4 Arrays and manipulation\nArrays\nWhile matrices are confined to two dimensions, arrays can be of any number of dimensions. The array function takes a dim attribute which creates the required number of dimension. Below is an example array with two elements which are 3x3 matrices each. Arrays can store data in more than two dimensions.\n\n\n# Create an array.\na <- array(c('green','yellow'),dim = c(3,3,2)) #  2 matrices each with 3 rows and 3 columns\nprint(a)\n\n, , 1\n\n     [,1]     [,2]     [,3]    \n[1,] \"green\"  \"yellow\" \"green\" \n[2,] \"yellow\" \"green\"  \"yellow\"\n[3,] \"green\"  \"yellow\" \"green\" \n\n, , 2\n\n     [,1]     [,2]     [,3]    \n[1,] \"yellow\" \"green\"  \"yellow\"\n[2,] \"green\"  \"yellow\" \"green\" \n[3,] \"yellow\" \"green\"  \"yellow\"\n\nArray Manipulation\nArrays are the R-objects that can store data in more than two dimensions. If we create an array of dimension (2, 3, 4) then it creates 4 rectangular matrices each with 2 rows and 3 columns. Arrays can store only data type.\nAn array is created using the array() function. It takes vectors as input and uses the values in the dim parameter to create an array.\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    5   10   13\n[2,]    9   11   14\n[3,]    3   12   15\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    5   10   13\n[2,]    9   11   14\n[3,]    3   12   15\n\n2.5 Factors and manipulation\nFactors\nFactors are the R-objects which are created using a vector. It stores a vector along with the distinct values of the elements in the vector as labels. The labels are always character irrespective of whether it is numeric or character or Boolean etc. in the input vector. They are useful in statistical modeling.\nFactors are created using the factor() function. The nlevels() functions gives the count of levels.\n\n\n# Create a vector\napple_colors <- c('green','green','yellow','red','red','red','green')\n\n# Create a factor object\nfactor_apple <- factor(apple_colors)\n\n# Print the factor\nprint(factor_apple)\n\n[1] green  green  yellow red    red    red    green \nLevels: green red yellow\n\nprint(nlevels(factor_apple))\n\n[1] 3\n\nFactor Manipulation\nFactors are the R-objects that are used to categorize the data and store it as levels. They can store both strings and integers. They are useful in the columns that have a limited number of unique values. Like “Male,”Female” and True, False etc. They are useful in data analysis for statistical modeling.\n\n [1] \"East\"  \"West\"  \"East\"  \"North\" \"North\" \"East\"  \"West\"  \"West\" \n [9] \"West\"  \"East\"  \"North\"\n[1] FALSE\n [1] East  West  East  North North East  West  West  West  East  North\nLevels: East North West\n[1] TRUE\n\n2.6 Data frames and manipulation\nData Frames\nData frames are tabular data objects. Unlike a matrix in data frame each column can contain different modes of data. The first column can be numeric while the second column can be character and third column can be logical. It is a list of vectors of equal length.\nData Frames are created using the data.frame() function.\n\n\n# Create the data frame.\nBMI <-   data.frame(\n   gender = c(\"Male\", \"Male\",\"Female\"), \n   height = c(152, 171.5, 165), \n   weight = c(81,93, 78),\n   Age = c(42,38,26)\n)\nprint(BMI)\n\n  gender height weight Age\n1   Male  152.0     81  42\n2   Male  171.5     93  38\n3 Female  165.0     78  26\n\nData frame Manipulation\nA data frame is a table or a two-dimensional array-like structure in which each column contains values of one variable and each row contains one set of values from each column. You can use data.frame() function to create a dataframe.\n\n  emp_id emp_name salary start_date\n1      1     Rick 623.30 2012-01-01\n2      2      Dan 515.20 2013-09-23\n3      3 Michelle 611.00 2014-11-15\n4      4     Ryan 729.00 2014-05-11\n5      5     Gary 843.25 2015-03-27\n\nThe structure of the data frame can be seen by using str() function.\n\n'data.frame':   5 obs. of  4 variables:\n $ emp_id    : int  1 2 3 4 5\n $ emp_name  : chr  \"Rick\" \"Dan\" \"Michelle\" \"Ryan\" ...\n $ salary    : num  623 515 611 729 843\n $ start_date: Date, format: \"2012-01-01\" ...\n\nThe statistical summary and nature of the data can be obtained by applying summary() function.\n\n     emp_id    emp_name             salary        start_date        \n Min.   :1   Length:5           Min.   :515.2   Min.   :2012-01-01  \n 1st Qu.:2   Class :character   1st Qu.:611.0   1st Qu.:2013-09-23  \n Median :3   Mode  :character   Median :623.3   Median :2014-05-11  \n Mean   :3                      Mean   :664.4   Mean   :2014-01-14  \n 3rd Qu.:4                      3rd Qu.:729.0   3rd Qu.:2014-11-15  \n Max.   :5                      Max.   :843.2   Max.   :2015-03-27  \n\nExtract specific column from a dataframe using column name.\n\n  emp.data.emp_name emp.data.salary\n1              Rick          623.30\n2               Dan          515.20\n3          Michelle          611.00\n4              Ryan          729.00\n5              Gary          843.25\n\nExtract the first two rows and then all columns.\n\n  emp_id emp_name salary start_date\n1      1     Rick  623.3 2012-01-01\n2      2      Dan  515.2 2013-09-23\n\nExtract 3rd and 5th row with 2nd and 4th column.\n\n  emp_name start_date\n3 Michelle 2014-11-15\n5     Gary 2015-03-27\n\nA dataframe can be expanded by adding columns and rows.\n\n  emp_id emp_name salary start_date       dept\n1      1     Rick 623.30 2012-01-01         IT\n2      2      Dan 515.20 2013-09-23 Operations\n3      3 Michelle 611.00 2014-11-15         IT\n4      4     Ryan 729.00 2014-05-11         HR\n5      5     Gary 843.25 2015-03-27    Finance\n\nTo add more rows permanently to an existing data frame, we need to bring in the new rows in the same structure as the existing data frame and use the rbind() function.\n\n  emp_id emp_name salary start_date       dept\n1      1     Rick 623.30 2012-01-01         IT\n2      2      Dan 515.20 2013-09-23 Operations\n3      3 Michelle 611.00 2014-11-15         IT\n4      4     Ryan 729.00 2014-05-11         HR\n5      5     Gary 843.25 2015-03-27    Finance\n6      6    Rasmi 578.00 2013-05-21         IT\n7      7   Pranab 722.50 2013-07-30 Operations\n8      8    Tusar 632.80 2014-06-17   Fianance\n\nData reshape\nData Reshaping in R is about changing the way data is organized into rows and columns. Most of the time data processing in R is done by taking the input data as a dataframe. It is easy to extract data from the rows and columns of a data frame but there are situations when we need the dataframe in a format that is different from format in which we received it. R has many functions to split, merge and change the rows to columns and vice-versa in a data frame.\nYou can join multiple vectors to create a data frame using the cbind()function. Also we can merge two data frames using rbind() function.\n\n# # # # The First data frame\n     city       state zipcode\n[1,] \"Tampa\"    \"FL\"  \"33602\"\n[2,] \"Seattle\"  \"WA\"  \"98104\"\n[3,] \"Hartford\" \"CT\"  \"6161\" \n[4,] \"Denver\"   \"CO\"  \"80294\"\n# # # The Second data frame\n       city state zipcode\n1     Lowry    CO   80230\n2 Charlotte    FL   33949\n# # # The combined data frame\n       city state zipcode\n1     Tampa    FL   33602\n2   Seattle    WA   98104\n3  Hartford    CT    6161\n4    Denver    CO   80294\n5     Lowry    CO   80230\n6 Charlotte    FL   33949\n\nYou can merge two dataframes by using the merge() function. The data frames must have same column names on which the merging happens.\nIn the example below, you consider the data sets about Diabetes in the MASS package. we merge the two data sets based on the values of blood pressure (“bp”) and body mass index (“bmi”). To choose these two columns for merging, the records where values of two variables match in both data sets are combined together to form a single data frame.\n\n   bp  bmi npreg.x glu.x skin.x ped.x age.x type.x npreg.y glu.y\n1  60 33.8       1   117     23 0.466    27     No       2   125\n2  64 29.7       2    75     24 0.370    33     No       2   100\n3  64 31.2       5   189     33 0.583    29    Yes       3   158\n4  64 33.2       4   117     27 0.230    24     No       1    96\n5  66 38.1       3   115     39 0.150    28     No       1   114\n6  68 38.5       2   100     25 0.324    26     No       7   129\n7  70 27.4       1   116     28 0.204    21     No       0   124\n8  70 33.1       4    91     32 0.446    22     No       9   123\n9  70 35.4       9   124     33 0.282    34     No       6   134\n10 72 25.6       1   157     21 0.123    24     No       4    99\n11 72 37.7       5    95     33 0.370    27     No       6   103\n12 74 25.9       9   134     33 0.460    81     No       8   126\n13 74 25.9       1    95     21 0.673    36     No       8   126\n14 78 27.6       5    88     30 0.258    37     No       6   125\n15 78 27.6      10   122     31 0.512    45     No       6   125\n16 78 39.4       2   112     50 0.175    24     No       4   112\n17 88 34.5       1   117     24 0.403    40    Yes       4   127\n   skin.y ped.y age.y type.y\n1      20 0.088    31     No\n2      23 0.368    21     No\n3      13 0.295    24     No\n4      27 0.289    21     No\n5      36 0.289    21     No\n6      49 0.439    43    Yes\n7      20 0.254    36    Yes\n8      44 0.374    40     No\n9      23 0.542    29    Yes\n10     17 0.294    28     No\n11     32 0.324    55     No\n12     38 0.162    39     No\n13     38 0.162    39     No\n14     31 0.565    49    Yes\n15     31 0.565    49    Yes\n16     40 0.236    38     No\n17     11 0.598    28     No\n[1] 17\n\nOne of the most interesting aspects of R is about changing the shape of the data in multiple steps to get a desired shape. The functions used to do this are called melt() and cast(). We consider the dataset called ships in the MASS package.\n\n\n\nYou can cast the molten data into a new form where the aggregate of each type of ship for each year is created. It is done using the cast() function.\n3. Data visualization in R\n3.1 Pie charts\nIn R the pie chart is created using the pie() function which takes positive numbers as a vector input.\n\n\n\n3.2 Bar charts\nThe below script will create and save the bar chart in the current R working directory.\n\n\n\n3.3 Boxplots\nBoxplots are created in R by using the boxplot() function.\n\n\n\n3.4 Histograms\nR creates histogram using hist() function. This function takes a vector as an input and uses some more parameters to plot histograms.\n\n\n\n3.5 Line Charts\nA line chart is a graph that connects a series of points by drawing line segments between them. The plot() function is used to create the line graph.\n\n\n\n3.6 Scatterplots\nScatterplots show many points plotted in the Cartesian plane. Each point represents the values of two variables. One variable is chosen in the horizontal axis and another in the vertical axis. The simple scatterplot is created using the plot() function.\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2023-10-25T09:38:16+08:00",
    "input_file": {}
  },
  {
    "path": "datexpl/2023-10-15-datavisualization/",
    "title": "Lesson 2: Data visualization",
    "description": "This section helps you create the most popular visualizations - from quick and dirty plots to publication-ready graphs. The text relies heavily on the ggplot2 package for graphics, but other approaches covered as well.",
    "author": [],
    "date": "2023-10-15",
    "categories": [],
    "contents": "\n\nContents\nLoad the required packages\n\nThis dataset contains four lists: env, fish, xy, species. Of these, the fish file lists all species that were recorded in the Doubs River. The data can be extracted as:\n\n\ndata(doubs, package = \"ade4\")\nDoubsSpe <- doubs$fish\nDoubsEnv <- doubs$env\nDoubsSpa <- doubs$xy\n\n\nLoad the required packages\nlibrary(vegan)\nlibrary(labdsv)\nlibrary(MASS)\nlibrary(mvpart)\nlibrary(ggplot2)\nIn ecological data, it’s common that a site or plot has 0’s value for any organisms. If so, you should remove it out of your dataset, and do not include the site for your analysis. As for the data of doubs, you can detect 0’s value like this:\n\n\nrowSums(DoubsSpe) == 0\n\n    1     2     3     4     5     6     7     8     9    10    11 \nFALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE \n   12    13    14    15    16    17    18    19    20    21    22 \nFALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE \n   23    24    25    26    27    28    29    30 \nFALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE \n\n#colSums(DoubsSpe) == 0\n\n\nBecause the site no. 8 contains no species, so we remove the row 8 and corresponding abiotic data (site 8), and assign the data new variables:\n\n\nspe <- DoubsSpe[-8, ]\nenv <- DoubsEnv[-8, ]\n\n\nExplore the environment dataset and calculate correlation efficients among variables. See the website for details.\n\n\n\nThe hclust() for hierarchical clustering order is used to reorder the correlation matrix according to the correlation coefficient. This is useful to identify the hidden pattern in the matrix.\n\n\n\nReordered correlation data:\n\n\n# Reorder the correlation matrix\ncormat <- reorder_cormat(cormat)\n\n\nThe correlation matrix has redundant information. We’ll use the codes below to set half of it to NA.\n\n\n# Get upper triangle of the correlation matrix\nget_upper_tri <- function(cormat){\n    cormat[lower.tri(cormat)] <- NA\n    return(cormat)\n  }\nupper_tri <- get_upper_tri(cormat)\n#upper_tri\n\n\nMelt the correlation data, and drop the rows with NA values:\n\n\n# Melt the correlation matrix and drop NA\nlibrary(reshape2) #The package reshape used to melt the correlation matrix\nmelted_cormat <- melt(upper_tri, na.rm = TRUE) # na.rm-->drop NA\n#head(melted_cormat)\n\n\nAdd mort details to plot for improving the figure and perform the codes below.\n\n\n# Create a heatmap\nlibrary(ggplot2)\nggheatmap <- ggplot(melted_cormat, aes(Var2, Var1, fill = value)) +\n   geom_tile(color = \"white\") + \n  # using geom_title() to create heetmap, \"color\" means the grid line color\n scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n   midpoint = 0, limit = c(-1,1), space = \"Lab\", \n    name=\"Pearson\\nCorrelation\") + \n  # use scale_fill_gradient2 () to creates a color gradient (low-mid-high), here bule notes egative correlations and red notes positive correlations. limit = c(-1,1) as correlation coefficients range from -1 to 1. See legend for details.\n  theme_minimal() + # Create a white background with no grid lines and no border\n  theme(axis.text.x = element_text(angle = 0, vjust = 1, \n    size = 10, hjust = 1)) + # x axis tick mark labels \n coord_fixed() # ensures one unit on the x-axis is the same length as one unit on the y-axis\n\n# Print the heatmap\nprint(ggheatmap)\n\n\n\n\n\n\n",
    "preview": "datexpl/2023-10-15-datavisualization/distill-preview.png",
    "last_modified": "2023-10-25T09:38:16+08:00",
    "input_file": {}
  },
  {
    "path": "datexpl/2023-10-15-exploratoryanalysis/",
    "title": "Lesson 3: Exploratory Analysis",
    "description": "Data exploration is an important step in data science. It mainly includes traditional statistical analysis to interpret data patterns, to find stories and to gain instights.",
    "author": [],
    "date": "2023-10-15",
    "categories": [],
    "contents": "\n\nContents\n1. Raw data and preprocessing\n1.1 Structures\n1.2 Preprocessing\n\n2. Descriptive statistics\n2.1 Species data distribution\n2.2 Species data standardization\n2.3 Species data transformations\n2.4 Environmental data VS. collinearity\n\n3. Preliminary analysis\n3.1 Association measures\n3.2 Clustering\n\n4. Advance analysis\n4.1 General introduction\n4.2 Unconstrained Ordination\n\n\nThis section is adopted from the module, the tutorial and the book.\n1. Raw data and preprocessing\n1.1 Structures\nA community sample (plot, sample, etc), represent presence/absence or quantity (count, cover or biomass) of each species in each sample. Three matrices are used to delineate these data, i.e., the matrix of species composition (L matrix, sample × species), the matrix of sample attributes (R matrix, sample × sample attributes, with environmental variables, ), and the matrix of species attributes, like species traits or species indicator values (Q matrix, species × species attributes).\n\n1.2 Preprocessing\nAfter the data have been imported into R, it is useful to explore data first, check for missing values and outliers, check for range and type of species and environmental data, apply transformation or standardization if necessary, check for possible correlations between environmental variables etc.\nMissing and 0 values\nMissing data are elements in the matrix with no value, in R usually replaced by NA (not available). Note that there is an important difference between 0 and NA (e.g. species was not recorded and gets zero cover or abundance). For example, a pH-meter got broken and didn’t measure pH in some samples, you should not replace the value by 0, since it does not mean that the pH of that sample is so low.\nAs for the samples with missing values, if there are lots of missing values scattered across different variables, the analysis will be based on rather few samples. One way to reduce this effect is to remove the variables with the highest proportion of missing values from the analysis. Another option is to replace the missing values by estimates if these could be reasonably accurate (mostly by interpolation, e.g. from similar plots, neighbours, values measured at the same time somewhere close, or values predicted by a model).\nis.na() will work on individual values, vectors, lists, and data frames. It will return TRUE or FALSE where you have an NA.\n\n\ndf <- read.csv('missd_exam.csv')\n# df\n# is.na(df)\nnames(df) # the column names\n\n[1] \"example\" \"data\"    \"set\"    \n\nsum(is.na(df)) # How many NAs in my data frame\n\n[1] 3\n\nYou can use sum() and which() to figure out where NAs locate, and finally remote them, like this:\n\n\n#which(is.na(df))\nwhich(is.na(df$data)) # Which row contains an NA in the 'data' column\n\n[1] 4\n\n# na.omit(df) # remove the rows with NAs\n\n\nAs for the 0’s values, you should delete them if they affect results. Take the doubs dataset as an example. This data set gives environmental variables, fish species and spatial coordinates for 30 sites.doubs is a list with 4 components.\n\n\ndata(doubs, package = \"ade4\") # load data from ade4 package\n# class(doubs) # find the object types\n# names(doubs) # check names of the list\n\n\nThen read the species, environment, distribution data from the list:\n\n\n#Species and environment data from doubs\nspecies <- doubs$fish\n# head(spe)\n# which(is.na(species)) # check missing values in a data set\n# rowSums(species) == 0 # check the site without any species\nspe <- species[-8,] # the site 8 without species is removed\n\nenvironment <- doubs$env\nenv <- environment[-8,] # remove corresponding abiotic data for the site 8 \n\n\nOutliers\nOutliers are those values within a given variable that are conspicuously different from other values. Outlier value could get quite influential in the analysis, so it is worth to treat it in advance. First, spending a reasonable time to ensure that such value is not a mistype. If the sample really describes conditions that are rather different from the rest of the data set, it may be reasonable to remove them, since there may not be enough replications to describe this difference or phenomena.\nThere is a number of ways how to detect outliers. A simple exploratory data analysis (EDA) could reveal it graphically, e.g. using a box plot or a histogram. In a box plot, the outlier is defined as a value 1.5 times of interquartile range above upper quartile (Q3) or below lower quartile (Q1); the interquartile range is the range between upper and lower quartile: IQR = Q3-Q1.\n\nVisual approaches such as histogram, scatter plot (such as Q-Q plot), and boxplot are the easiest method to detect outliers. Let’s take an example of the univariate dataset and identify outliers using visual approaches.\n\n\n# x <- c(5, 8, 8, 12, 14, 15, 16, 19, 20, 22, 24, 25, 25, 26, 30, 48)\n# boxplot(x) # show the outliers\n\n\nYou can also use boxplot() to remove the outliers, like this:\n\n\n# boxplot(x, outline=FALSE) # remove the outliers\n\n\nUsing the interquartile range (IQR) to detect the data points which ranks at 25th percentile (first quartile or Q1) and 75th percentile (third quartile or Q3) in the dataset (IQR = Q3 - Q1), and futher detect outliers in three steps:\nFirst, calculating Q1 and Q3 with summary():\n\n\n# get values of Q1, Q3, and IQR\n# summary(x)\n#   Min.  1st Qu.  Median    Mean   3rd Qu.    Max. \n#   5.00   13.50   19.50    19.81   25.00     48.00 \n\n\nThen getting IQR with IQR() to calculate the threshold:\n\n\n\nFinally detecting the outliers and remove them:\n\n\n# find outlier\n# x[which(x < Tmin | x > Tmax)]\n# [1] 48\n# remove outlier\n# x[which(x > Tmin & x < Tmax)]\n\n\nHomework assignment: detecting the outliers of the doubs dataset following the above procedure.\nData transformation\nTransforming data is needed because statistical analyses and tests require that the residuals are normally distributed and have homogeneous variance, or because linear relationships may be easy to interpret. A good indicator of whether data need to be transformed is projecting the values using the histograms and checking whether the distribution is symmetrical, right-skewed or left-skewed. Ecological data are often right-skewed because they are limited by zero at the beginning. Several transformation ways are as follows:\nLog transformation is suitable for strongly right-skewed data\n\n\n# y <- log10(x) # for positively skewed data,\n# y <- log10(max(x+1) - x) # for negatively skewed data\n\n\n\nSquare-root transformation is suitable for slightly right-skewed data.\n\n\n# y <- sqrt(x) # for positively skewed data,\n# y <- sqrt(max(x+1) - x) # for negatively skewed data\n\n\nPower transformation is suitable for left-skewed data.\nReciprocal transformation is suitable for ratios (e.g. height/weight body ratio).\n\n\n# y <- 1/x # for positively skewed data\n# y <- 1/(max(x+1) - x) # for negatively skewed data\n\n\n2. Descriptive statistics\n2.1 Species data distribution\nTake the doubs datasets for example, illustrate how to do exploratory data analysis. First, we analysed species distribution.\n\n\n# fish species names\n#names(spe)\n\n# all species distribution\nab <- table(unlist(spe)) # if want to see, put (the entire code line) in a bracket\nbarplot(ab, las=1, # flips labels on y-axis into horizontal position\n        xlab=\"Abundance class\", ylab=\"Frequency\", col=grey(5:0/5))\n\n\n# individual species distribution\n# ggplot(spe, aes(x = Cogo)) + geom_histogram()\n# get the data\n# cogo <- table(spe$Cogo)\n# barplot(cogo, las=1, xlab=\"Abundance class\", ylab=\"Frequency\", col=grey(5:0/5))\n\n# Can see that an intermediate number of sites contain the highest number of species.\n# spe.pres <- colSums(spe > 0) # the number of sites where each species is present. \n# hist(spe.pres, main=\"Species occurrence\", las=1, xlab=\"Frequency of occurrences\", # breaks=seq(0,30, by=5), col=\"grey\")\n\n# Calculate the number of species that occur at each site\n#site.pres <- rowSums(spe>0) #number of species with a value greater than 0 in that site row\n#hist(site.pres, main=\"Species richness\", las=1, xlab=\"Frequency of sites\", ylab=\"Number of species\", breaks=seq(0,30, by=5), col=\"grey\")\n\n\n2.2 Species data standardization\nStandardization changes the data using a statistic calculated from data itself, e.g. mean, range, the sum of values (it is data-dependent). The most common reason to apply standardization is to remove differences in relative weights (importance) of individual variables or samples.\nCentring: Standardised variable has mean equal to zero.\nz-scores: Standardised variable has mean equal to zero and standard deviation equal to one.\n\n\n# creating Standardization function\n#standardize = function(x){\n#  z <- (x - mean(x)) / sd(x)\n#  return( z)\n#}\n  \n# apply your function to the dataset\n#dataframe[2:3] <-\n#  apply(dataframe[2:3], 2, standardize)\n  \n#displaying result\n#dataframe\n\n\nRanging: Changes the range of variable, e.g. into [0, 1].\n2.3 Species data transformations\nSometimes species/community data may also need to be standardized or transformed. The decostand() function in vegan provides standardization and transformation options for community composition data.\n\n\n#Transforming abundance or count data to pres-abs data\nspe.pa<-vegan::decostand(spe, method=\"pa\") \n\n#Hellinger transformation\nspe.hel<-vegan::decostand(spe, method=\"hellinger\") #can also use method=”hell”\n \n#Chi-square transformation\nspe.chi<-vegan::decostand(spe, method=\"chi.square\")\n\n\n2.4 Environmental data VS. collinearity\n\n\n# names(env)\n# dim(env)\n# str(env)\n# head(env)\n# summary(env)\npairs(env, main=\"Bivariate Plots of the Environmental Data\" ) \n\n\n\nIn this case, the environmental data (explanatory variables) are all in different units and need to be standardized prior to computing distance measures to perform ordination analyses. Standardize the environmental data (11 variables) using the function decostand() in vegan.\n\n\n\n3. Preliminary analysis\n3.1 Association measures\nPrior to starting multivariate analyses, you have matrices with ecological data (such as the DoubsEnv or DoubsSpe), and use them to create association matrices between objects or among descriptors. Exploring the possible association measures can help you to understand what distance measure to use within ordination methods.\nDistance measures of species data\nWe can use the vegdist() function to compute dissimilarity indices in order to quantifying community composition data. These can then be visualized as a matrix if desired.\n\n\n\nIn the spe.db matrix, the numbers represent the distance (dissimilarity) between the first 3 species in DoubsSpe would look like this:\n\n[1] \"matrix\" \"array\" \n          Cogo      Satr      Phph\nCogo 0.0000000 0.6000000 0.6842105\nSatr 0.6000000 0.0000000 0.1428571\nPhph 0.6842105 0.1428571 0.0000000\n\nYou can see that when comparing a species to itself (e.g. Cogo to Cogo), the distance = 0, because species 1 is like itself. You can create graphical depictions of these association matrices using the coldiss() function of the gclus package.\n\n\n library(gclus)\n source(\"coldiss.R\")\n# coldiss(spe.db, byrank=FALSE, diag=TRUE) # Heat map of Bray-Curtis dissimilarity\n# coldiss(spe.dj, byrank=FALSE, diag=TRUE) # Heat map of Jaccard dissimilarity\n# coldiss(spe.dg, byrank=FALSE, diag=TRUE) # Heat map of Gower dissimilarity\n\n\nDistance measures of env data\nLet’s look at associations between environmental variables (also known as Q mode)\n\n\n\nWe can then look at the dependence between environmental variables (also known as R mode):\n\n\n#(env.pearson<-cor(env)) # Pearson r linear correlation\n#round(env.pearson, 2) #Rounds the coefficients to 2 decimal points \n#(env.ken<-cor(env, method=\"kendall\")) # Kendall tau rank correlation\n#round(env.ken, 2) \n\n\nThe Pearson correlation measures the linear correlation between two variables. The Kendall tau is a rank correlation which means that it quantifies the relationship between two descriptors or variables when the data are ordered within each variable.\nIn some cases, there may be mixed types of environmental variables. Q mode can still be used to find associations between these environmental variables. We’ll do this by first creating an example dataframe:\n3.2 Clustering\nOne application of association matrices is clustering. It is not a statistical method per se, because it does not test a hypothesis, but it highlights structures in the data by partitioning the objects or the descriptors. As a result, similar objects are combined into groups. One goal of ecologists could be to divide a set of sites into groups with respect to their environmental conditions or their community composition.\nThere are several families of clustering methods. Let’s compare the single and complete linkage clustering methods using the Doubs fish species data.\n\n\nspe.dhel <- vegdist(spe.hel, method=\"euclidean\") #generates the distance matrix from Hellinger transformed data\n \nhead(spe.dhel)# Hellinger distances among sites\n\n[1] 0.8420247 0.9391305 1.0616631 1.2308244 1.1153793 0.9391305\n\n#Perform single linkage clustering\nspe.dhel.single<-hclust(spe.dhel, method=\"single\")\nplot(spe.dhel.single)\n\n\n#Perform complete linkage clustering\nspe.dhel.complete<-hclust(spe.dhel, method=\"complete\")\nplot(spe.dhel.complete)\n\n\n\nIn order to compare this dendrogram to the single and complete linkage clustering results, one must calculate the square root of the distances.\n\n\n#Perform Ward minimum variance clustering\nspe.dhel.ward<-hclust(spe.dhel, method=\"ward.D2\")\nplot(spe.dhel.ward)\n\n\n#Re-plot the dendrogram by using the square roots of the fusion levels\nspe.dhel.ward$height<-sqrt(spe.dhel.ward$height)\nplot(spe.dhel.ward)\n\n\nplot(spe.dhel.ward, hang=-1) # hang=-1 aligns all objets on the same line\n\n\n\nOne must be careful in the choice of an association measure and clustering method in order to correctly address a problem. What are you most interested in: gradients? Contrasts? In addition, the results should be interpreted with respect to the properties of the method used. If more than one method seems suitable to an ecological question, computing them all and compare the results would be to go. As a reminder, clustering is not a statistical method, but further steps can be taken to identify interpretable clusters, or to compute clustering statistics.\n4. Advance analysis\n4.1 General introduction\nOrdination is to reduce multidimensional information stored in community data into a few imaginable, interpretable and printable dimensions. We use it either to describe community pattern (usually the purpose of unconstrained = indirect ordination) or to explain changes in species composition by some (e.g. environmental, spatial, temporal) variables (constrained = direct ordination).\nOrdination methods can be divided according to two criteria: whether their algorithm includes environmental variables along to the species composition data (unconstrained ordination methods do not, while constrained do), and what type of species composition data is used for analysis (raw data of sample-species matrix of species composition, pre-transformed data using Hellinger transformation, or distance matrix (sample-sample symmetric matrix of distances between samples).\n\nRaw data-based\n\nTransformation-based\nDistance-based\n\nLinear\nUnimodal\n\n\n\n\n\n\n\nUnconstrained\nPCA\nCA & DCA\ntb-PCA\nPCoA, NMDS\nConstrained\nRDA\nCCA\ntb-RDA\ndb-RDA\nThe schemas below show the three alternative approaches you can use for the ordination of community ecology data, for either unconstrained or constrained ordination. You can decide to analyze data by either a) PCA/CA (depending on whether community composition data are homogeneous or heterogeneous), b) transformation-based PCA (first pre-transforming species composition data via Hellinger standardization, and then using PCA; doesn’t matter whether community composition data are homogeneous or heterogeneous), or c) distance-based PCoA or NMDS. But it often does not make much sense to combine these approaches.\n4.2 Unconstrained Ordination\nUnconstrained ordination allows us to organize samples, sites or species along continuous gradients (e.g. ecological or environmental). The key difference between unconstrained and constrained ordination is that in the unconstrained techniques we are not attempting to define a relationship between independent and dependent sets of variables.\nUnconstrained ordination can be used to:\nAssess relationships within a set of variables (not between sets).\nFind key components of variation between samples, sites, species etc.\nReduce the number dimensions in multivariate data without substantial loss of information.\nCreate new variables for use in subsequent analyses (such as regression). These principal components are weighted, linear combinations of the original variables in the ordination.\nPrincipal Component Analysis\nPrincipal component analysis (PCA) is used to generate a few key variables from a larger set of variables that still represent as much of the variation in the dataset as possible. PCA is powerful to analyze quantitative descriptors (such as species abundances), but can not be applied to binary data (such as species absence/presence). PCA preserves Euclidean distances and detects linear relationships. As a consequence, raw species abundance data are subjected to a pre-transformation (i.e. a Hellinger transformation) before computing a PCA.\nTo do a PCA you need:\nA set of variables (with no distinction between independent or dependent variables, i.e. a set of species OR a set of environmental variables).\nSamples that are measured for the same set of variables.\nGenerally a dataset that is longer than it is wide is preferred.\nThe “spe” data includes 27 fish taxa. To simplify the 27 fish taxa into a smaller number of fish-related variables or to identify where different sites or samples are associated with particular fish taxa we can run a PCA. Run a PCA on Hellinger-transformed species data:\n\n\n\nThe eigenvalue is the value of the change in the length of a vector. It is the amount of variation explained by each axis in a PCA. From the summary, you can see how much of the variance in the data is explained by the unconstrained variables. In this case, the total variance of the sites explained by the species is 0.5. The summary also tells you what proportion of the total explained variance is explained by each principal component in the PCA: the first axis of the PCA thus explains 51.33% of the variation, and the second axis 12.78%.\nSometimes you may want to extract the scores (i.e. the coordinates within a PCA biplot) for either the “sites” (the rows in your dataset, whether they be actual sites or not) or the “species” (the variables in your data, whether they be actual species or some other variables). This is useful if you want to then use a principal component as a variable in another analysis, or to make additional graphics. For example, with the “spe” dataset, you might want to obtain a single variable that is a composite of all the fish abundance data and then use that use that variable in a regression with another variable, or plot across a spatial gradient. To extract scores from a PCA, use the scores() function:\n\n\n\nThe PCA on the “spe” fish data produces as many principal components as there are fish taxon (columns), which in this case means that 27 principal components are produced. In many cases though, you may have done a PCA to reduce the number of variables to deal with and produce composite variables for the fish. In this case, you are likely interested in knowing how many of these principal components are actually significant or adding new information to the PCA (i.e. how many principal components do you need to retain before you aren’t really explaining any more variance with the additional principal components). To determine this, you can use the Kaiser-Guttman criterion and produce a barplot showing at what point the principal components are no longer explaining significant amount of variance. The code for the barplot below shows the point at which the variance explained by a new principal component explains less than the average amount explained by all of the eigenvalues:\n\n       PC1        PC2        PC3        PC4        PC5 \n0.25796049 0.06424089 0.04632294 0.03850244 0.02196526 \n\n\nFrom this barplot, you can see that once you reach PC6, the proportion of variance explained falls below the average proportion explained by the other components. If you take another look at the PCA summary, you will notice that by the time you reach PC5, the cumulative proportion of variance explained by the principal components is 85%.\nA PCA is not just for species data. It can also be run and interpreted in the same way using standardized environmental variables:\n\n\nCall:\nrda(X = env.z) \n\nPartitioning of variance:\n              Inertia Proportion\nTotal              11          1\nUnconstrained      11          1\n\nEigenvalues, and their contribution to the variance \n\nImportance of components:\n                        PC1    PC2     PC3     PC4     PC5     PC6\nEigenvalue            6.446 2.2252 0.99825 0.39831 0.36224 0.25361\nProportion Explained  0.586 0.2023 0.09075 0.03621 0.03293 0.02306\nCumulative Proportion 0.586 0.7883 0.87903 0.91524 0.94817 0.97123\n                          PC7     PC8      PC9     PC10     PC11\nEigenvalue            0.16106 0.11062 0.022949 0.017476 0.004378\nProportion Explained  0.01464 0.01006 0.002086 0.001589 0.000398\nCumulative Proportion 0.98587 0.99593 0.998013 0.999602 1.000000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  4.189264 \n\n\nSpecies scores\n\n         PC1     PC2      PC3       PC4       PC5      PC6\ndfs  1.10786  0.4901 -0.20230  0.045871  0.187500 -0.20053\nalt -1.06604 -0.5607  0.15135  0.193929 -0.096582 -0.05541\nslo -0.95746 -0.5295 -0.25933 -0.352365 -0.066943 -0.39476\nflo  0.98732  0.6262 -0.23731  0.009137  0.105599 -0.31209\npH  -0.02679  0.4991  1.14319 -0.014864  0.005398 -0.17594\nhar  0.91493  0.5514 -0.09538 -0.128923 -0.639664  0.07145\npho  1.02238 -0.6481  0.20016 -0.178923 -0.040772 -0.04615\nnit  1.14057 -0.1628  0.05081 -0.283125  0.272940  0.20434\namm  0.96776 -0.7382  0.18740 -0.198493  0.021140  0.04733\noxy -0.99282  0.4993  0.04744 -0.543295  0.012373  0.06953\nbdo  0.96051 -0.7274  0.09630  0.089149 -0.178489 -0.14521\n\n\nSite scores (weighted sums of species scores)\n\n        PC1     PC2       PC3      PC4     PC5        PC6\n1  -1.35309 -1.0614 -0.626184 -1.14841  1.0494 -1.821e+00\n2  -1.05499 -0.7841  0.195705  0.90757  1.7294  2.655e-01\n3  -0.97457 -0.4890  1.340010  0.61152  0.8592 -7.288e-01\n4  -0.90281 -0.3118  0.000372  0.17712 -0.2052  5.288e-01\n5  -0.45666 -0.6973  0.550276  1.16964 -1.2500  1.273e-01\n6  -0.81070 -0.7590 -0.318284  0.77249  0.2537  1.263e-01\n7  -0.85277 -0.1858  0.231151 -0.37159 -1.3564 -3.616e-01\n9  -0.27926 -0.4610  0.061754  1.60909 -1.2127  8.870e-01\n10 -0.59145 -0.5554 -1.595293 -0.35281 -0.6397 -2.559e-01\n11 -0.34078  0.3167  0.005014 -1.23777 -0.7883 -2.345e-01\n12 -0.44165  0.3209 -0.697214 -0.46903 -0.3928  5.963e-01\n13 -0.39855  0.6314  0.003511 -0.90415 -1.0778  5.002e-05\n14 -0.22649  0.7350  0.946755 -0.87823 -0.8730  3.814e-02\n15 -0.21927  1.0432  2.269502 -0.19576  0.1024 -2.360e-01\n16 -0.16778  0.2507 -0.340084 -0.54136  0.1054  6.195e-01\n17  0.14914  0.3628 -0.171537 -0.14337  0.1372  1.435e+00\n18  0.08633  0.3674 -0.238531 -0.44014  0.2901  1.015e+00\n19  0.10967  0.4821  0.232435 -0.28363  0.7316  9.414e-01\n20  0.18575  0.3732 -0.268777 -0.69333  0.9729  1.131e+00\n21  0.16766  0.3112 -0.834583  0.21603  0.7147  5.100e-01\n22  0.13041  0.4842 -0.108135  0.18812  0.2895 -5.716e-01\n23  1.28519 -1.3164  0.715652 -0.57204 -0.6499 -1.021e+00\n24  1.01542 -0.4735  0.019519  1.43289 -0.5981  1.440e-01\n25  2.10059 -2.1406  0.361157 -1.21146  0.1786  4.424e-01\n26  0.89379 -0.1213 -0.671652  0.86581  0.3046 -8.871e-02\n27  0.61092  0.3178 -0.139402  0.31511  0.8178 -9.684e-01\n28  0.82353  0.8569  0.802337 -0.04239  0.8747  5.103e-02\n29  0.67793  1.0652 -1.729365  0.28387 -0.2944 -1.028e+00\n30  0.83450  1.4380  0.003890  0.93621 -0.0730 -1.543e+00\n\nCall:\nrda(X = env.z) \n\nPartitioning of variance:\n              Inertia Proportion\nTotal              11          1\nUnconstrained      11          1\n\nEigenvalues, and their contribution to the variance \n\nImportance of components:\n                        PC1    PC2     PC3     PC4     PC5     PC6\nEigenvalue            6.446 2.2252 0.99825 0.39831 0.36224 0.25361\nProportion Explained  0.586 0.2023 0.09075 0.03621 0.03293 0.02306\nCumulative Proportion 0.586 0.7883 0.87903 0.91524 0.94817 0.97123\n                          PC7     PC8      PC9     PC10     PC11\nEigenvalue            0.16106 0.11062 0.022949 0.017476 0.004378\nProportion Explained  0.01464 0.01006 0.002086 0.001589 0.000398\nCumulative Proportion 0.98587 0.99593 0.998013 0.999602 1.000000\n\nScaling 2 for species and site scores\n* Species are scaled proportional to eigenvalues\n* Sites are unscaled: weighted dispersion equal on all dimensions\n* General scaling constant of scores:  4.189264 \n\n\nSpecies scores\n\n         PC1     PC2      PC3       PC4       PC5      PC6\ndfs  1.10786  0.4901 -0.20230  0.045871  0.187500 -0.20053\nalt -1.06604 -0.5607  0.15135  0.193929 -0.096582 -0.05541\nslo -0.95746 -0.5295 -0.25933 -0.352365 -0.066943 -0.39476\nflo  0.98732  0.6262 -0.23731  0.009137  0.105599 -0.31209\npH  -0.02679  0.4991  1.14319 -0.014864  0.005398 -0.17594\nhar  0.91493  0.5514 -0.09538 -0.128923 -0.639664  0.07145\npho  1.02238 -0.6481  0.20016 -0.178923 -0.040772 -0.04615\nnit  1.14057 -0.1628  0.05081 -0.283125  0.272940  0.20434\namm  0.96776 -0.7382  0.18740 -0.198493  0.021140  0.04733\noxy -0.99282  0.4993  0.04744 -0.543295  0.012373  0.06953\nbdo  0.96051 -0.7274  0.09630  0.089149 -0.178489 -0.14521\n\n\nSite scores (weighted sums of species scores)\n\n        PC1     PC2       PC3      PC4     PC5        PC6\n1  -1.35309 -1.0614 -0.626184 -1.14841  1.0494 -1.821e+00\n2  -1.05499 -0.7841  0.195705  0.90757  1.7294  2.655e-01\n3  -0.97457 -0.4890  1.340010  0.61152  0.8592 -7.288e-01\n4  -0.90281 -0.3118  0.000372  0.17712 -0.2052  5.288e-01\n5  -0.45666 -0.6973  0.550276  1.16964 -1.2500  1.273e-01\n6  -0.81070 -0.7590 -0.318284  0.77249  0.2537  1.263e-01\n7  -0.85277 -0.1858  0.231151 -0.37159 -1.3564 -3.616e-01\n9  -0.27926 -0.4610  0.061754  1.60909 -1.2127  8.870e-01\n10 -0.59145 -0.5554 -1.595293 -0.35281 -0.6397 -2.559e-01\n11 -0.34078  0.3167  0.005014 -1.23777 -0.7883 -2.345e-01\n12 -0.44165  0.3209 -0.697214 -0.46903 -0.3928  5.963e-01\n13 -0.39855  0.6314  0.003511 -0.90415 -1.0778  5.002e-05\n14 -0.22649  0.7350  0.946755 -0.87823 -0.8730  3.814e-02\n15 -0.21927  1.0432  2.269502 -0.19576  0.1024 -2.360e-01\n16 -0.16778  0.2507 -0.340084 -0.54136  0.1054  6.195e-01\n17  0.14914  0.3628 -0.171537 -0.14337  0.1372  1.435e+00\n18  0.08633  0.3674 -0.238531 -0.44014  0.2901  1.015e+00\n19  0.10967  0.4821  0.232435 -0.28363  0.7316  9.414e-01\n20  0.18575  0.3732 -0.268777 -0.69333  0.9729  1.131e+00\n21  0.16766  0.3112 -0.834583  0.21603  0.7147  5.100e-01\n22  0.13041  0.4842 -0.108135  0.18812  0.2895 -5.716e-01\n23  1.28519 -1.3164  0.715652 -0.57204 -0.6499 -1.021e+00\n24  1.01542 -0.4735  0.019519  1.43289 -0.5981  1.440e-01\n25  2.10059 -2.1406  0.361157 -1.21146  0.1786  4.424e-01\n26  0.89379 -0.1213 -0.671652  0.86581  0.3046 -8.871e-02\n27  0.61092  0.3178 -0.139402  0.31511  0.8178 -9.684e-01\n28  0.82353  0.8569  0.802337 -0.04239  0.8747  5.103e-02\n29  0.67793  1.0652 -1.729365  0.28387 -0.2944 -1.028e+00\n30  0.83450  1.4380  0.003890  0.93621 -0.0730 -1.543e+00\n\nScaling refers to what portion of the PCA is scaled to the eigenvalues. Scaling = 2 means that the species scores are scaled by eigenvalues, whereas scaling = 1 means that site scores are scaled by eigenvalues. Scaling = 3 means that both species and site scores are scaled symmetrically by square root of eigenvalues. Using scaling = 1 means that the Euclidean distances among objects (e.g. the rows in your data) are preserved, whereas scaling = 2 means that the correlations among descriptors (e.g. the columns in this data) are preserved. This means that when you look at a biplot of a PCA that has been run with scaling=2, the angle between descriptors represents correlation.\n\n     PC1      PC2 \n6.445919 2.225186 \n\n\nAs you saw in the explanation of the summary output, a lot of information can be extracted from a PCA before even plotting it. A PCA figure is the best way to convey major patterns. A PCA biplot includes the x-axis as the first Principal Component and the y-axis as the second Principal Component. A basic biplot without any customization could be plotted like this, where the site positions are shown by black numbers and species’ positions are shown by red species codes. Remember, species positions come from plotting species along PCs and the site positions are derived from the weighted sums of species scores.\n\n\n\nWhat conclusions can you draw from this plot? You can see that there are only a few sites that are farther away from the majority. The species names are shown by their names in red and from the plot, you can see for example that the species “ABL” is not found or not found in the same prevalence in the majority of sites as other species closer to the centre of the ordination.\nNow let’s look at a plot of the environmental PCA:\n\n\n\nNonmetric MultiDimensional Scaling\nThe unconstrained ordination allow to organize objects (e.g. sites) characterized by descriptors (e.g. species) in full-dimensional space. In other words, PCA, CA and PCoA computes lots of ordination axes representing the variation of species among sites and preserve distance among objects (the Euclidean distances in PCA, the Chi2 distances in CA and the other distances in PCoA). Users can then select the axis of interest to represent objects in an ordination plot. The produced biplot represents the distance among objects (e.g. the between-sites similarity), but fails to represent the whole variation dimensions of the ordination space.\nIn some case, the priority is not to preserve the exact distances among sites, but rather to represent as accurately as possible the relationships among objects in a number of axes (generally two or three). In such cases, nonmetric multidimensional scaling (NMDS) is the solution. A biplot of NMDS is better to represent similarity between objects: dissimilar objects are apart in the ordination space and similar objects close to one another. Also, NMDS allows users to choose the distance measure applied to calculate the ordination.\nTo find the best object representation, NMDS applies an iterative procedure to position the objects in the number of dimensions to minimize a stress function (scaled from 0 to 1). Consequently, the lower the stress value, the better the representation of objects in the ordination-space is. An additional way to assess the appropriateness of an NDMS is to construct a Shepard diagram which plot distances among objects in the ordination plot against the original distances. The R2 obtained from the regression between these two distances measure the goodness-of-fit of the NMDS ordination.\n\nRun 0 stress 0.0747782 \nRun 1 stress 0.1152149 \nRun 2 stress 0.07478399 \n... Procrustes: rmse 0.003625267  max resid 0.0143458 \nRun 3 stress 0.08843919 \nRun 4 stress 0.07477801 \n... New best solution\n... Procrustes: rmse 0.0002980715  max resid 0.001431492 \n... Similar to previous best\nRun 5 stress 0.1196716 \nRun 6 stress 0.1124391 \nRun 7 stress 0.1141839 \nRun 8 stress 0.09157455 \nRun 9 stress 0.08801543 \nRun 10 stress 0.08841667 \nRun 11 stress 0.08695588 \nRun 12 stress 0.07376241 \n... New best solution\n... Procrustes: rmse 0.01940148  max resid 0.09468274 \nRun 13 stress 0.122613 \nRun 14 stress 0.07477812 \nRun 15 stress 0.08917037 \nRun 16 stress 0.1223245 \nRun 17 stress 0.07383674 \n... Procrustes: rmse 0.003819841  max resid 0.01488639 \nRun 18 stress 0.1204797 \nRun 19 stress 0.08901468 \nRun 20 stress 0.08927164 \n*** Best solution was not repeated -- monoMDS stopping criteria:\n     1: no. of iterations >= maxit\n    19: stress ratio > sratmax\n[1] 0.07376241\n\n\nThe Shepard plot identifies a strong correlation between observed dissimilarity and ordination distance (\\(R^2 > 0.95\\)), highlighting a high goodness-of-fit of the NMDS.\n\n\n\n\n\n\n",
    "preview": "datexpl/2023-10-15-exploratoryanalysis/box2-1.png",
    "last_modified": "2023-10-25T09:38:16+08:00",
    "input_file": {}
  },
  {
    "path": "datexpl/2023-12-05-remote-sensing-image/",
    "title": "Lesson 4: Remote sensing images in Ecology",
    "description": "Remote sensing images are great sources for ecological researches. They are especially useful for natural resoures and a variety of socio-economic researches and applications. This section will introduce how to manipluate these data.",
    "author": [],
    "date": "2023-10-15",
    "categories": [],
    "contents": "\n\nContents\n1. Remote sensing data\n2. Data operations and tools\n2.1 Download data of aoi\n2.2 Merging, cropping and masking\n2.3 Extracting values and computing statistics\n2.4 Storing and exporting results\n\n3. A practical example\n\n1. Remote sensing data\nForest Cover Data\nThis section is adopted from the module. we will primarily work with the Vegetation Continuous Fields (VCF) data provided by the Land Processes Distributed Active Archive Center (LP DAAC), a component of NASA’s Earth Observing System Data and Information System (EOSDIS). The MOD44B Version 6 VCF is a yearly representation of surface vegetation from 2000 to 2020 at 250 m resolution. Each pixel stores a percentage of three ground cover components: percent tree cover, percent non-tree cover, and percent bare.\nThe ground cover percentages are estimates from a machine learning model based on the combination of the Moderate Resolution Imaging Spectroradiometer (MODIS) data and other high resolution data from NASA and Google Earth. The machine learning model incorporates the visible bandwidth as well as other bandwidth such as brightness temperature (from MODIS bands 20, 31, 32).\nThe VCF data utilize thermal signatures and other correlates to distinguish forest and non-forest plantation, which is an improvement compared to the Normalized Differenced Vegetation Index (NDVI). For this use case, VCF also improves on the Global Forest Cover (GFC) data set, another source used to study deforestation, which only provides binary data points. GFC records baseline forest cover in the year 2000 and includes a binary indicator for the year of deforestation for each 30m × 30m pixel. If over 90% of the forest cover was lost in a pixel by a given year, then the pixel is marked as deforested, while a pixel is marked as reforested if the forest cover went from 0 in 2000 to a positive value. The VCF records continuous changes in percent of ground cover components, which provides more details than the GFC data.\nNighttime lights\nThere is a strong correlation between nighttime lights and Gross State Product (GSP) or Gross Domestic Product (GDP) measures, at the national, state and regional levels or even at a more granular resolution. Thus, nighttime light observations can be used as a proxy for economic activity, especially over periods or regions where these data are not available or where the statistical systems are of low quality or when no recent population or economic censuses are available. Similarly, changes in nighttime light intensity can be used by economists as an additional measure of income growth when no other measures of income growth are available.\nProville et al. (2017) examined trends observed by DMSP-OLS in the period 1992-2013 and their correlation with a series of socio-economic indicators. They found the strongest correlations between nighttime lights, electricity consumption, CO2 emissions, and GDP, followed by population, CH4 emissions, N2O emissions, poverty and F-gas emissions.\n2. Data operations and tools\n2.1 Download data of aoi\nIn order to perform data manipulation, we need to attach packages. We are going to use the package luna to download data from MODIS and the packages terra, tidyverse, raster, and sf for data manipulation.\n\n\n\nWe follow thistutorial to get MODIS data with luna. For details of the terra package, please refer to the package manuscript and this tutorial. If you are not familiar with the tidyverse workflow, please refer to the R for Data Science.\nOnce the required packages have been attached, we can access VCF in R. We prefer using R for its ability to download large numbers of files and enable regular, automated updates.\nWe can first use luna to check the list of data products available from MODIS. Since luna can also access data from the LANDSAT and SENTINEL platforms, we add “MOD|MYD|^MCD” to narrow our scope to MODIS data. The printed results below list six products from MODIS.\n\n\n\nThe product name for VCF is MOD44B. We can use the function productInfo to launch the information page of VCF.\n\n\n\nWe can query MODIS and only download a subset of the data. We need to specify the start and end dates and our area of interest (AOI). The date format is “yyyy-mm-dd”. Suppose here we want to subset data from 2010 to 2012.\n\n\n\nIn order to subset your area of interest, you need to provide a “map” to getModis(). This can be obtained from online databases such as the global administrative area database (GADM). You can download map data directly from GADM or you can use R to obtain GADM map data. We will use R below, which requires first installing the package geodata.\n\n\n\nGeographic levels in GADM are defined as:\nlevel 0: National\nlevel 1: State/province/equivalent\nlevel 2: County/district/equivalent\nlevel 3/4: Smaller administrative levels\nFor our example, we are interested in India at the district level. We can download the map of India and its level 2 administrative areas with the following code:\n\n\n\nThe boundary data is downloaded to the path that you specified in the path argument. The downloaded data through gadm() will be in the PackedSpatVector class. If you want to convert it to another class (for example, the sf class, which is easier to work with in R), you can first read it using readRDS(), then convert to a SpatVector via vect() from the terra package, and finally convert it to a sf object.\n\n\n\nThe map we downloaded is at the district level (level 2). Assume our AOI is the state of Odisha. Each row of the data represents a county in Odisha, and the geospatial information for each county is stored in the last column: geometry. We can filter to obtain the boundaries for our AOI, which will return aoi in vector format, stored as a data frame in R.\n\n\n\n\n\n\nNow that we have our AOI as well as time frame, we can filter the MODIS VCF data on these values and see what is available.\n\n\n\nThe products we are going to download are tiled products. For details of tiled products, the tilling system, and the naming convention, please refer to the MODIS overview page. In essence, we will be downloading grids of maps that cover our AOI.\nTo actually download these files from the NASA server, you will need a username and password. Please register on NASA Earth Data if you haven’t done so.\nThe following code will download the files. Replace the path value with the location on your computer where you would like to store these files. Replace the username and password values with your NASA Earth Data credentials from above.\n\n\n\nThe data format from MODIS is HDF and may include sub-datasets. We can use terra to read these files and create raster files. For example,\n\n\n\nWe can find basic information such as the coordinate reference system, number of cells, and resolution from the above output. There are 7 layers in each of the VCF tiled files. We are interested in the percent tree coverage layer.\n\n\n\nA quick plot of the data can be done with the plotRBG() function.\n\n\n\n2.2 Merging, cropping and masking\nSince there are four hdf files in each year for our AOI, we can first merge the four SpatRaster files into one file per year. We’ll use 2010 as an example. We can filter to only include our layer of interest - percent of tree cover - from each hdf file, which can be done by subsetting the output using [[1]] (using 1 because percent tree cover is the first layer in each file).\n\n\n\nBefore we merge these SpatRster objects, it is often a good practice to check their origins and resolutions. merge requires origin and resolution to be the same across objects.\n\n\n\n\n\n\nWe see that origins of these files are slightly different, but all are close to (0, 0). We do not need to worry about these slight differences, as merge will handle them automatically.\n\n\n\nNote: cells with 200% represent water and rivers.\nWe are now ready to crop and mask the raster file to match our AOI. This tutorial explains the difference between cropping and masking.\nTo crop a raster file according to vector data boundaries (eg, our aoi object representing Odisha districts), we first align the coordinate reference systems of our raster file and vector file. Then, use crop(raster data, vector data). To mask, use mask(raster data, vector data). Note that for terra::mask(), the second argument needs to be SpatVector. terra does not support sf objects yet, so we use vect(aoi) to convert our sf object aoi to a SpatVector.\n\n\n\nTo plot our new raster file, we use:\n\n\n\n2.3 Extracting values and computing statistics\nAfter we have cropped and masked the raster file to our AOI, we can extract values for each county in the state of Odisha.\n\n\n\nThe values extracted by terra::extract are stored in a data frame. Note that the ID corresponds to the row number of your vector file (i.e. object aoi in our case). We can then compute statistics based on this data frame. Here we compute several statistics describing the percent of forest cover for each county. Note that cells with 200% represent water and river and should be excluded from calculation.\n\n\n\n2.4 Storing and exporting results\nWith terra you can easily write shape files and several formats of raster files. The main function for writing vector data is writeVector(), while for writing raster data we use writeRaster(). For details, you can refer to this page and the documentation of terra.\n3. A practical example\nWe will replicate some main results in the paper. To access the full replication data and code, check this github repo. We are going to replicate Table 3 in the paper.\nThe research question is whether newly constructed rural roads impact local deforestation. The authors explored this question using two empirical strategies: fuzzy RD and difference-in-difference. In the following sections, we implement the difference-in-difference method and replicate the regression results.\nIn order to run fixed effects models, we will need the fixest package. This tutorial is a good reference for introducing fixest functions.\nData for this exercise was processed and stored in pmgsy_trees_rural_panel.csv, which you can find the through the link to the CSV data in the github repo. Each row of the data frame presents a village in a specific year.\n\n\n\nThe paper estimated the following equation:\n\\[\nForest_{vdt} = β_{1}Award_{vdt} + β_{2}Complete_{vdt} + α_{v} + γ_{dt} + X_{v}⋅V_{t} + η_{vdt}\n\\]\nwhere \\(Forest_{vdt}\\) is forest cover of village \\(v\\) in district \\(d\\) in year \\(t\\). \\(Award_{vdt}\\) is a dummy variable which takes one during the period when the new road is awarded to the village but has not been built. \\(Complete_{vdt}\\) is also a dummy variable which takes one for all years following the completion of a new road to village \\(v\\). \\(α_{v}\\) are village fixed effects, while \\(γ_{dt}\\) are the district-year fixed effects. \\(X_{v}\\) controls some baseline characteristics (e.g. forest cover in 2000, total population) and is interacted with year fixed effects \\(V_{t}\\).\nThere is one more step before we run the regressions. In Stata, which the authors used for their regression, reghdfe removed singleton groups automatically. However, the fixest package currently doesn’t possess this functionality, so for now, we will manually remove these observations.\n\n\n\nFinally, we can run our regressions. Following the authors, we test the effect of being awarded a new road and receiving the road on the log forest cover as well as on the average forest cover.\n\n\n\nOur results align with the authors’ findings presented in Table 3 which show that being awarded a road has a negative impact on forest cover (approximately 0.5% loss in the construction period between being awarded a road and its completion), but after the road is constructed, forest cover appears to return. This could incorrectly be interpreted as a positive effect of roads on tree cover if the award term is left out. This determination that rural roads have no effect on forest loss, in combination with the authors’ additional findings of substantial forest loss due to highway construction, have important policy implications for governments considering similar infrastructure expansion. The use of VCF data in this study enabled significant insights, and the potential use cases for VCF data remain numerous.\n\n\n\n",
    "preview": "datexpl/2023-12-05-remote-sensing-image/remotesensing.gif",
    "last_modified": "2023-12-05T16:08:16+08:00",
    "input_file": {}
  }
]
